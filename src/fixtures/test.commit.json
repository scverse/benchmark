{
    "sha": "f88f7bd4250b963752d615e491b7e676ce5eb7f0",
    "node_id": "C_kwDOBfZ26doAKGY4OGY3YmQ0MjUwYjk2Mzc1MmQ2MTVlNDkxYjdlNjc2Y2U1ZWI3ZjA",
    "commit": {
        "author": {
            "name": "Isaac Virshup",
            "email": "ivirshup@gmail.com",
            "date": "2024-02-27T13:30:07Z"
        },
        "committer": {
            "name": "GitHub",
            "email": "noreply@github.com",
            "date": "2024-02-27T13:30:07Z"
        },
        "message": "Merge branch 'main' into ooc-docs",
        "tree": {
            "sha": "82bb877b848bad1ad0ef3a78e1e0aeece77ca9fc",
            "url": "https://api.github.com/repos/scverse/anndata/git/trees/82bb877b848bad1ad0ef3a78e1e0aeece77ca9fc"
        },
        "url": "https://api.github.com/repos/scverse/anndata/git/commits/f88f7bd4250b963752d615e491b7e676ce5eb7f0",
        "comment_count": 0,
        "verification": {
            "verified": true,
            "reason": "valid",
            "signature": "-----BEGIN PGP SIGNATURE-----\n\nwsFcBAABCAAQBQJl3ePfCRC1aQ7uu5UhlAAAil0QADfg4wbs5w3+nicxWn1MxC/9\nKumnClYGdv6m4h9O8wVoLPbpKRuqVPkcJ2z8nhGZTP/PPdq7GCRR/L+W7Z9yoyed\nYoRg8G6VIOTNFF3Qh812cWq5+CvQRBr3dHbWprrB6J1ph6/E20ozhOWZMCVFUhfj\n/GeUvkbIFBF3idb68eQE7G3eHK1moqTwCxhPW5VcWXwiXqsMw5iCwFUaDQPIWnJU\n5mLeypoi058BfNXsuGHcDfU91eUMiWAKrF7VtLhAiR65h4Om5lp6ZZTgGRyzhiY9\nBiunzNzZGHnXnlhYz2SwOjYChdcXOAg8BracUy3Fg/osBpWZcCF84cDGRXB9fFn1\niUNJ/icYVDwfK/aVjpJJfe5VTxjujX+G5HzbK67sFtV1pTzWqOXAISrP1Op4agsB\nEQ1srAuN6+tVbEH7SLx9jEa/72TAvUVhYoSgbaxYLhKbtetdpzEXo8OKtROoX3xH\n4FpC4JhTMwZEavEhT98sSGAV2jBlp7KfLVEia/IQuPLtQZsh28YtWLwjXaiU8aW8\n5uk7fTh0jHjAcyrEyldWdFt/q7zMhvtb6QFtnhds7zDYxwl0yuWsKkoADwvkf5BT\n0+kyihC2x2YF/LcjlhC61Xwibec0V++eON99h2eOmCJBHzTpBv3cEtPdl2bOnN+Y\nmazzaByEzItd++z+hIfB\n=JjEf\n-----END PGP SIGNATURE-----\n",
            "payload": "tree 82bb877b848bad1ad0ef3a78e1e0aeece77ca9fc\nparent cc6d6ea741ff6c35df3747a95c4869cc3ed5f84e\nparent a4786471ee4d4e894fec150e426c3551db0f31e0\nauthor Isaac Virshup <ivirshup@gmail.com> 1709040607 +1100\ncommitter GitHub <noreply@github.com> 1709040607 +1100\n\nMerge branch 'main' into ooc-docs"
        }
    },
    "url": "https://api.github.com/repos/scverse/anndata/commits/f88f7bd4250b963752d615e491b7e676ce5eb7f0",
    "html_url": "https://github.com/scverse/anndata/commit/f88f7bd4250b963752d615e491b7e676ce5eb7f0",
    "comments_url": "https://api.github.com/repos/scverse/anndata/commits/f88f7bd4250b963752d615e491b7e676ce5eb7f0/comments",
    "author": {
        "login": "ivirshup",
        "id": 8238804,
        "node_id": "MDQ6VXNlcjgyMzg4MDQ=",
        "avatar_url": "https://avatars.githubusercontent.com/u/8238804?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/ivirshup",
        "html_url": "https://github.com/ivirshup",
        "followers_url": "https://api.github.com/users/ivirshup/followers",
        "following_url": "https://api.github.com/users/ivirshup/following{/other_user}",
        "gists_url": "https://api.github.com/users/ivirshup/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/ivirshup/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/ivirshup/subscriptions",
        "organizations_url": "https://api.github.com/users/ivirshup/orgs",
        "repos_url": "https://api.github.com/users/ivirshup/repos",
        "events_url": "https://api.github.com/users/ivirshup/events{/privacy}",
        "received_events_url": "https://api.github.com/users/ivirshup/received_events",
        "type": "User",
        "site_admin": false
    },
    "committer": {
        "login": "web-flow",
        "id": 19864447,
        "node_id": "MDQ6VXNlcjE5ODY0NDQ3",
        "avatar_url": "https://avatars.githubusercontent.com/u/19864447?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/web-flow",
        "html_url": "https://github.com/web-flow",
        "followers_url": "https://api.github.com/users/web-flow/followers",
        "following_url": "https://api.github.com/users/web-flow/following{/other_user}",
        "gists_url": "https://api.github.com/users/web-flow/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/web-flow/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/web-flow/subscriptions",
        "organizations_url": "https://api.github.com/users/web-flow/orgs",
        "repos_url": "https://api.github.com/users/web-flow/repos",
        "events_url": "https://api.github.com/users/web-flow/events{/privacy}",
        "received_events_url": "https://api.github.com/users/web-flow/received_events",
        "type": "User",
        "site_admin": false
    },
    "parents": [
        {
            "sha": "cc6d6ea741ff6c35df3747a95c4869cc3ed5f84e",
            "url": "https://api.github.com/repos/scverse/anndata/commits/cc6d6ea741ff6c35df3747a95c4869cc3ed5f84e",
            "html_url": "https://github.com/scverse/anndata/commit/cc6d6ea741ff6c35df3747a95c4869cc3ed5f84e"
        },
        {
            "sha": "a4786471ee4d4e894fec150e426c3551db0f31e0",
            "url": "https://api.github.com/repos/scverse/anndata/commits/a4786471ee4d4e894fec150e426c3551db0f31e0",
            "html_url": "https://github.com/scverse/anndata/commit/a4786471ee4d4e894fec150e426c3551db0f31e0"
        }
    ],
    "stats": {
        "total": 2877,
        "additions": 2347,
        "deletions": 530
    },
    "files": [
        {
            "sha": "81768c77668a63f61ed8c2a09cd0d53c3ac85e47",
            "filename": ".azure-pipelines.yml",
            "status": "modified",
            "additions": 33,
            "deletions": 13,
            "changes": 46,
            "blob_url": "https://github.com/scverse/anndata/blob/f88f7bd4250b963752d615e491b7e676ce5eb7f0/.azure-pipelines.yml",
            "raw_url": "https://github.com/scverse/anndata/raw/f88f7bd4250b963752d615e491b7e676ce5eb7f0/.azure-pipelines.yml",
            "contents_url": "https://api.github.com/repos/scverse/anndata/contents/.azure-pipelines.yml?ref=f88f7bd4250b963752d615e491b7e676ce5eb7f0",
            "patch": "@@ -1,26 +1,34 @@\n trigger:\n   - main\n+  - \"*.*.x\"\n \n variables:\n   PIP_CACHE_DIR: $(Pipeline.Workspace)/.pip\n   RUN_COVERAGE: no\n   PYTEST_ADDOPTS: --color=yes --junitxml=test-data/test-results.xml\n-  PRERELEASE_DEPENDENCIES: no\n+  DEPENDENCIES_VERSION: \"latest\" # |\"pre-release\" | \"minimum-version\"\n+  TEST_TYPE: \"standard\" # | \"coverage\"\n \n jobs:\n   - job: PyTest\n     pool:\n       vmImage: \"ubuntu-22.04\"\n     strategy:\n       matrix:\n-        Python3.11:\n-          python.version: \"3.11\"\n+        Python3.12:\n+          python.version: \"3.12\"\n           RUN_COVERAGE: yes\n+          TEST_TYPE: \"coverage\"\n         Python3.9:\n           python.version: \"3.9\"\n         PreRelease:\n-          python.version: \"3.11\"\n-          PRERELEASE_DEPENDENCIES: yes\n+          python.version: \"3.12\"\n+          DEPENDENCIES_VERSION: \"pre-release\"\n+          TEST_TYPE: \"strict-warning\"\n+        minimum_versions:\n+          python.version: \"3.9\"\n+          DEPENDENCIES_VERSION: \"minimum\"\n+          TEST_TYPE: \"coverage\"\n     steps:\n       - task: UsePythonVersion@0\n         inputs:\n@@ -40,13 +48,20 @@ jobs:\n           python -m pip install --upgrade pip wheel\n           pip install .[dev,test]\n         displayName: \"Install dependencies\"\n-        condition: eq(variables['PRERELEASE_DEPENDENCIES'], 'no')\n+        condition: eq(variables['DEPENDENCIES_VERSION'], 'latest')\n+\n+      - script: |\n+          python -m pip install pip wheel tomli packaging pytest-cov\n+          pip install `python3 ci/scripts/min-deps.py pyproject.toml --extra dev test`\n+          pip install --no-deps .\n+        displayName: \"Install minimum dependencies\"\n+        condition: eq(variables['DEPENDENCIES_VERSION'], 'minimum')\n \n       - script: |\n           python -m pip install --pre --upgrade pip wheel\n           pip install --pre .[dev,test]\n         displayName: \"Install dependencies release candidates\"\n-        condition: eq(variables['PRERELEASE_DEPENDENCIES'], 'yes')\n+        condition: eq(variables['DEPENDENCIES_VERSION'], 'pre-release')\n \n       - script: |\n           pip list\n@@ -55,18 +70,23 @@ jobs:\n       - script: |\n           pytest\n         displayName: \"PyTest\"\n-        condition: eq(variables['RUN_COVERAGE'], 'no')\n+        condition: eq(variables['TEST_TYPE'], 'standard')\n \n       - script: |\n           pytest --cov --cov-report=xml --cov-context=test\n         displayName: \"PyTest (coverage)\"\n-        condition: eq(variables['RUN_COVERAGE'], 'yes')\n+        condition: eq(variables['TEST_TYPE'], 'coverage')\n+\n+      - script: |\n+          pytest --strict-warnings\n+        displayName: \"PyTest (treat warnings as errors)\"\n+        condition: eq(variables['TEST_TYPE'], 'strict-warning')\n \n       - task: PublishCodeCoverageResults@1\n         inputs:\n           codeCoverageTool: Cobertura\n           summaryFileLocation: \"test-data/coverage.xml\"\n-        condition: eq(variables['RUN_COVERAGE'], 'yes')\n+        condition: eq(variables['TEST_TYPE'], 'coverage')\n \n       - task: PublishTestResults@2\n         condition: succeededOrFailed()\n@@ -77,16 +97,16 @@ jobs:\n \n       - script: bash <(curl -s https://codecov.io/bash)\n         displayName: \"Upload to codecov.io\"\n-        condition: eq(variables['RUN_COVERAGE'], 'yes')\n+        condition: eq(variables['TEST_TYPE'], 'coverage')\n \n   - job: CheckBuild\n     pool:\n       vmImage: \"ubuntu-22.04\"\n     steps:\n       - task: UsePythonVersion@0\n         inputs:\n-          versionSpec: \"3.11\"\n-        displayName: \"Use Python 3.11\"\n+          versionSpec: \"3.12\"\n+        displayName: \"Use Python 3.12\"\n \n       - script: |\n           python -m pip install --upgrade pip"
        },
        {
            "sha": "68cc92f2dd37b59397fcbc6936e10bd482c423d7",
            "filename": ".codecov.yml",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/scverse/anndata/blob/f88f7bd4250b963752d615e491b7e676ce5eb7f0/.codecov.yml",
            "raw_url": "https://github.com/scverse/anndata/raw/f88f7bd4250b963752d615e491b7e676ce5eb7f0/.codecov.yml",
            "contents_url": "https://api.github.com/repos/scverse/anndata/contents/.codecov.yml?ref=f88f7bd4250b963752d615e491b7e676ce5eb7f0",
            "patch": "@@ -8,7 +8,6 @@ coverage:\n       default:\n         # Require 1% coverage, i.e., always succeed\n         target: 1\n-    patch: false\n     changes: false\n \n comment:"
        },
        {
            "sha": "e28d0778a893d8168079fc22c29921434fa1860d",
            "filename": ".github/ISSUE_TEMPLATE/bug-report.yml",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/scverse/anndata/blob/f88f7bd4250b963752d615e491b7e676ce5eb7f0/.github%2FISSUE_TEMPLATE%2Fbug-report.yml",
            "raw_url": "https://github.com/scverse/anndata/raw/f88f7bd4250b963752d615e491b7e676ce5eb7f0/.github%2FISSUE_TEMPLATE%2Fbug-report.yml",
            "contents_url": "https://api.github.com/repos/scverse/anndata/contents/.github%2FISSUE_TEMPLATE%2Fbug-report.yml?ref=f88f7bd4250b963752d615e491b7e676ce5eb7f0",
            "patch": "@@ -56,6 +56,6 @@ body:\n         ```python\n         >>> import anndata, session_info; session_info.show(html=False, dependencies=True)\n         ```\n-        render: python\n+      render: python\n     validations:\n       required: true"
        },
        {
            "sha": "e524628a5279a10ce5bdb34baa2a8db25738cd04",
            "filename": ".github/ISSUE_TEMPLATE/config.yml",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/scverse/anndata/blob/f88f7bd4250b963752d615e491b7e676ce5eb7f0/.github%2FISSUE_TEMPLATE%2Fconfig.yml",
            "raw_url": "https://github.com/scverse/anndata/raw/f88f7bd4250b963752d615e491b7e676ce5eb7f0/.github%2FISSUE_TEMPLATE%2Fconfig.yml",
            "contents_url": "https://api.github.com/repos/scverse/anndata/contents/.github%2FISSUE_TEMPLATE%2Fconfig.yml?ref=f88f7bd4250b963752d615e491b7e676ce5eb7f0",
            "patch": "@@ -1,5 +1,8 @@\n-blank_issues_enabled: false\n+blank_issues_enabled: true\n contact_links:\n   - name: Scverse Community Forum\n     url: https://discourse.scverse.org/\n     about: If you have questions about “How to do X”, please ask them here.\n+  - name: Blank issue\n+    url: https://github.com/scverse/anndata/issues/new\n+    about: For things that don't quite fit elsewhere. Please note that other templates should be used in most cases – this is mainly for use by the developers."
        },
        {
            "sha": "f46beb0949ae9ecc9c74675eafc0f8c7097fdd3b",
            "filename": ".github/workflows/benchmark.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/scverse/anndata/blob/f88f7bd4250b963752d615e491b7e676ce5eb7f0/.github%2Fworkflows%2Fbenchmark.yml",
            "raw_url": "https://github.com/scverse/anndata/raw/f88f7bd4250b963752d615e491b7e676ce5eb7f0/.github%2Fworkflows%2Fbenchmark.yml",
            "contents_url": "https://api.github.com/repos/scverse/anndata/contents/.github%2Fworkflows%2Fbenchmark.yml?ref=f88f7bd4250b963752d615e491b7e676ce5eb7f0",
            "patch": "@@ -2,7 +2,7 @@ name: Benchmark\n \n on:\n   push:\n-    branches: [main]\n+    branches: [main, \"[0-9]+.[0-9]+.x\"]\n   pull_request:\n     branches: [main]\n \n@@ -16,7 +16,7 @@ jobs:\n     strategy:\n       fail-fast: false\n       matrix:\n-        python: [\"3.11\"]\n+        python: [\"3.12\"]\n         os: [ubuntu-latest]\n \n     env:"
        },
        {
            "sha": "c67973007d9cf05040e00e750d6d7c3e2d49ea1c",
            "filename": ".github/workflows/publish.yml",
            "status": "added",
            "additions": 21,
            "deletions": 0,
            "changes": 21,
            "blob_url": "https://github.com/scverse/anndata/blob/f88f7bd4250b963752d615e491b7e676ce5eb7f0/.github%2Fworkflows%2Fpublish.yml",
            "raw_url": "https://github.com/scverse/anndata/raw/f88f7bd4250b963752d615e491b7e676ce5eb7f0/.github%2Fworkflows%2Fpublish.yml",
            "contents_url": "https://api.github.com/repos/scverse/anndata/contents/.github%2Fworkflows%2Fpublish.yml?ref=f88f7bd4250b963752d615e491b7e676ce5eb7f0",
            "patch": "@@ -0,0 +1,21 @@\n+name: Publish Python Package\n+\n+on:\n+  release:\n+    types: [published]\n+\n+jobs:\n+  publish:\n+    runs-on: ubuntu-latest\n+    environment: pypi\n+    permissions:\n+      id-token: write # to authenticate as Trusted Publisher to pypi.org\n+    steps:\n+      - uses: actions/checkout@v4\n+      - uses: actions/setup-python@v4\n+        with:\n+          python-version: \"3.x\"\n+          cache: \"pip\"\n+      - run: pip install build\n+      - run: python -m build\n+      - uses: pypa/gh-action-pypi-publish@release/v1"
        },
        {
            "sha": "8b7099f76c4b110d390d97e2b3d95ea0f3c66e54",
            "filename": ".github/workflows/test-gpu.yml",
            "status": "modified",
            "additions": 11,
            "deletions": 11,
            "changes": 22,
            "blob_url": "https://github.com/scverse/anndata/blob/f88f7bd4250b963752d615e491b7e676ce5eb7f0/.github%2Fworkflows%2Ftest-gpu.yml",
            "raw_url": "https://github.com/scverse/anndata/raw/f88f7bd4250b963752d615e491b7e676ce5eb7f0/.github%2Fworkflows%2Ftest-gpu.yml",
            "contents_url": "https://api.github.com/repos/scverse/anndata/contents/.github%2Fworkflows%2Ftest-gpu.yml?ref=f88f7bd4250b963752d615e491b7e676ce5eb7f0",
            "patch": "@@ -2,7 +2,7 @@ name: AWS GPU\n \n on:\n   push:\n-    branches: [main]\n+    branches: [main, \"[0-9]+.[0-9]+.x\"]\n   pull_request:\n     types:\n       - labeled\n@@ -35,9 +35,14 @@ jobs:\n     name: GPU Tests\n     needs: check\n     runs-on: \"cirun-aws-gpu--${{ github.run_id }}\"\n+    # Setting a timeout of 30 minutes, as the AWS costs money\n+    # At time of writing, a typical run takes about 5 minutes\n+    timeout-minutes: 30\n+\n     defaults:\n       run:\n         shell: bash -el {0}\n+\n     steps:\n       - uses: actions/checkout@v3\n         with:\n@@ -49,23 +54,18 @@ jobs:\n       - uses: mamba-org/setup-micromamba@v1\n         with:\n           micromamba-version: \"1.3.1-0\"\n-          environment-name: anndata-gpu-ci\n-          create-args: >-\n-            python=3.11\n-            cupy\n-            numba\n-            pytest\n-            pytest-cov\n-            pytest-xdist\n+          environment-file: ci/gpu_ci.yml\n           init-shell: >-\n             bash\n           generate-run-shell: false\n \n       - name: Install AnnData\n         run: pip install .[dev,test,gpu]\n \n-      - name: Mamba list\n-        run: micromamba list\n+      - name: Env list\n+        run: |\n+          micromamba list\n+          pip list\n \n       - name: Run test\n         run: pytest -m gpu --cov --cov-report=xml --cov-context=test -n 4"
        },
        {
            "sha": "88f0e90c34f9c8176a5c7e3d5bf2cbf47384225e",
            "filename": ".gitignore",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/scverse/anndata/blob/f88f7bd4250b963752d615e491b7e676ce5eb7f0/.gitignore",
            "raw_url": "https://github.com/scverse/anndata/raw/f88f7bd4250b963752d615e491b7e676ce5eb7f0/.gitignore",
            "contents_url": "https://api.github.com/repos/scverse/anndata/contents/.gitignore?ref=f88f7bd4250b963752d615e491b7e676ce5eb7f0",
            "patch": "@@ -5,6 +5,7 @@\n # Caches for compiled and downloaded files\n __pycache__/\n /*cache/\n+/node_modules/\n /data/\n \n # Distribution / packaging"
        },
        {
            "sha": "064bdc00f49ed4e2c86a2684969faa174b92139b",
            "filename": ".pre-commit-config.yaml",
            "status": "modified",
            "additions": 8,
            "deletions": 9,
            "changes": 17,
            "blob_url": "https://github.com/scverse/anndata/blob/f88f7bd4250b963752d615e491b7e676ce5eb7f0/.pre-commit-config.yaml",
            "raw_url": "https://github.com/scverse/anndata/raw/f88f7bd4250b963752d615e491b7e676ce5eb7f0/.pre-commit-config.yaml",
            "contents_url": "https://api.github.com/repos/scverse/anndata/contents/.pre-commit-config.yaml?ref=f88f7bd4250b963752d615e491b7e676ce5eb7f0",
            "patch": "@@ -1,20 +1,20 @@\n repos:\n-  - repo: https://github.com/psf/black\n-    rev: 23.9.1\n-    hooks:\n-      - id: black\n   - repo: https://github.com/astral-sh/ruff-pre-commit\n-    # Ruff version.\n-    rev: \"v0.0.292\"\n+    rev: v0.2.2\n     hooks:\n       - id: ruff\n+        types_or: [python, pyi, jupyter]\n         args: [\"--fix\"]\n+      - id: ruff-format\n+        types_or: [python, pyi, jupyter]\n   - repo: https://github.com/pre-commit/mirrors-prettier\n-    rev: v3.0.3\n+    rev: v4.0.0-alpha.8\n     hooks:\n       - id: prettier\n+        exclude_types:\n+          - markdown\n   - repo: https://github.com/pre-commit/pre-commit-hooks\n-    rev: v4.4.0\n+    rev: v4.5.0\n     hooks:\n       - id: trailing-whitespace\n       - id: end-of-file-fixer\n@@ -26,7 +26,6 @@ repos:\n       - id: detect-private-key\n       - id: no-commit-to-branch\n         args: [\"--branch=main\"]\n-\n   - repo: https://github.com/codespell-project/codespell\n     rev: v2.2.6\n     hooks:"
        },
        {
            "sha": "764eb57bd983028bf92bb81118a0f7823de5d0f8",
            "filename": ".readthedocs.yml",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/scverse/anndata/blob/f88f7bd4250b963752d615e491b7e676ce5eb7f0/.readthedocs.yml",
            "raw_url": "https://github.com/scverse/anndata/raw/f88f7bd4250b963752d615e491b7e676ce5eb7f0/.readthedocs.yml",
            "contents_url": "https://api.github.com/repos/scverse/anndata/contents/.readthedocs.yml?ref=f88f7bd4250b963752d615e491b7e676ce5eb7f0",
            "patch": "@@ -2,7 +2,7 @@ version: 2\n build:\n   os: ubuntu-20.04\n   tools:\n-    python: \"3.11\"\n+    python: \"3.12\"\n sphinx:\n   configuration: docs/conf.py\n   fail_on_warning: true # do not change or you will be fired"
        },
        {
            "sha": "6cae971ac4ce9d5a4b413f71f5fbdaed723b2a64",
            "filename": "anndata/__init__.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/scverse/anndata/blob/f88f7bd4250b963752d615e491b7e676ce5eb7f0/anndata%2F__init__.py",
            "raw_url": "https://github.com/scverse/anndata/raw/f88f7bd4250b963752d615e491b7e676ce5eb7f0/anndata%2F__init__.py",
            "contents_url": "https://api.github.com/repos/scverse/anndata/contents/anndata%2F__init__.py?ref=f88f7bd4250b963752d615e491b7e676ce5eb7f0",
            "patch": "@@ -34,6 +34,7 @@\n     read_umi_tools,\n     read_zarr,\n )\n+from ._settings import settings\n from ._warnings import (\n     ExperimentalFeatureWarning,\n     ImplicitModificationWarning,\n@@ -75,4 +76,5 @@ def read(*args, **kwargs):\n     \"ImplicitModificationWarning\",\n     \"ExperimentalFeatureWarning\",\n     \"experimental\",\n+    \"settings\",\n ]"
        },
        {
            "sha": "f57dfb272306352b2b12c9a098024ea7b9a263fc",
            "filename": "anndata/_core/aligned_mapping.py",
            "status": "modified",
            "additions": 30,
            "deletions": 30,
            "changes": 60,
            "blob_url": "https://github.com/scverse/anndata/blob/f88f7bd4250b963752d615e491b7e676ce5eb7f0/anndata%2F_core%2Faligned_mapping.py",
            "raw_url": "https://github.com/scverse/anndata/raw/f88f7bd4250b963752d615e491b7e676ce5eb7f0/anndata%2F_core%2Faligned_mapping.py",
            "contents_url": "https://api.github.com/repos/scverse/anndata/contents/anndata%2F_core%2Faligned_mapping.py?ref=f88f7bd4250b963752d615e491b7e676ce5eb7f0",
            "patch": "@@ -8,6 +8,7 @@\n from typing import (\n     TYPE_CHECKING,\n     ClassVar,\n+    Literal,\n     TypeVar,\n     Union,\n )\n@@ -19,7 +20,7 @@\n from anndata._warnings import ExperimentalFeatureWarning, ImplicitModificationWarning\n from anndata.compat import AwkArray\n \n-from ..utils import deprecated, dim_len, ensure_df_homogeneous\n+from ..utils import deprecated, dim_len, ensure_df_homogeneous, warn_once\n from .access import ElementRef\n from .index import _subset\n from .views import as_view, view_update\n@@ -61,35 +62,31 @@ def _ipython_key_completions_(self) -> list[str]:\n     def _validate_value(self, val: V, key: str) -> V:\n         \"\"\"Raises an error if value is invalid\"\"\"\n         if isinstance(val, AwkArray):\n-            warnings.warn(\n+            warn_once(\n                 \"Support for Awkward Arrays is currently experimental. \"\n                 \"Behavior may change in the future. Please report any issues you may encounter!\",\n                 ExperimentalFeatureWarning,\n                 # stacklevel=3,\n             )\n-            # Prevent from showing up every time an awkward array is used\n-            # You'd think `once` works, but it doesn't at the repl and in notebooks\n-            warnings.filterwarnings(\n-                \"ignore\",\n-                category=ExperimentalFeatureWarning,\n-                message=\"Support for Awkward Arrays is currently experimental.*\",\n-            )\n         for i, axis in enumerate(self.axes):\n-            if self.parent.shape[axis] != dim_len(val, i):\n-                right_shape = tuple(self.parent.shape[a] for a in self.axes)\n-                actual_shape = tuple(dim_len(val, a) for a, _ in enumerate(self.axes))\n-                if actual_shape[i] is None and isinstance(val, AwkArray):\n-                    raise ValueError(\n-                        f\"The AwkwardArray is of variable length in dimension {i}.\",\n-                        f\"Try ak.to_regular(array, {i}) before including the array in AnnData\",\n-                    )\n-                else:\n-                    raise ValueError(\n-                        f\"Value passed for key {key!r} is of incorrect shape. \"\n-                        f\"Values of {self.attrname} must match dimensions \"\n-                        f\"{self.axes} of parent. Value had shape {actual_shape} while \"\n-                        f\"it should have had {right_shape}.\"\n-                    )\n+            if self.parent.shape[axis] == dim_len(val, i):\n+                continue\n+            right_shape = tuple(self.parent.shape[a] for a in self.axes)\n+            actual_shape = tuple(dim_len(val, a) for a, _ in enumerate(self.axes))\n+            if actual_shape[i] is None and isinstance(val, AwkArray):\n+                dim = (\"obs\", \"var\")[i]\n+                msg = (\n+                    f\"The AwkwardArray is of variable length in dimension {dim}.\",\n+                    f\"Try ak.to_regular(array, {i}) before including the array in AnnData\",\n+                )\n+            else:\n+                dims = tuple((\"obs\", \"var\")[ax] for ax in self.axes)\n+                msg = (\n+                    f\"Value passed for key {key!r} is of incorrect shape. \"\n+                    f\"Values of {self.attrname} must match dimensions {dims} of parent. \"\n+                    f\"Value had shape {actual_shape} while it should have had {right_shape}.\"\n+                )\n+            raise ValueError(msg)\n \n         if not self._allow_df and isinstance(val, pd.DataFrame):\n             name = self.attrname.title().rstrip(\"s\")\n@@ -104,7 +101,7 @@ def attrname(self) -> str:\n \n     @property\n     @abstractmethod\n-    def axes(self) -> tuple[int, ...]:\n+    def axes(self) -> tuple[Literal[0, 1], ...]:\n         \"\"\"Which axes of the parent is this aligned to?\"\"\"\n         pass\n \n@@ -131,7 +128,7 @@ def _view(self, parent: AnnData, subset_idx: I):\n         \"\"\"Returns a subset copy-on-write view of the object.\"\"\"\n         return self._view_class(self, parent, subset_idx)\n \n-    @deprecated(\"dict(obj)\")\n+    @deprecated(\"dict(obj)\", FutureWarning)\n     def as_dict(self) -> dict:\n         return dict(self)\n \n@@ -166,7 +163,10 @@ def __setitem__(self, key: str, value: V):\n             new_mapping[key] = value\n \n     def __delitem__(self, key: str):\n-        _ = key in self  # Make sure it exists before bothering with a copy\n+        if key not in self:\n+            raise KeyError(\n+                \"'{key!r}' not found in view of {self.attrname}\"\n+            )  # Make sure it exists before bothering with a copy\n         warnings.warn(\n             f\"Removing element `.{self.attrname}['{key}']` of view, \"\n             \"initializing view as actual.\",\n@@ -226,7 +226,7 @@ def attrname(self) -> str:\n         return f\"{self.dim}m\"\n \n     @property\n-    def axes(self) -> tuple[int]:\n+    def axes(self) -> tuple[Literal[0, 1]]:\n         \"\"\"Axes of the parent this is aligned to\"\"\"\n         return (self._axis,)\n \n@@ -260,7 +260,7 @@ def _validate_value(self, val: V, key: str) -> V:\n             try:\n                 pd.testing.assert_index_equal(val.index, self.dim_names)\n             except AssertionError as e:\n-                msg = f\"value.index does not match parent’s axis {self.axes[0]} names:\\n{e}\"\n+                msg = f\"value.index does not match parent’s {self.dim} names:\\n{e}\"\n                 raise ValueError(msg) from None\n             else:\n                 msg = \"Index.equals and pd.testing.assert_index_equal disagree\"\n@@ -361,7 +361,7 @@ def attrname(self) -> str:\n         return f\"{self.dim}p\"\n \n     @property\n-    def axes(self) -> tuple[int, int]:\n+    def axes(self) -> tuple[Literal[0], Literal[0]] | tuple[Literal[1], Literal[1]]:\n         \"\"\"Axes of the parent this is aligned to\"\"\"\n         return self._axis, self._axis\n "
        },
        {
            "sha": "7d4f2e573ed06078a899cb1ec38770ec1e99737e",
            "filename": "anndata/_core/anndata.py",
            "status": "modified",
            "additions": 42,
            "deletions": 32,
            "changes": 74,
            "blob_url": "https://github.com/scverse/anndata/blob/f88f7bd4250b963752d615e491b7e676ce5eb7f0/anndata%2F_core%2Fanndata.py",
            "raw_url": "https://github.com/scverse/anndata/raw/f88f7bd4250b963752d615e491b7e676ce5eb7f0/anndata%2F_core%2Fanndata.py",
            "contents_url": "https://api.github.com/repos/scverse/anndata/contents/anndata%2F_core%2Fanndata.py?ref=f88f7bd4250b963752d615e491b7e676ce5eb7f0",
            "patch": "@@ -25,11 +25,12 @@\n from numpy import ma\n from pandas.api.types import infer_dtype, is_string_dtype\n from scipy import sparse\n-from scipy.sparse import csr_matrix, issparse\n+from scipy.sparse import issparse\n \n from anndata._warnings import ImplicitModificationWarning\n \n from .. import utils\n+from .._settings import settings\n from ..compat import (\n     CupyArray,\n     CupySparseMatrix,\n@@ -39,7 +40,7 @@\n     _move_adj_mtx,\n )\n from ..logging import anndata_logger as logger\n-from ..utils import convert_to_dict, dim_len, ensure_df_homogeneous\n+from ..utils import convert_to_dict, deprecated, dim_len, ensure_df_homogeneous\n from .access import ElementRef\n from .aligned_mapping import (\n     AxisArrays,\n@@ -74,7 +75,7 @@ class StorageType(Enum):\n     DaskArray = DaskArray\n     CupyArray = CupyArray\n     CupySparseMatrix = CupySparseMatrix\n-    BackedSparseMAtrix = BaseCompressedSparseDataset\n+    BackedSparseMatrix = BaseCompressedSparseDataset\n \n     @classmethod\n     def classes(cls):\n@@ -413,8 +414,9 @@ def _init_as_view(self, adata_ref: AnnData, oidx: Index, vidx: Index):\n         self._varp = adata_ref.varp._view(self, vidx)\n         # fix categories\n         uns = copy(adata_ref._uns)\n-        self._remove_unused_categories(adata_ref.obs, obs_sub, uns)\n-        self._remove_unused_categories(adata_ref.var, var_sub, uns)\n+        if settings.remove_unused_categories:\n+            self._remove_unused_categories(adata_ref.obs, obs_sub, uns)\n+            self._remove_unused_categories(adata_ref.var, var_sub, uns)\n         # set attributes\n         self._obs = DataFrameView(obs_sub, view_args=(self, \"obs\"))\n         self._var = DataFrameView(var_sub, view_args=(self, \"var\"))\n@@ -592,28 +594,37 @@ def _init_as_actual(\n         # layers\n         self._layers = Layers(self, layers)\n \n-    def __sizeof__(self, show_stratified=None) -> int:\n-        def get_size(X):\n-            if issparse(X):\n-                X_csr = csr_matrix(X)\n-                return X_csr.data.nbytes + X_csr.indptr.nbytes + X_csr.indices.nbytes\n+    def __sizeof__(self, show_stratified=None, with_disk: bool = False) -> int:\n+        def get_size(X) -> int:\n+            def cs_to_bytes(X) -> int:\n+                return int(X.data.nbytes + X.indptr.nbytes + X.indices.nbytes)\n+\n+            if isinstance(X, h5py.Dataset) and with_disk:\n+                return int(np.array(X.shape).prod() * X.dtype.itemsize)\n+            elif isinstance(X, BaseCompressedSparseDataset) and with_disk:\n+                return cs_to_bytes(X._to_backed())\n+            elif isinstance(X, (sparse.csr_matrix, sparse.csc_matrix)):\n+                return cs_to_bytes(X)\n             else:\n                 return X.__sizeof__()\n \n-        size = 0\n-        attrs = list([\"_X\", \"_obs\", \"_var\"])\n-        attrs_multi = list([\"_uns\", \"_obsm\", \"_varm\", \"varp\", \"_obsp\", \"_layers\"])\n+        sizes = {}\n+        attrs = [\"X\", \"_obs\", \"_var\"]\n+        attrs_multi = [\"_uns\", \"_obsm\", \"_varm\", \"varp\", \"_obsp\", \"_layers\"]\n         for attr in attrs + attrs_multi:\n             if attr in attrs_multi:\n                 keys = getattr(self, attr).keys()\n-                s = sum([get_size(getattr(self, attr)[k]) for k in keys])\n+                s = sum(get_size(getattr(self, attr)[k]) for k in keys)\n             else:\n                 s = get_size(getattr(self, attr))\n             if s > 0 and show_stratified:\n-                str_attr = attr.replace(\"_\", \".\") + \" \" * (7 - len(attr))\n-                print(f\"Size of {str_attr}: {'%3.2f' % (s / (1024 ** 2))} MB\")\n-            size += s\n-        return size\n+                from tqdm import tqdm\n+\n+                print(\n+                    f\"Size of {attr.replace('_', '.'):<7}: {tqdm.format_sizeof(s, 'B')}\"\n+                )\n+            sizes[attr] = s\n+        return sum(sizes.values())\n \n     def _gen_repr(self, n_obs, n_vars) -> str:\n         if self.isbacked:\n@@ -875,23 +886,21 @@ def _prep_dim_index(self, value, attr: str) -> pd.Index:\n             value = pd.Index(value)\n             if not isinstance(value.name, (str, type(None))):\n                 value.name = None\n-        # fmt: off\n         if (\n-            not isinstance(value, pd.RangeIndex)\n+            len(value) > 0\n+            and not isinstance(value, pd.RangeIndex)\n             and infer_dtype(value) not in (\"string\", \"bytes\")\n         ):\n             sample = list(value[: min(len(value), 5)])\n-            warnings.warn(dedent(\n+            msg = dedent(\n                 f\"\"\"\n                 AnnData expects .{attr}.index to contain strings, but got values like:\n                     {sample}\n \n                     Inferred to be: {infer_dtype(value)}\n                 \"\"\"\n-                ), # noqa\n-                stacklevel=2,\n             )\n-        # fmt: on\n+            warnings.warn(msg, stacklevel=2)\n         return value\n \n     def _set_dim_index(self, value: pd.Index, attr: str):\n@@ -1303,6 +1312,7 @@ def _inplace_subset_var(self, index: Index1D):\n         Same as `adata = adata[:, index]`, but inplace.\n         \"\"\"\n         adata_subset = self[:, index].copy()\n+\n         self._init_as_actual(adata_subset)\n \n     def _inplace_subset_obs(self, index: Index1D):\n@@ -1312,6 +1322,7 @@ def _inplace_subset_obs(self, index: Index1D):\n         Same as `adata = adata[index, :]`, but inplace.\n         \"\"\"\n         adata_subset = self[index].copy()\n+\n         self._init_as_actual(adata_subset)\n \n     # TODO: Update, possibly remove\n@@ -1597,6 +1608,13 @@ def copy(self, filename: PathLike | None = None) -> AnnData:\n             write_h5ad(filename, self)\n             return read_h5ad(filename, backed=mode)\n \n+    @deprecated(\n+        \"anndata.concat\",\n+        FutureWarning,\n+        \"See the tutorial for concat at: \"\n+        \"https://anndata.readthedocs.io/en/latest/concatenation.html\",\n+        hide=False,\n+    )\n     def concatenate(\n         self,\n         *adatas: AnnData,\n@@ -1820,14 +1838,6 @@ def concatenate(\n         \"\"\"\n         from .merge import concat, merge_dataframes, merge_outer, merge_same\n \n-        warnings.warn(\n-            \"The AnnData.concatenate method is deprecated in favour of the \"\n-            \"anndata.concat function. Please use anndata.concat instead.\\n\\n\"\n-            \"See the tutorial for concat at: \"\n-            \"https://anndata.readthedocs.io/en/latest/concatenation.html\",\n-            FutureWarning,\n-        )\n-\n         if self.isbacked:\n             raise ValueError(\"Currently, concatenate only works in memory mode.\")\n "
        },
        {
            "sha": "bdf1bfe27caf9c8a8dbe78acb263af0776e86645",
            "filename": "anndata/_core/index.py",
            "status": "modified",
            "additions": 12,
            "deletions": 6,
            "changes": 18,
            "blob_url": "https://github.com/scverse/anndata/blob/f88f7bd4250b963752d615e491b7e676ce5eb7f0/anndata%2F_core%2Findex.py",
            "raw_url": "https://github.com/scverse/anndata/raw/f88f7bd4250b963752d615e491b7e676ce5eb7f0/anndata%2F_core%2Findex.py",
            "contents_url": "https://api.github.com/repos/scverse/anndata/contents/anndata%2F_core%2Findex.py?ref=f88f7bd4250b963752d615e491b7e676ce5eb7f0",
            "patch": "@@ -42,11 +42,11 @@ def _normalize_index(\n     | np.integer\n     | int\n     | str\n-    | Sequence[int | np.integer]\n+    | Sequence[bool | int | np.integer]\n     | np.ndarray\n     | pd.Index,\n     index: pd.Index,\n-) -> slice | int | np.ndarray:  # ndarray of int\n+) -> slice | int | np.ndarray:  # ndarray of int or bool\n     if not isinstance(index, pd.RangeIndex):\n         assert (\n             index.dtype != float and index.dtype != int\n@@ -81,6 +81,8 @@ def name_idx(i):\n             indexer = np.ravel(indexer)\n         if not isinstance(indexer, (np.ndarray, pd.Index)):\n             indexer = np.array(indexer)\n+            if len(indexer) == 0:\n+                indexer = indexer.astype(int)\n         if issubclass(indexer.dtype.type, (np.integer, np.floating)):\n             return indexer  # Might not work for range indexes\n         elif issubclass(indexer.dtype.type, np.bool_):\n@@ -90,8 +92,7 @@ def name_idx(i):\n                     f\"dimension. Boolean index has shape {indexer.shape} while \"\n                     f\"AnnData index has shape {index.shape}.\"\n                 )\n-            positions = np.where(indexer)[0]\n-            return positions  # np.ndarray[int]\n+            return indexer\n         else:  # indexer should be string array\n             positions = index.get_indexer(indexer)\n             if np.any(positions < 0):\n@@ -162,7 +163,10 @@ def _subset_dask(a: DaskArray, subset_idx: Index):\n def _subset_spmatrix(a: spmatrix, subset_idx: Index):\n     # Correcting for indexing behaviour of sparse.spmatrix\n     if len(subset_idx) > 1 and all(isinstance(x, cabc.Iterable) for x in subset_idx):\n-        subset_idx = (subset_idx[0].reshape(-1, 1), *subset_idx[1:])\n+        first_idx = subset_idx[0]\n+        if issubclass(first_idx.dtype.type, np.bool_):\n+            first_idx = np.where(first_idx)[0]\n+        subset_idx = (first_idx.reshape(-1, 1), *subset_idx[1:])\n     return a[subset_idx]\n \n \n@@ -186,7 +190,9 @@ def _subset_dataset(d, subset_idx):\n     ordered = list(subset_idx)\n     rev_order = [slice(None) for _ in range(len(subset_idx))]\n     for axis, axis_idx in enumerate(ordered.copy()):\n-        if isinstance(axis_idx, np.ndarray) and axis_idx.dtype.type != bool:\n+        if isinstance(axis_idx, np.ndarray):\n+            if axis_idx.dtype == bool:\n+                axis_idx = np.where(axis_idx)[0]\n             order = np.argsort(axis_idx)\n             ordered[axis] = axis_idx[order]\n             rev_order[axis] = np.argsort(order)"
        },
        {
            "sha": "48f36be9d84098f81f0c9e6e8c56bb2016825b24",
            "filename": "anndata/_core/merge.py",
            "status": "modified",
            "additions": 68,
            "deletions": 51,
            "changes": 119,
            "blob_url": "https://github.com/scverse/anndata/blob/f88f7bd4250b963752d615e491b7e676ce5eb7f0/anndata%2F_core%2Fmerge.py",
            "raw_url": "https://github.com/scverse/anndata/raw/f88f7bd4250b963752d615e491b7e676ce5eb7f0/anndata%2F_core%2Fmerge.py",
            "contents_url": "https://api.github.com/repos/scverse/anndata/contents/anndata%2F_core%2Fmerge.py?ref=f88f7bd4250b963752d615e491b7e676ce5eb7f0",
            "patch": "@@ -13,11 +13,11 @@\n     MutableSet,\n     Sequence,\n )\n-from functools import reduce, singledispatch\n+from functools import partial, reduce, singledispatch\n from itertools import repeat\n from operator import and_, or_, sub\n from typing import Any, Literal, TypeVar\n-from warnings import filterwarnings, warn\n+from warnings import warn\n \n import numpy as np\n import pandas as pd\n@@ -27,8 +27,15 @@\n \n from anndata._warnings import ExperimentalFeatureWarning\n \n-from ..compat import AwkArray, CupyArray, CupyCSRMatrix, CupySparseMatrix, DaskArray\n-from ..utils import asarray, dim_len\n+from ..compat import (\n+    AwkArray,\n+    CupyArray,\n+    CupyCSRMatrix,\n+    CupySparseMatrix,\n+    DaskArray,\n+    _map_cat_to_str,\n+)\n+from ..utils import asarray, dim_len, warn_once\n from .anndata import AnnData\n from .index import _subset, make_slice\n \n@@ -134,7 +141,13 @@ def equal_dask_array(a, b) -> bool:\n \n @equal.register(np.ndarray)\n def equal_array(a, b) -> bool:\n-    return equal(pd.DataFrame(a), pd.DataFrame(asarray(b)))\n+    # Reshaping allows us to compare inputs with >2 dimensions\n+    # We cast to pandas since it will still work with non-numeric types\n+    b = asarray(b)\n+    if a.shape != b.shape:\n+        return False\n+\n+    return equal(pd.DataFrame(a.reshape(-1)), pd.DataFrame(b.reshape(-1)))\n \n \n @equal.register(CupyArray)\n@@ -212,6 +225,7 @@ def unify_dtypes(dfs: Iterable[pd.DataFrame]) -> list[pd.DataFrame]:\n \n     For catching cases where pandas would convert to object dtype.\n     \"\"\"\n+    dfs = list(dfs)\n     # Get shared categorical columns\n     df_dtypes = [dict(df.dtypes) for df in dfs]\n     columns = reduce(lambda x, y: x.union(y), [df.columns for df in dfs])\n@@ -264,7 +278,7 @@ def try_unifying_dtype(\n                 dtypes.add(dtype)\n                 ordered = ordered | dtype.ordered\n             elif not pd.isnull(dtype):\n-                return False\n+                return None\n         if len(dtypes) > 0 and not ordered:\n             categories = reduce(\n                 lambda x, y: x.union(y),\n@@ -745,9 +759,9 @@ def concat_arrays(arrays, reindexers, axis=0, index=None, fill_value=None):\n             )\n         # TODO: behaviour here should be chosen through a merge strategy\n         df = pd.concat(\n-            unify_dtypes([f(x) for f, x in zip(reindexers, arrays)]),\n-            ignore_index=True,\n+            unify_dtypes(f(x) for f, x in zip(reindexers, arrays)),\n             axis=axis,\n+            ignore_index=True,\n         )\n         df.index = index\n         return df\n@@ -812,7 +826,7 @@ def concat_arrays(arrays, reindexers, axis=0, index=None, fill_value=None):\n         )\n \n \n-def inner_concat_aligned_mapping(mappings, reindexers=None, index=None, axis=0):\n+def inner_concat_aligned_mapping(mappings, *, reindexers=None, index=None, axis=0):\n     result = {}\n \n     for k in intersect_keys(mappings):\n@@ -871,17 +885,12 @@ def gen_outer_reindexers(els, shapes, new_index: pd.Index, *, axis=0):\n             raise NotImplementedError(\n                 \"Cannot concatenate an AwkwardArray with other array types.\"\n             )\n-        warn(\n-            \"Outer joins on awkward.Arrays will have different return values in the future.\"\n+        warn_once(\n+            \"Outer joins on awkward.Arrays will have different return values in the future. \"\n             \"For details, and to offer input, please see:\\n\\n\\t\"\n             \"https://github.com/scverse/anndata/issues/898\",\n             ExperimentalFeatureWarning,\n         )\n-        filterwarnings(\n-            \"ignore\",\n-            category=ExperimentalFeatureWarning,\n-            message=r\"Outer joins on awkward.Arrays will have different return values.*\",\n-        )\n         # all_keys = union_keys(el.fields for el in els if not_missing(el))\n         reindexers = []\n         for el in els:\n@@ -905,11 +914,18 @@ def gen_outer_reindexers(els, shapes, new_index: pd.Index, *, axis=0):\n \n \n def outer_concat_aligned_mapping(\n-    mappings, reindexers=None, index=None, fill_value=None, axis=0\n+    mappings, *, reindexers=None, index=None, axis=0, fill_value=None\n ):\n     result = {}\n     ns = [m.parent.shape[axis] for m in mappings]\n \n+    def missing_element(n: int, axis: Literal[0, 1] = 0) -> np.ndarray:\n+        \"\"\"Generates value to use when there is a missing element.\"\"\"\n+        if axis == 0:\n+            return np.zeros((n, 0), dtype=bool)\n+        else:\n+            return np.zeros((0, n), dtype=bool)\n+\n     for k in union_keys(mappings):\n         els = [m.get(k, MissingVal) for m in mappings]\n         if reindexers is None:\n@@ -921,7 +937,7 @@ def outer_concat_aligned_mapping(\n         # We should probably just handle missing elements for all types\n         result[k] = concat_arrays(\n             [\n-                el if not_missing(el) else np.zeros((n, 0), dtype=bool)\n+                el if not_missing(el) else missing_element(n, axis=axis)\n                 for el, n in zip(els, ns)\n             ],\n             cur_reindexers,\n@@ -1105,12 +1121,18 @@ def concat(\n     ...     X=sparse.csr_matrix(np.array([[0, 1], [2, 3]])),\n     ...     obs=pd.DataFrame({\"group\": [\"a\", \"b\"]}, index=[\"s1\", \"s2\"]),\n     ...     var=pd.DataFrame(index=[\"var1\", \"var2\"]),\n-    ...     varm={\"ones\": np.ones((2, 5)), \"rand\": np.random.randn(2, 3), \"zeros\": np.zeros((2, 5))},\n+    ...     varm={\n+    ...         \"ones\": np.ones((2, 5)),\n+    ...         \"rand\": np.random.randn(2, 3),\n+    ...         \"zeros\": np.zeros((2, 5)),\n+    ...     },\n     ...     uns={\"a\": 1, \"b\": 2, \"c\": {\"c.a\": 3, \"c.b\": 4}},\n     ... )\n     >>> b = ad.AnnData(\n     ...     X=sparse.csr_matrix(np.array([[4, 5, 6], [7, 8, 9]])),\n-    ...     obs=pd.DataFrame({\"group\": [\"b\", \"c\"], \"measure\": [1.2, 4.3]}, index=[\"s3\", \"s4\"]),\n+    ...     obs=pd.DataFrame(\n+    ...         {\"group\": [\"b\", \"c\"], \"measure\": [1.2, 4.3]}, index=[\"s3\", \"s4\"]\n+    ...     ),\n     ...     var=pd.DataFrame(index=[\"var1\", \"var2\", \"var3\"]),\n     ...     varm={\"ones\": np.ones((3, 5)), \"rand\": np.random.randn(3, 5)},\n     ...     uns={\"a\": 1, \"b\": 3, \"c\": {\"c.b\": 4}},\n@@ -1144,7 +1166,7 @@ def concat(\n     >>> (inner.obs_names, inner.var_names)  # doctest: +NORMALIZE_WHITESPACE\n     (Index(['s1', 's2', 's3', 's4'], dtype='object'),\n     Index(['var1', 'var2'], dtype='object'))\n-    >>> outer = ad.concat([a, b], join=\"outer\") # Joining on union of variables\n+    >>> outer = ad.concat([a, b], join=\"outer\")  # Joining on union of variables\n     >>> outer\n     AnnData object with n_obs × n_vars = 4 × 3\n         obs: 'group', 'measure'\n@@ -1239,7 +1261,9 @@ def concat(\n         [pd.Series(dim_indices(a, axis=axis)) for a in adatas], ignore_index=True\n     )\n     if index_unique is not None:\n-        concat_indices = concat_indices.str.cat(label_col.map(str), sep=index_unique)\n+        concat_indices = concat_indices.str.cat(\n+            _map_cat_to_str(label_col), sep=index_unique\n+        )\n     concat_indices = pd.Index(concat_indices)\n \n     alt_indices = merge_indices(\n@@ -1252,7 +1276,7 @@ def concat(\n     # Annotation for concatenation axis\n     check_combinable_cols([getattr(a, dim).columns for a in adatas], join=join)\n     concat_annot = pd.concat(\n-        unify_dtypes([getattr(a, dim) for a in adatas]),\n+        unify_dtypes(getattr(a, dim) for a in adatas),\n         join=join,\n         ignore_index=True,\n     )\n@@ -1268,37 +1292,30 @@ def concat(\n     X = concat_Xs(adatas, reindexers, axis=axis, fill_value=fill_value)\n \n     if join == \"inner\":\n-        layers = inner_concat_aligned_mapping(\n-            [a.layers for a in adatas], axis=axis, reindexers=reindexers\n-        )\n-        concat_mapping = inner_concat_aligned_mapping(\n-            [getattr(a, f\"{dim}m\") for a in adatas], index=concat_indices\n-        )\n-        if pairwise:\n-            concat_pairwise = concat_pairwise_mapping(\n-                mappings=[getattr(a, f\"{dim}p\") for a in adatas],\n-                shapes=[a.shape[axis] for a in adatas],\n-                join_keys=intersect_keys,\n-            )\n-        else:\n-            concat_pairwise = {}\n+        concat_aligned_mapping = inner_concat_aligned_mapping\n+        join_keys = intersect_keys\n     elif join == \"outer\":\n-        layers = outer_concat_aligned_mapping(\n-            [a.layers for a in adatas], reindexers, axis=axis, fill_value=fill_value\n+        concat_aligned_mapping = partial(\n+            outer_concat_aligned_mapping, fill_value=fill_value\n         )\n-        concat_mapping = outer_concat_aligned_mapping(\n-            [getattr(a, f\"{dim}m\") for a in adatas],\n-            index=concat_indices,\n-            fill_value=fill_value,\n+        join_keys = union_keys\n+    else:\n+        assert False, f\"{join=} should have been validated above by pd.concat\"\n+\n+    layers = concat_aligned_mapping(\n+        [a.layers for a in adatas], axis=axis, reindexers=reindexers\n+    )\n+    concat_mapping = concat_aligned_mapping(\n+        [getattr(a, f\"{dim}m\") for a in adatas], index=concat_indices\n+    )\n+    if pairwise:\n+        concat_pairwise = concat_pairwise_mapping(\n+            mappings=[getattr(a, f\"{dim}p\") for a in adatas],\n+            shapes=[a.shape[axis] for a in adatas],\n+            join_keys=join_keys,\n         )\n-        if pairwise:\n-            concat_pairwise = concat_pairwise_mapping(\n-                mappings=[getattr(a, f\"{dim}p\") for a in adatas],\n-                shapes=[a.shape[axis] for a in adatas],\n-                join_keys=union_keys,\n-            )\n-        else:\n-            concat_pairwise = {}\n+    else:\n+        concat_pairwise = {}\n \n     # TODO: Reindex lazily, so we don't have to make those copies until we're sure we need the element\n     alt_mapping = merge("
        },
        {
            "sha": "6dfb89745d9b5fb7b7f83ea5745cb20f2d56aadb",
            "filename": "anndata/_core/sparse_dataset.py",
            "status": "modified",
            "additions": 133,
            "deletions": 19,
            "changes": 152,
            "blob_url": "https://github.com/scverse/anndata/blob/f88f7bd4250b963752d615e491b7e676ce5eb7f0/anndata%2F_core%2Fsparse_dataset.py",
            "raw_url": "https://github.com/scverse/anndata/raw/f88f7bd4250b963752d615e491b7e676ce5eb7f0/anndata%2F_core%2Fsparse_dataset.py",
            "contents_url": "https://api.github.com/repos/scverse/anndata/contents/anndata%2F_core%2Fsparse_dataset.py?ref=f88f7bd4250b963752d615e491b7e676ce5eb7f0",
            "patch": "@@ -15,7 +15,10 @@\n import collections.abc as cabc\n import warnings\n from abc import ABC\n+from functools import cached_property\n from itertools import accumulate, chain\n+from math import floor\n+from pathlib import Path\n from typing import TYPE_CHECKING, Literal, NamedTuple\n \n import h5py\n@@ -24,7 +27,7 @@\n from scipy.sparse import _sparsetools\n \n from anndata._core.index import _fix_slice_bounds\n-from anndata.compat import H5Group, ZarrGroup\n+from anndata.compat import H5Group, ZarrArray, ZarrGroup\n \n from ..compat import _read_attr\n \n@@ -39,6 +42,8 @@\n if TYPE_CHECKING:\n     from collections.abc import Iterable, Sequence\n \n+    from .._types import GroupStorageType\n+\n \n class BackedFormat(NamedTuple):\n     format: str\n@@ -57,8 +62,17 @@ class BackedSparseMatrix(_cs_matrix):\n     def copy(self) -> ss.spmatrix:\n         if isinstance(self.data, h5py.Dataset):\n             return sparse_dataset(self.data.parent).to_memory()\n-        else:\n-            return super().copy()\n+        if isinstance(self.data, ZarrArray):\n+            import zarr\n+\n+            return sparse_dataset(\n+                zarr.open(\n+                    store=self.data.store,\n+                    mode=\"r\",\n+                    chunk_store=self.data.chunk_store,  # chunk_store is needed, not clear why\n+                )[Path(self.data.path).parent]\n+            ).to_memory()\n+        return super().copy()\n \n     def _set_many(self, i: Iterable[int], j: Iterable[int], x):\n         \"\"\"\\\n@@ -127,7 +141,7 @@ def _offsets(\n     def _get_contiguous_compressed_slice(\n         self, s: slice\n     ) -> tuple[np.ndarray, np.ndarray, np.ndarray]:\n-        new_indptr = self.indptr[s.start : s.stop + 1]\n+        new_indptr = self.indptr[s.start : s.stop + 1].copy()\n \n         start = new_indptr[0]\n         stop = new_indptr[-1]\n@@ -154,7 +168,6 @@ def _get_sliceXslice(self, row: slice, col: slice) -> ss.csr_matrix:\n             slice_len(row, self.shape[0]),\n             slice_len(col, self.shape[1]),\n         )\n-\n         if out_shape[0] == 1:\n             return self._get_intXslice(slice_as_int(row, self.shape[0]), col)\n         elif out_shape[1] == self.shape[1] and out_shape[0] < self.shape[0]:\n@@ -167,6 +180,8 @@ def _get_sliceXslice(self, row: slice, col: slice) -> ss.csr_matrix:\n \n     def _get_arrayXslice(self, row: Sequence[int], col: slice) -> ss.csr_matrix:\n         idxs = np.asarray(row)\n+        if len(idxs) == 0:\n+            return ss.csr_matrix((0, self.shape[1]))\n         if idxs.dtype == bool:\n             idxs = np.where(idxs)\n         return ss.csr_matrix(\n@@ -201,6 +216,8 @@ def _get_sliceXslice(self, row: slice, col: slice) -> ss.csc_matrix:\n \n     def _get_sliceXarray(self, row: slice, col: Sequence[int]) -> ss.csc_matrix:\n         idxs = np.asarray(col)\n+        if len(idxs) == 0:\n+            return ss.csc_matrix((self.shape[0], 0))\n         if idxs.dtype == bool:\n             idxs = np.where(idxs)\n         return ss.csc_matrix(\n@@ -236,6 +253,28 @@ def get_compressed_vectors(\n     return data, indices, indptr\n \n \n+def get_compressed_vectors_for_slices(\n+    x: BackedSparseMatrix, slices: Iterable[slice]\n+) -> tuple[Sequence, Sequence, Sequence]:\n+    indptr_sels = [x.indptr[slice(s.start, s.stop + 1)] for s in slices]\n+    data = np.concatenate([x.data[s[0] : s[-1]] for s in indptr_sels])\n+    indices = np.concatenate([x.indices[s[0] : s[-1]] for s in indptr_sels])\n+    # Need to track the size of the gaps in the slices to each indptr subselection\n+    total = indptr_sels[0][0]\n+    offsets = [total]\n+    for i, sel in enumerate(indptr_sels[1:]):\n+        total = (sel[0] - indptr_sels[i][-1]) + total\n+        offsets.append(total)\n+    start_indptr = indptr_sels[0] - offsets[0]\n+    if len(slices) < 2:  # there is only one slice so no need to concatenate\n+        return data, indices, start_indptr\n+    end_indptr = np.concatenate(\n+        [s[1:] - offsets[i + 1] for i, s in enumerate(indptr_sels[1:])]\n+    )\n+    indptr = np.concatenate([start_indptr, end_indptr])\n+    return data, indices, indptr\n+\n+\n def get_compressed_vector(\n     x: BackedSparseMatrix, idx: int\n ) -> tuple[Sequence, Sequence, Sequence]:\n@@ -246,6 +285,23 @@ def get_compressed_vector(\n     return data, indices, indptr\n \n \n+def subset_by_major_axis_mask(\n+    mtx: ss.spmatrix, mask: np.ndarray\n+) -> tuple[np.ndarray, np.ndarray, np.ndarray]:\n+    slices = np.ma.extras._ezclump(mask)\n+\n+    def mean_slice_length(slices):\n+        return floor(sum(s.stop - s.start for s in slices) / len(slices))\n+\n+    # heuristic for whether slicing should be optimized\n+    if len(slices) > 0:\n+        if mean_slice_length(slices) <= 7:\n+            return get_compressed_vectors(mtx, np.where(mask)[0])\n+        else:\n+            return get_compressed_vectors_for_slices(mtx, slices)\n+    return [], [], [0]\n+\n+\n def get_format(data: ss.spmatrix) -> str:\n     for fmt, _, memory_class in FORMATS:\n         if isinstance(data, memory_class):\n@@ -280,13 +336,26 @@ def _get_group_format(group) -> str:\n class BaseCompressedSparseDataset(ABC):\n     \"\"\"Analogous to :class:`h5py.Dataset <h5py:Dataset>` or `zarr.Array`, but for sparse matrices.\"\"\"\n \n-    def __init__(self, group: h5py.Group | ZarrGroup):\n+    _group: GroupStorageType\n+\n+    def __init__(self, group: GroupStorageType):\n         type(self)._check_group_format(group)\n-        self.group = group\n+        self._group = group\n \n     shape: tuple[int, int]\n     \"\"\"Shape of the matrix.\"\"\"\n \n+    @property\n+    def group(self):\n+        \"\"\"The group underlying the backed matrix.\"\"\"\n+        return self._group\n+\n+    @group.setter\n+    def group(self, val):\n+        raise AttributeError(\n+            f\"Do not reset group on a {type(self)} with {val}.  Instead use `sparse_dataset` to make a new class.\"\n+        )\n+\n     @property\n     def backend(self) -> Literal[\"zarr\", \"hdf5\"]:\n         if isinstance(self.group, ZarrGroup):\n@@ -341,9 +410,22 @@ def __repr__(self) -> str:\n         return f\"{type(self).__name__}: backend {self.backend}, shape {self.shape}, data_dtype {self.dtype}\"\n \n     def __getitem__(self, index: Index | tuple[()]) -> float | ss.spmatrix:\n-        row, col = self._normalize_index(index)\n+        indices = self._normalize_index(index)\n+        row, col = indices\n         mtx = self._to_backed()\n-        sub = mtx[row, col]\n+\n+        # Handle masked indexing along major axis\n+        if self.format == \"csr\" and np.array(row).dtype == bool:\n+            sub = ss.csr_matrix(\n+                subset_by_major_axis_mask(mtx, row), shape=(row.sum(), mtx.shape[1])\n+            )[:, col]\n+        elif self.format == \"csc\" and np.array(col).dtype == bool:\n+            sub = ss.csc_matrix(\n+                subset_by_major_axis_mask(mtx, col), shape=(mtx.shape[0], col.sum())\n+            )[row, :]\n+        else:\n+            sub = mtx[row, col]\n+\n         # If indexing is array x array it returns a backed_sparse_matrix\n         # Not sure what the performance is on that operation\n         if isinstance(sub, BackedSparseMatrix):\n@@ -354,7 +436,7 @@ def __getitem__(self, index: Index | tuple[()]) -> float | ss.spmatrix:\n     def _normalize_index(\n         self, index: Index | tuple[()]\n     ) -> tuple[np.ndarray, np.ndarray]:\n-        if index == ():\n+        if isinstance(index, tuple) and not len(index):\n             index = slice(None)\n         row, col = unpack_index(index)\n         if all(isinstance(x, cabc.Iterable) for x in (row, col)):\n@@ -431,20 +513,25 @@ def append(self, sparse_matrix: ss.spmatrix):\n         indices.resize((orig_data_size + sparse_matrix.indices.shape[0],))\n         indices[orig_data_size:] = sparse_matrix.indices\n \n+    @cached_property\n+    def indptr(self) -> np.ndarray:\n+        arr = self.group[\"indptr\"][...]\n+        return arr\n+\n     def _to_backed(self) -> BackedSparseMatrix:\n         format_class = get_backed_class(self.format)\n         mtx = format_class(self.shape, dtype=self.dtype)\n         mtx.data = self.group[\"data\"]\n         mtx.indices = self.group[\"indices\"]\n-        mtx.indptr = self.group[\"indptr\"][:]\n+        mtx.indptr = self.indptr\n         return mtx\n \n     def to_memory(self) -> ss.spmatrix:\n         format_class = get_memory_class(self.format)\n         mtx = format_class(self.shape, dtype=self.dtype)\n         mtx.data = self.group[\"data\"][...]\n         mtx.indices = self.group[\"indices\"][...]\n-        mtx.indptr = self.group[\"indptr\"][...]\n+        mtx.indptr = self.indptr\n         return mtx\n \n \n@@ -472,7 +559,7 @@ class CSCDataset(BaseCompressedSparseDataset):\n     format = \"csc\"\n \n \n-def sparse_dataset(group: ZarrGroup | H5Group) -> CSRDataset | CSCDataset:\n+def sparse_dataset(group: GroupStorageType) -> CSRDataset | CSCDataset:\n     \"\"\"Generates a backed mode-compatible sparse dataset class.\n \n     Parameters\n@@ -489,12 +576,12 @@ def sparse_dataset(group: ZarrGroup | H5Group) -> CSRDataset | CSCDataset:\n \n     >>> import zarr\n     >>> from anndata.experimental import sparse_dataset\n-    >>> group = zarr.open_group('./my_test_store.zarr')\n-    >>> group['data'] = [10, 20, 30, 40, 50, 60, 70, 80]\n-    >>> group['indices'] = [0, 1, 1, 3, 2, 3, 4, 5]\n-    >>> group['indptr'] = [0, 2, 4, 7, 8]\n-    >>> group.attrs['shape'] = (4, 6)\n-    >>> group.attrs['encoding-type'] = 'csr_matrix'\n+    >>> group = zarr.open_group(\"./my_test_store.zarr\")\n+    >>> group[\"data\"] = [10, 20, 30, 40, 50, 60, 70, 80]\n+    >>> group[\"indices\"] = [0, 1, 1, 3, 2, 3, 4, 5]\n+    >>> group[\"indptr\"] = [0, 2, 4, 7, 8]\n+    >>> group.attrs[\"shape\"] = (4, 6)\n+    >>> group.attrs[\"encoding-type\"] = \"csr_matrix\"\n     >>> sparse_dataset(group)\n     CSRDataset: backend zarr, shape (4, 6), data_dtype int64\n     \"\"\"\n@@ -508,3 +595,30 @@ def sparse_dataset(group: ZarrGroup | H5Group) -> CSRDataset | CSCDataset:\n @_subset.register(BaseCompressedSparseDataset)\n def subset_sparsedataset(d, subset_idx):\n     return d[subset_idx]\n+\n+\n+## Backwards compat\n+\n+_sparsedataset_depr_msg = \"\"\"\\\n+SparseDataset is deprecated and will be removed in late 2024. It has been replaced by the public classes CSRDataset and CSCDataset.\n+\n+For instance checks, use `isinstance(X, (anndata.experimental.CSRDataset, anndata.experimental.CSCDataset))` instead.\n+\n+For creation, use `anndata.experimental.sparse_dataset(X)` instead.\n+\"\"\"\n+\n+\n+class SparseDataset(ABC):\n+    \"\"\"DEPRECATED.\n+\n+    Use CSRDataset, CSCDataset, and sparse_dataset from anndata.experimental instead.\n+    \"\"\"\n+\n+    def __new__(cls, group):\n+        warnings.warn(FutureWarning(_sparsedataset_depr_msg), stacklevel=2)\n+        return sparse_dataset(group)\n+\n+    @classmethod\n+    def __subclasshook__(cls, C):\n+        warnings.warn(FutureWarning(_sparsedataset_depr_msg), stacklevel=3)\n+        return issubclass(C, (CSRDataset, CSCDataset))"
        },
        {
            "sha": "ce86a27ee5480f419605d7749b4ee911be4a6f12",
            "filename": "anndata/_core/views.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/scverse/anndata/blob/f88f7bd4250b963752d615e491b7e676ce5eb7f0/anndata%2F_core%2Fviews.py",
            "raw_url": "https://github.com/scverse/anndata/raw/f88f7bd4250b963752d615e491b7e676ce5eb7f0/anndata%2F_core%2Fviews.py",
            "contents_url": "https://api.github.com/repos/scverse/anndata/contents/anndata%2F_core%2Fviews.py?ref=f88f7bd4250b963752d615e491b7e676ce5eb7f0",
            "patch": "@@ -395,6 +395,10 @@ def _resolve_idx(old, new, l):\n \n @_resolve_idx.register(np.ndarray)\n def _resolve_idx_ndarray(old, new, l):\n+    if is_bool_dtype(old) and is_bool_dtype(new):\n+        mask_new = np.zeros_like(old)\n+        mask_new[np.flatnonzero(old)[new]] = True\n+        return mask_new\n     if is_bool_dtype(old):\n         old = np.where(old)[0]\n     return old[new]"
        },
        {
            "sha": "5f31da04ac6ff071ccf0617b6a2ac1d85536aba7",
            "filename": "anndata/_io/h5ad.py",
            "status": "modified",
            "additions": 15,
            "deletions": 6,
            "changes": 21,
            "blob_url": "https://github.com/scverse/anndata/blob/f88f7bd4250b963752d615e491b7e676ce5eb7f0/anndata%2F_io%2Fh5ad.py",
            "raw_url": "https://github.com/scverse/anndata/raw/f88f7bd4250b963752d615e491b7e676ce5eb7f0/anndata%2F_io%2Fh5ad.py",
            "contents_url": "https://api.github.com/repos/scverse/anndata/contents/anndata%2F_io%2Fh5ad.py?ref=f88f7bd4250b963752d615e491b7e676ce5eb7f0",
            "patch": "@@ -6,6 +6,7 @@\n from types import MappingProxyType\n from typing import (\n     TYPE_CHECKING,\n+    Any,\n     Callable,\n     Literal,\n     TypeVar,\n@@ -29,6 +30,7 @@\n )\n from ..experimental import read_dispatched\n from .specs import read_elem, write_elem\n+from .specs.registry import IOSpec, write_spec\n from .utils import (\n     H5PY_V3,\n     _read_legacy_raw,\n@@ -110,7 +112,14 @@ def write_h5ad(\n \n \n @report_write_key_on_error\n-def write_sparse_as_dense(f, key, value, dataset_kwargs=MappingProxyType({})):\n+@write_spec(IOSpec(\"array\", \"0.2.0\"))\n+def write_sparse_as_dense(\n+    f: h5py.Group,\n+    key: str,\n+    value: sparse.spmatrix | BaseCompressedSparseDataset,\n+    *,\n+    dataset_kwargs: Mapping[str, Any] = MappingProxyType({}),\n+):\n     real_key = None  # Flag for if temporary key was used\n     if key in f:\n         if isinstance(value, BaseCompressedSparseDataset) and (\n@@ -267,7 +276,7 @@ def callback(func, elem_name: str, elem, iospec):\n def _read_raw(\n     f: h5py.File | AnnDataFileManager,\n     as_sparse: Collection[str] = (),\n-    rdasp: Callable[[h5py.Dataset], sparse.spmatrix] = None,\n+    rdasp: Callable[[h5py.Dataset], sparse.spmatrix] | None = None,\n     *,\n     attrs: Collection[str] = (\"X\", \"var\", \"varm\"),\n ) -> dict:\n@@ -284,7 +293,7 @@ def _read_raw(\n \n \n @report_read_key_on_error\n-def read_dataframe_legacy(dataset) -> pd.DataFrame:\n+def read_dataframe_legacy(dataset: h5py.Dataset) -> pd.DataFrame:\n     \"\"\"Read pre-anndata 0.7 dataframes.\"\"\"\n     warn(\n         f\"'{dataset.name}' was written with a very old version of AnnData. \"\n@@ -303,7 +312,7 @@ def read_dataframe_legacy(dataset) -> pd.DataFrame:\n     return df\n \n \n-def read_dataframe(group) -> pd.DataFrame:\n+def read_dataframe(group: h5py.Group | h5py.Dataset) -> pd.DataFrame:\n     \"\"\"Backwards compat function\"\"\"\n     if not isinstance(group, h5py.Group):\n         return read_dataframe_legacy(group)\n@@ -350,7 +359,7 @@ def read_dense_as_sparse(\n         raise ValueError(f\"Cannot read dense array as type: {sparse_format}\")\n \n \n-def read_dense_as_csr(dataset, axis_chunk=6000):\n+def read_dense_as_csr(dataset: h5py.Dataset, axis_chunk: int = 6000):\n     sub_matrices = []\n     for idx in idx_chunks_along_axis(dataset.shape, 0, axis_chunk):\n         dense_chunk = dataset[idx]\n@@ -359,7 +368,7 @@ def read_dense_as_csr(dataset, axis_chunk=6000):\n     return sparse.vstack(sub_matrices, format=\"csr\")\n \n \n-def read_dense_as_csc(dataset, axis_chunk=6000):\n+def read_dense_as_csc(dataset: h5py.Dataset, axis_chunk: int = 6000):\n     sub_matrices = []\n     for idx in idx_chunks_along_axis(dataset.shape, 1, axis_chunk):\n         sub_matrix = sparse.csc_matrix(dataset[idx])"
        },
        {
            "sha": "a50c4b2ef6ab759b770cc3e8199f2678f56fa6ee",
            "filename": "anndata/_io/read.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/scverse/anndata/blob/f88f7bd4250b963752d615e491b7e676ce5eb7f0/anndata%2F_io%2Fread.py",
            "raw_url": "https://github.com/scverse/anndata/raw/f88f7bd4250b963752d615e491b7e676ce5eb7f0/anndata%2F_io%2Fread.py",
            "contents_url": "https://api.github.com/repos/scverse/anndata/contents/anndata%2F_io%2Fread.py?ref=f88f7bd4250b963752d615e491b7e676ce5eb7f0",
            "patch": "@@ -274,16 +274,16 @@ def read_loom(\n         uns = {}\n         if cleanup:\n             uns_obs = {}\n-            for key in list(obs.keys()):\n-                if len(set(obs[key])) == 1:\n-                    uns_obs[f\"{key}\"] = obs[key][0]\n+            for key in obs.columns:\n+                if len(obs[key].unique()) == 1:\n+                    uns_obs[key] = obs[key].iloc[0]\n                     del obs[key]\n             if uns_obs:\n                 uns[\"loom-obs\"] = uns_obs\n             uns_var = {}\n-            for key in list(var.keys()):\n-                if len(set(var[key])) == 1:\n-                    uns_var[f\"{key}\"] = var[key][0]\n+            for key in var.columns:\n+                if len(var[key].unique()) == 1:\n+                    uns_var[key] = var[key].iloc[0]\n                     del var[key]\n             if uns_var:\n                 uns[\"loom-var\"] = uns_var"
        },
        {
            "sha": "70bd36945218f527e564c69c4aa522193042d92a",
            "filename": "anndata/_io/specs/methods.py",
            "status": "modified",
            "additions": 14,
            "deletions": 1,
            "changes": 15,
            "blob_url": "https://github.com/scverse/anndata/blob/f88f7bd4250b963752d615e491b7e676ce5eb7f0/anndata%2F_io%2Fspecs%2Fmethods.py",
            "raw_url": "https://github.com/scverse/anndata/raw/f88f7bd4250b963752d615e491b7e676ce5eb7f0/anndata%2F_io%2Fspecs%2Fmethods.py",
            "contents_url": "https://api.github.com/repos/scverse/anndata/contents/anndata%2F_io%2Fspecs%2Fmethods.py?ref=f88f7bd4250b963752d615e491b7e676ce5eb7f0",
            "patch": "@@ -645,7 +645,7 @@ def read_awkward(elem, _reader):\n     length = _read_attr(elem.attrs, \"length\")\n     container = {k: _reader.read_elem(elem[k]) for k in elem.keys()}\n \n-    return ak.from_buffers(form, length, container)\n+    return ak.from_buffers(form, int(length), container)\n \n \n ##############\n@@ -663,10 +663,23 @@ def write_dataframe(f, key, df, _writer, dataset_kwargs=MappingProxyType({})):\n         if reserved in df.columns:\n             raise ValueError(f\"{reserved!r} is a reserved name for dataframe columns.\")\n     group = f.require_group(key)\n+    if not df.columns.is_unique:\n+        duplicates = list(df.columns[df.columns.duplicated()])\n+        raise ValueError(\n+            f\"Found repeated column names: {duplicates}. Column names must be unique.\"\n+        )\n     col_names = [check_key(c) for c in df.columns]\n     group.attrs[\"column-order\"] = col_names\n \n     if df.index.name is not None:\n+        if df.index.name in col_names and not pd.Series(\n+            df.index, index=df.index\n+        ).equals(df[df.index.name]):\n+            raise ValueError(\n+                f\"DataFrame.index.name ({df.index.name!r}) is also used by a column \"\n+                \"whose values are different. This is not supported. Please make sure \"\n+                \"the values are the same, or use a different name.\"\n+            )\n         index_name = df.index.name\n     else:\n         index_name = \"_index\""
        },
        {
            "sha": "a8357295d99f86895193106fac59480ca348077b",
            "filename": "anndata/_io/specs/registry.py",
            "status": "modified",
            "additions": 36,
            "deletions": 45,
            "changes": 81,
            "blob_url": "https://github.com/scverse/anndata/blob/f88f7bd4250b963752d615e491b7e676ce5eb7f0/anndata%2F_io%2Fspecs%2Fregistry.py",
            "raw_url": "https://github.com/scverse/anndata/raw/f88f7bd4250b963752d615e491b7e676ce5eb7f0/anndata%2F_io%2Fspecs%2Fregistry.py",
            "contents_url": "https://api.github.com/repos/scverse/anndata/contents/anndata%2F_io%2Fspecs%2Fregistry.py?ref=f88f7bd4250b963752d615e491b7e676ce5eb7f0",
            "patch": "@@ -1,6 +1,6 @@\n from __future__ import annotations\n \n-from collections.abc import Callable, Iterable, Mapping\n+from collections.abc import Mapping\n from dataclasses import dataclass\n from functools import singledispatch, wraps\n from types import MappingProxyType\n@@ -10,12 +10,13 @@\n from anndata.compat import _read_attr\n \n if TYPE_CHECKING:\n+    from collections.abc import Callable, Generator, Iterable\n+\n     from anndata._types import GroupStorageType, StorageType\n \n+\n # TODO: This probably should be replaced by a hashable Mapping due to conversion b/w \"_\" and \"-\"\n # TODO: Should filetype be included in the IOSpec if it changes the encoding? Or does the intent that these things be \"the same\" overrule that?\n-\n-\n @dataclass(frozen=True)\n class IOSpec:\n     encoding_type: str\n@@ -25,7 +26,9 @@ class IOSpec:\n # TODO: Should this subclass from LookupError?\n class IORegistryError(Exception):\n     @classmethod\n-    def _from_write_parts(cls, dest_type, typ, modifiers) -> IORegistryError:\n+    def _from_write_parts(\n+        cls, dest_type: type, typ: type, modifiers: frozenset[str]\n+    ) -> IORegistryError:\n         msg = f\"No method registered for writing {typ} into {dest_type}\"\n         if modifiers:\n             msg += f\" with {modifiers}\"\n@@ -36,7 +39,7 @@ def _from_read_parts(\n         cls,\n         method: str,\n         registry: Mapping,\n-        src_typ: StorageType,\n+        src_typ: type[StorageType],\n         spec: IOSpec,\n     ) -> IORegistryError:\n         # TODO: Improve error message if type exists, but version does not\n@@ -50,7 +53,7 @@ def _from_read_parts(\n def write_spec(spec: IOSpec):\n     def decorator(func: Callable):\n         @wraps(func)\n-        def wrapper(g, k, *args, **kwargs):\n+        def wrapper(g: GroupStorageType, k: str, *args, **kwargs):\n             result = func(g, k, *args, **kwargs)\n             g[k].attrs.setdefault(\"encoding-type\", spec.encoding_type)\n             g[k].attrs.setdefault(\"encoding-version\", spec.encoding_version)\n@@ -193,12 +196,12 @@ def proc_spec(spec) -> IOSpec:\n \n \n @proc_spec.register(IOSpec)\n-def proc_spec_spec(spec) -> IOSpec:\n+def proc_spec_spec(spec: IOSpec) -> IOSpec:\n     return spec\n \n \n @proc_spec.register(Mapping)\n-def proc_spec_mapping(spec) -> IOSpec:\n+def proc_spec_mapping(spec: Mapping[str, str]) -> IOSpec:\n     return IOSpec(**{k.replace(\"-\", \"_\"): v for k, v in spec.items()})\n \n \n@@ -213,7 +216,9 @@ def get_spec(\n     )\n \n \n-def _iter_patterns(elem):\n+def _iter_patterns(\n+    elem,\n+) -> Generator[tuple[type, type | str] | tuple[type, type, str], None, None]:\n     \"\"\"Iterates over possible patterns for an element in order of precedence.\"\"\"\n     from anndata.compat import DaskArray\n \n@@ -236,40 +241,27 @@ def __init__(self, registry: IORegistry, callback: Callable | None = None) -> No\n     def read_elem(\n         self,\n         elem: StorageType,\n-        modifiers: frozenset(str) = frozenset(),\n+        modifiers: frozenset[str] = frozenset(),\n     ) -> Any:\n         \"\"\"Read an element from a store. See exported function for more details.\"\"\"\n         from functools import partial\n \n-        read_func = self.registry.get_reader(\n-            type(elem), get_spec(elem), frozenset(modifiers)\n+        iospec = get_spec(elem)\n+        read_func = partial(\n+            self.registry.get_reader(type(elem), iospec, modifiers),\n+            _reader=self,\n         )\n-        read_func = partial(read_func, _reader=self)\n-        if self.callback is not None:\n-            return self.callback(read_func, elem.name, elem, iospec=get_spec(elem))\n-        else:\n+        if self.callback is None:\n             return read_func(elem)\n+        return self.callback(read_func, elem.name, elem, iospec=iospec)\n \n \n class Writer:\n-    def __init__(\n-        self,\n-        registry: IORegistry,\n-        callback: Callable[\n-            [\n-                GroupStorageType,\n-                str,\n-                StorageType,\n-                dict,\n-            ],\n-            None,\n-        ]\n-        | None = None,\n-    ):\n+    def __init__(self, registry: IORegistry, callback: Callable | None = None):\n         self.registry = registry\n         self.callback = callback\n \n-    def find_writer(self, dest_type, elem, modifiers):\n+    def find_writer(self, dest_type: type, elem, modifiers: frozenset[str]):\n         for pattern in _iter_patterns(elem):\n             if self.registry.has_writer(dest_type, pattern, modifiers):\n                 return self.registry.get_writer(dest_type, pattern, modifiers)\n@@ -281,10 +273,10 @@ def write_elem(\n         self,\n         store: GroupStorageType,\n         k: str,\n-        elem,\n+        elem: Any,\n         *,\n-        dataset_kwargs=MappingProxyType({}),\n-        modifiers=frozenset(),\n+        dataset_kwargs: Mapping[str, Any] = MappingProxyType({}),\n+        modifiers: frozenset[str] = frozenset(),\n     ):\n         from functools import partial\n         from pathlib import PurePosixPath\n@@ -313,17 +305,16 @@ def write_elem(\n             _writer=self,\n         )\n \n-        if self.callback is not None:\n-            return self.callback(\n-                write_func,\n-                store,\n-                k,\n-                elem,\n-                dataset_kwargs=dataset_kwargs,\n-                iospec=self.registry.get_spec(elem),\n-            )\n-        else:\n+        if self.callback is None:\n             return write_func(store, k, elem, dataset_kwargs=dataset_kwargs)\n+        return self.callback(\n+            write_func,\n+            store,\n+            k,\n+            elem,\n+            dataset_kwargs=dataset_kwargs,\n+            iospec=self.registry.get_spec(elem),\n+        )\n \n \n def read_elem(elem: StorageType) -> Any:\n@@ -346,7 +337,7 @@ def write_elem(\n     k: str,\n     elem: Any,\n     *,\n-    dataset_kwargs: Mapping = MappingProxyType({}),\n+    dataset_kwargs: Mapping[str, Any] = MappingProxyType({}),\n ) -> None:\n     \"\"\"\n     Write an element to a storage group using anndata encoding."
        },
        {
            "sha": "6f46ad7ebec6816c616d7a877030b8700e29ea86",
            "filename": "anndata/_io/utils.py",
            "status": "modified",
            "additions": 41,
            "deletions": 36,
            "changes": 77,
            "blob_url": "https://github.com/scverse/anndata/blob/f88f7bd4250b963752d615e491b7e676ce5eb7f0/anndata%2F_io%2Futils.py",
            "raw_url": "https://github.com/scverse/anndata/raw/f88f7bd4250b963752d615e491b7e676ce5eb7f0/anndata%2F_io%2Futils.py",
            "contents_url": "https://api.github.com/repos/scverse/anndata/contents/anndata%2F_io%2Futils.py?ref=f88f7bd4250b963752d615e491b7e676ce5eb7f0",
            "patch": "@@ -1,19 +1,23 @@\n from __future__ import annotations\n \n from functools import wraps\n-from typing import Callable, Literal\n+from typing import TYPE_CHECKING, Callable, Literal, Union, cast\n from warnings import warn\n \n import h5py\n-from packaging import version\n-\n-from anndata.compat import H5Group, ZarrGroup, add_note\n+from packaging.version import Version\n \n from .._core.sparse_dataset import BaseCompressedSparseDataset\n+from ..compat import H5Group, ZarrGroup, add_note, pairwise\n+\n+if TYPE_CHECKING:\n+    from .._types import StorageType\n+\n+    Storage = Union[StorageType, BaseCompressedSparseDataset]\n \n # For allowing h5py v3\n # https://github.com/scverse/anndata/issues/442\n-H5PY_V3 = version.parse(h5py.__version__).major >= 3\n+H5PY_V3 = Version(h5py.__version__).major >= 3\n \n # -------------------------------------------------------------------------------\n # Type conversion\n@@ -151,38 +155,31 @@ class AnnDataReadError(OSError):\n     pass\n \n \n-def _get_parent(elem):\n-    try:\n-        import zarr\n-    except ImportError:\n-        zarr = None\n-    if zarr and isinstance(elem, (zarr.Group, zarr.Array)):\n-        parent = elem.store  # Not sure how to always get a name out of this\n-    elif isinstance(elem, BaseCompressedSparseDataset):\n-        parent = elem.group.file.name\n-    else:\n-        parent = elem.file.name\n-    return parent\n+def _get_display_path(store: Storage) -> str:\n+    \"\"\"Return an absolute path of an element (always starts with “/”).\"\"\"\n+    if isinstance(store, BaseCompressedSparseDataset):\n+        store = store.group\n+    path = store.name or \"??\"  # can be None\n+    return f'/{path.removeprefix(\"/\")}'\n \n \n-def re_raise_error(e, elem, key, op=Literal[\"read\", \"writ\"]):\n+def add_key_note(\n+    e: BaseException, store: Storage, path: str, key: str, op: Literal[\"read\", \"writ\"]\n+) -> None:\n     if any(\n         f\"Error raised while {op}ing key\" in note\n         for note in getattr(e, \"__notes__\", [])\n     ):\n-        raise\n-    else:\n-        parent = _get_parent(elem)\n-        add_note(\n-            e,\n-            f\"Error raised while {op}ing key {key!r} of {type(elem)} to \" f\"{parent}\",\n-        )\n-        raise e\n+        return\n+\n+    dir = \"to\" if op == \"writ\" else \"from\"\n+    msg = f\"Error raised while {op}ing key {key!r} of {type(store)} {dir} {path}\"\n+    add_note(e, msg)\n \n \n def report_read_key_on_error(func):\n     \"\"\"\\\n-    A decorator for zarr element reading which makes keys involved in errors get reported.\n+    A decorator for hdf5/zarr element reading which makes keys involved in errors get reported.\n \n     Example\n     -------\n@@ -200,20 +197,25 @@ def func_wrapper(*args, **kwargs):\n         from anndata._io.specs import Reader\n \n         # Figure out signature (method vs function) by going through args\n-        for elem in args:\n-            if not isinstance(elem, Reader):\n+        for arg in args:\n+            if not isinstance(arg, Reader):\n+                store = cast(\"Storage\", arg)\n                 break\n+        else:\n+            raise ValueError(\"No element found in args.\")\n         try:\n             return func(*args, **kwargs)\n         except Exception as e:\n-            re_raise_error(e, elem, elem.name, \"read\")\n+            path, key = _get_display_path(store).rsplit(\"/\", 1)\n+            add_key_note(e, store, path or \"/\", key, \"read\")\n+            raise\n \n     return func_wrapper\n \n \n def report_write_key_on_error(func):\n     \"\"\"\\\n-    A decorator for zarr element reading which makes keys involved in errors get reported.\n+    A decorator for hdf5/zarr element writing which makes keys involved in errors get reported.\n \n     Example\n     -------\n@@ -231,15 +233,18 @@ def func_wrapper(*args, **kwargs):\n         from anndata._io.specs import Writer\n \n         # Figure out signature (method vs function) by going through args\n-        for i in range(len(args)):\n-            elem = args[i]\n-            key = args[i + 1]\n-            if not isinstance(elem, Writer):\n+        for arg, key in pairwise(args):\n+            if not isinstance(arg, Writer):\n+                store = cast(\"Storage\", arg)\n                 break\n+        else:\n+            raise ValueError(\"No element found in args.\")\n         try:\n             return func(*args, **kwargs)\n         except Exception as e:\n-            re_raise_error(e, elem, key, \"writ\")\n+            path = _get_display_path(store)\n+            add_key_note(e, store, path, key, \"writ\")\n+            raise\n \n     return func_wrapper\n "
        },
        {
            "sha": "864475848ce71320028cbeca7ecde2490661e154",
            "filename": "anndata/_io/zarr.py",
            "status": "modified",
            "additions": 3,
            "deletions": 9,
            "changes": 12,
            "blob_url": "https://github.com/scverse/anndata/blob/f88f7bd4250b963752d615e491b7e676ce5eb7f0/anndata%2F_io%2Fzarr.py",
            "raw_url": "https://github.com/scverse/anndata/raw/f88f7bd4250b963752d615e491b7e676ce5eb7f0/anndata%2F_io%2Fzarr.py",
            "contents_url": "https://api.github.com/repos/scverse/anndata/contents/anndata%2F_io%2Fzarr.py?ref=f88f7bd4250b963752d615e491b7e676ce5eb7f0",
            "patch": "@@ -12,16 +12,10 @@\n from anndata._warnings import OldFormatWarning\n \n from .._core.anndata import AnnData\n-from ..compat import (\n-    _clean_uns,\n-    _from_fixed_length_strings,\n-)\n+from ..compat import _clean_uns, _from_fixed_length_strings\n from ..experimental import read_dispatched, write_dispatched\n from .specs import read_elem\n-from .utils import (\n-    _read_legacy_raw,\n-    report_read_key_on_error,\n-)\n+from .utils import _read_legacy_raw, report_read_key_on_error\n \n if TYPE_CHECKING:\n     from collections.abc import MutableMapping\n@@ -139,7 +133,7 @@ def read_dataframe_legacy(dataset: zarr.Array) -> pd.DataFrame:\n \n \n @report_read_key_on_error\n-def read_dataframe(group) -> pd.DataFrame:\n+def read_dataframe(group: zarr.Group | zarr.Array) -> pd.DataFrame:\n     # Fast paths\n     if isinstance(group, zarr.Array):\n         return read_dataframe_legacy(group)"
        },
        {
            "sha": "5128d79b6407ab09a3cd6411d268071fd6a5889c",
            "filename": "anndata/_settings.py",
            "status": "added",
            "additions": 377,
            "deletions": 0,
            "changes": 377,
            "blob_url": "https://github.com/scverse/anndata/blob/f88f7bd4250b963752d615e491b7e676ce5eb7f0/anndata%2F_settings.py",
            "raw_url": "https://github.com/scverse/anndata/raw/f88f7bd4250b963752d615e491b7e676ce5eb7f0/anndata%2F_settings.py",
            "contents_url": "https://api.github.com/repos/scverse/anndata/contents/anndata%2F_settings.py?ref=f88f7bd4250b963752d615e491b7e676ce5eb7f0",
            "patch": "@@ -0,0 +1,377 @@\n+from __future__ import annotations\n+\n+import os\n+import textwrap\n+import warnings\n+from collections.abc import Iterable\n+from contextlib import contextmanager\n+from enum import Enum\n+from inspect import Parameter, signature\n+from typing import TYPE_CHECKING, Any, NamedTuple, TypeVar\n+\n+from anndata.compat.exceptiongroups import add_note\n+\n+if TYPE_CHECKING:\n+    from collections.abc import Callable, Sequence\n+\n+T = TypeVar(\"T\")\n+\n+\n+class DeprecatedOption(NamedTuple):\n+    option: str\n+    message: str | None\n+    removal_version: str | None\n+\n+\n+# TODO: inherit from Generic[T] as well after python 3.9 is no longer supported\n+class RegisteredOption(NamedTuple):\n+    option: str\n+    default_value: T\n+    doc: str\n+    validate: Callable[[T], bool] | None\n+    type: object\n+\n+\n+def check_and_get_environ_var(\n+    key: str,\n+    default_value: str,\n+    allowed_values: Sequence[str] | None = None,\n+    cast: Callable[[Any], T] | type[Enum] = lambda x: x,\n+) -> T:\n+    \"\"\"Get the environment variable and return it is a (potentially) non-string, usable value.\n+\n+    Parameters\n+    ----------\n+    key\n+        The environment variable name.\n+    default_value\n+        The default value for `os.environ.get`.\n+    allowed_values\n+        Allowable string values., by default None\n+    cast\n+        Casting from the string to a (potentially different) python object, by default lambdax:x\n+\n+    Returns\n+    -------\n+    The casted value.\n+    \"\"\"\n+    environ_value_or_default_value = os.environ.get(key, default_value)\n+    if (\n+        allowed_values is not None\n+        and environ_value_or_default_value not in allowed_values\n+    ):\n+        warnings.warn(\n+            f'Value \"{environ_value_or_default_value}\" is not in allowed {allowed_values} for environment variable {key}.\\\n+                      Default {default_value} will be used.'\n+        )\n+        environ_value_or_default_value = default_value\n+    return (\n+        cast(environ_value_or_default_value)\n+        if not isinstance(cast, type(Enum))\n+        else cast[environ_value_or_default_value]\n+    )\n+\n+\n+def check_and_get_bool(option, default_value):\n+    return check_and_get_environ_var(\n+        \"ANNDATA_\" + option.upper(),\n+        str(int(default_value)),\n+        [\"0\", \"1\"],\n+        lambda x: bool(int(x)),\n+    )\n+\n+\n+_docstring = \"\"\"\n+This manager allows users to customize settings for the anndata package.\n+Settings here will generally be for advanced use-cases and should be used with caution.\n+\n+The following options are available:\n+\n+{options_description}\n+\n+For setting an option please use :func:`~anndata.settings.override` (local) or set the above attributes directly (global) i.e., `anndata.settings.my_setting = foo`.\n+For assignment by environment variable, use the variable name in all caps with `ANNDATA_` as the prefix before import of :mod:`anndata`.\n+For boolean environment variable setting, use 1 for `True` and 0 for `False`.\n+\"\"\"\n+\n+\n+class SettingsManager:\n+    _registered_options: dict[str, RegisteredOption] = {}\n+    _deprecated_options: dict[str, DeprecatedOption] = {}\n+    _config: dict[str, object] = {}\n+    __doc_tmpl__: str = _docstring\n+\n+    def describe(\n+        self,\n+        option: str | Iterable[str] | None = None,\n+        *,\n+        print_description: bool = True,\n+    ) -> str:\n+        \"\"\"Print and/or return a (string) description of the option(s).\n+\n+        Parameters\n+        ----------\n+        option\n+            Option(s) to be described, by default None (i.e., do all option)\n+        print_description\n+            Whether or not to print the description in addition to returning it., by default True\n+\n+        Returns\n+        -------\n+        The description.\n+        \"\"\"\n+        if option is None:\n+            return self.describe(\n+                self._registered_options.keys(), print_description=print_description\n+            )\n+        if isinstance(option, Iterable) and not isinstance(option, str):\n+            return \"\\n\".join(\n+                [self.describe(k, print_description=print_description) for k in option]\n+            )\n+        registered_option = self._registered_options[option]\n+        doc = registered_option.doc.rstrip(\"\\n\")\n+        if option in self._deprecated_options:\n+            opt = self._deprecated_options[option]\n+            if opt.message is not None:\n+                doc += \" *\" + opt.message\n+            doc += f\" {option} will be removed in {opt.removal_version}.*\"\n+        if print_description:\n+            print(doc)\n+        return doc\n+\n+    def deprecate(\n+        self, option: str, removal_version: str, message: str | None = None\n+    ) -> None:\n+        \"\"\"Deprecate options with a message at a version.\n+\n+        Parameters\n+        ----------\n+        option\n+            Which option should be deprecated.\n+        removal_version\n+            The version targeted for removal.\n+        message\n+            A custom message.\n+        \"\"\"\n+        self._deprecated_options[option] = DeprecatedOption(\n+            option, message, removal_version\n+        )\n+\n+    def register(\n+        self,\n+        option: str,\n+        default_value: T,\n+        description: str,\n+        validate: Callable[[T], bool],\n+        option_type: object | None = None,\n+        get_from_env: Callable[[str, T], T] = lambda x, y: y,\n+    ) -> None:\n+        \"\"\"Register an option so it can be set/described etc. by end-users\n+\n+        Parameters\n+        ----------\n+        option\n+            Option to be set.\n+        default_value\n+            Default value with which to set the option.\n+        description\n+            Description to be used in the docstring.\n+        validate\n+            A function which returns True if the option's value is valid and otherwise should raise a `ValueError` or `TypeError`.\n+        option\n+            Optional override for the option type to be displayed.  Otherwise `type(default_value)`.\n+        get_from_env\n+            An optional function which takes as arguments the name of the option and a default value and returns the value from the environment variable `ANNDATA_CAPS_OPTION` (or default if not present).\n+            Default behavior is to return `default_value` without checking the environment.\n+        \"\"\"\n+        try:\n+            validate(default_value)\n+        except (ValueError, TypeError) as e:\n+            add_note(e, f\"for option {repr(option)}\")\n+            raise e\n+        option_type_str = (\n+            type(default_value).__name__ if option_type is None else str(option_type)\n+        )\n+        option_type = type(default_value) if option_type is None else option_type\n+        doc = f\"\"\"\\\n+        {option}: {option_type_str}\n+            {description} Default value of {default_value}.\n+        \"\"\"\n+        doc = textwrap.dedent(doc)\n+        self._registered_options[option] = RegisteredOption(\n+            option, default_value, doc, validate, option_type\n+        )\n+        self._config[option] = get_from_env(option, default_value)\n+        self._update_override_function_for_new_option(option)\n+\n+    def _update_override_function_for_new_option(\n+        self,\n+        option: str,\n+    ):\n+        \"\"\"This function updates the keyword arguments, docstring, and annotations of the `SettingsManager.override` function as the `SettingsManager.register` method is called.\n+\n+        Parameters\n+        ----------\n+        option\n+            The option being registered for which the override function needs updating.\n+        \"\"\"\n+        option_type = self._registered_options[option].type\n+        # Update annotations for type checking.\n+        self.override.__annotations__[option] = option_type\n+        # __signature__ needs to be updated for tab autocompletion in IPython.\n+        # See https://github.com/ipython/ipython/issues/11624 for inspiration.\n+        self.override.__func__.__signature__ = signature(self.override).replace(\n+            parameters=[\n+                Parameter(name=\"self\", kind=Parameter.POSITIONAL_ONLY),\n+                *[\n+                    Parameter(\n+                        name=k,\n+                        annotation=option_type,\n+                        kind=Parameter.KEYWORD_ONLY,\n+                    )\n+                    for k in self._registered_options\n+                ],\n+            ]\n+        )\n+        # Update docstring for `SettingsManager.override` as well.\n+        insert_index = self.override.__doc__.find(\"\\n        Yields\")\n+        option_docstring = \"\\t\" + \"\\t\".join(\n+            self.describe(option, print_description=False).splitlines(keepends=True)\n+        )\n+        self.override.__func__.__doc__ = (\n+            self.override.__doc__[:insert_index]\n+            + \"\\n\"\n+            + option_docstring\n+            + self.override.__doc__[insert_index:]\n+        )\n+\n+    def __setattr__(self, option: str, val: object) -> None:\n+        \"\"\"\n+        Set an option to a value.  To see the allowed option to be set and their description,\n+        use describe_option.\n+\n+        Parameters\n+        ----------\n+        option\n+            Option to be set.\n+        val\n+            Value with which to set the option.\n+\n+        Raises\n+        ------\n+        AttributeError\n+            If the option has not been registered, this function will raise an error.\n+        \"\"\"\n+        if hasattr(super(), option):\n+            super().__setattr__(option, val)\n+        elif option not in self._registered_options:\n+            raise AttributeError(\n+                f\"{option} is not an available option for anndata.\\\n+                Please open an issue if you believe this is a mistake.\"\n+            )\n+        registered_option = self._registered_options[option]\n+        registered_option.validate(val)\n+        self._config[option] = val\n+\n+    def __getattr__(self, option: str) -> object:\n+        \"\"\"\n+        Gets the option's value.\n+\n+        Parameters\n+        ----------\n+        option\n+            Option to be got.\n+\n+        Returns\n+        -------\n+        Value of the option.\n+        \"\"\"\n+        if option in self._deprecated_options:\n+            deprecated = self._deprecated_options[option]\n+            warnings.warn(\n+                DeprecationWarning(\n+                    f\"{repr(option)} will be removed in {deprecated.removal_version}. \"\n+                    + deprecated.message\n+                )\n+            )\n+        if option in self._config:\n+            return self._config[option]\n+        raise AttributeError(f\"{option} not found.\")\n+\n+    def __dir__(self) -> Iterable[str]:\n+        return sorted((*dir(super()), *self._config.keys()))\n+\n+    def reset(self, option: Iterable[str] | str) -> None:\n+        \"\"\"\n+        Resets option(s) to its (their) default value(s).\n+\n+        Parameters\n+        ----------\n+        option\n+            The option(s) to be reset.\n+        \"\"\"\n+        if isinstance(option, Iterable) and not isinstance(option, str):\n+            for opt in option:\n+                self.reset(opt)\n+        else:\n+            self._config[option] = self._registered_options[option].default_value\n+\n+    @contextmanager\n+    def override(self, **overrides):\n+        \"\"\"\n+        Provides local override via keyword arguments as a context manager.\n+\n+        Parameters\n+        ----------\n+\n+        Yields\n+        ------\n+        None\n+        \"\"\"\n+        restore = {a: getattr(self, a) for a in overrides}\n+        try:\n+            for attr, value in overrides.items():\n+                setattr(self, attr, value)\n+            yield None\n+        finally:\n+            for attr, value in restore.items():\n+                setattr(self, attr, value)\n+\n+    @property\n+    def __doc__(self):\n+        options_description = self.describe(print_description=False)\n+        return self.__doc_tmpl__.format(\n+            options_description=options_description,\n+        )\n+\n+\n+settings = SettingsManager()\n+\n+##################################################################################\n+# PLACE REGISTERED SETTINGS HERE SO THEY CAN BE PICKED UP FOR DOCSTRING CREATION #\n+##################################################################################\n+\n+\n+categories_option = \"remove_unused_categories\"\n+categories_default_value = True\n+categories_description = (\n+    \"Whether or not to remove unused categories with :class:`~pandas.Categorical`.\"\n+)\n+\n+\n+def validate_bool(val) -> bool:\n+    if not isinstance(val, bool):\n+        raise TypeError(f\"{val} not valid boolean\")\n+    return True\n+\n+\n+settings.register(\n+    categories_option,\n+    categories_default_value,\n+    categories_description,\n+    validate_bool,\n+    get_from_env=check_and_get_bool,\n+)\n+\n+##################################################################################\n+##################################################################################"
        },
        {
            "sha": "39323d73aad665dea44c89be73c2f4e83834b121",
            "filename": "anndata/compat/__init__.py",
            "status": "modified",
            "additions": 24,
            "deletions": 2,
            "changes": 26,
            "blob_url": "https://github.com/scverse/anndata/blob/f88f7bd4250b963752d615e491b7e676ce5eb7f0/anndata%2Fcompat%2F__init__.py",
            "raw_url": "https://github.com/scverse/anndata/raw/f88f7bd4250b963752d615e491b7e676ce5eb7f0/anndata%2Fcompat%2F__init__.py",
            "contents_url": "https://api.github.com/repos/scverse/anndata/contents/anndata%2Fcompat%2F__init__.py?ref=f88f7bd4250b963752d615e491b7e676ce5eb7f0",
            "patch": "@@ -1,6 +1,7 @@\n from __future__ import annotations\n \n import os\n+import sys\n from codecs import decode\n from collections.abc import Mapping\n from contextlib import AbstractContextManager\n@@ -14,6 +15,7 @@\n import h5py\n import numpy as np\n import pandas as pd\n+from packaging.version import Version\n from scipy.sparse import issparse, spmatrix\n \n from .exceptiongroups import add_note  # noqa: F401\n@@ -34,9 +36,9 @@ class Empty:\n #############################\n \n \n-try:\n+if sys.version_info >= (3, 11):\n     from contextlib import chdir\n-except ImportError:  # Python < 3.11\n+else:\n \n     @dataclass\n     class chdir(AbstractContextManager):\n@@ -51,6 +53,18 @@ def __exit__(self, *_exc_info) -> None:\n             os.chdir(self._old_cwd.pop())\n \n \n+if sys.version_info >= (3, 10):\n+    from itertools import pairwise\n+else:\n+\n+    def pairwise(iterable):\n+        from itertools import tee\n+\n+        a, b = tee(iterable)\n+        next(b, None)\n+        return zip(a, b)\n+\n+\n #############################\n # Optional deps\n #############################\n@@ -391,3 +405,11 @@ def _safe_transpose(x):\n         return _transpose_by_block(x)\n     else:\n         return x.T\n+\n+\n+def _map_cat_to_str(cat: pd.Categorical) -> pd.Categorical:\n+    if Version(pd.__version__) >= Version(\"2.1\"):\n+        # Argument added in pandas 2.1\n+        return cat.map(str, na_action=\"ignore\")\n+    else:\n+        return cat.map(str)"
        },
        {
            "sha": "8e6ef038250addc8c8615f651709bb963d96d26a",
            "filename": "anndata/core.py",
            "status": "removed",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/scverse/anndata/blob/cc6d6ea741ff6c35df3747a95c4869cc3ed5f84e/anndata%2Fcore.py",
            "raw_url": "https://github.com/scverse/anndata/raw/cc6d6ea741ff6c35df3747a95c4869cc3ed5f84e/anndata%2Fcore.py",
            "contents_url": "https://api.github.com/repos/scverse/anndata/contents/anndata%2Fcore.py?ref=cc6d6ea741ff6c35df3747a95c4869cc3ed5f84e",
            "patch": "@@ -1,7 +0,0 @@\n-from __future__ import annotations\n-\n-from warnings import warn\n-\n-warn(\"Please only import from anndata, not anndata.core\", DeprecationWarning)\n-\n-from ._core import *  # noqa: F403, E402"
        },
        {
            "sha": "2a399d5402032afa5741d771f1193a23f5504945",
            "filename": "anndata/experimental/_dispatch_io.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/scverse/anndata/blob/f88f7bd4250b963752d615e491b7e676ce5eb7f0/anndata%2Fexperimental%2F_dispatch_io.py",
            "raw_url": "https://github.com/scverse/anndata/raw/f88f7bd4250b963752d615e491b7e676ce5eb7f0/anndata%2Fexperimental%2F_dispatch_io.py",
            "contents_url": "https://api.github.com/repos/scverse/anndata/contents/anndata%2Fexperimental%2F_dispatch_io.py?ref=f88f7bd4250b963752d615e491b7e676ce5eb7f0",
            "patch": "@@ -4,6 +4,8 @@\n from typing import TYPE_CHECKING, Any, Callable\n \n if TYPE_CHECKING:\n+    from collections.abc import Mapping\n+\n     from anndata._io.specs import IOSpec\n     from anndata._types import GroupStorageType, StorageType\n \n@@ -55,7 +57,7 @@ def write_dispatched(\n         None,\n     ],\n     *,\n-    dataset_kwargs=MappingProxyType({}),\n+    dataset_kwargs: Mapping[str, Any] = MappingProxyType({}),\n ) -> None:\n     \"\"\"\n     Write elem to store, recursively calling callback at each sub-element."
        },
        {
            "sha": "a3ffc155563f276431180b16bc50baa1a728f5ec",
            "filename": "anndata/experimental/merge.py",
            "status": "modified",
            "additions": 10,
            "deletions": 5,
            "changes": 15,
            "blob_url": "https://github.com/scverse/anndata/blob/f88f7bd4250b963752d615e491b7e676ce5eb7f0/anndata%2Fexperimental%2Fmerge.py",
            "raw_url": "https://github.com/scverse/anndata/raw/f88f7bd4250b963752d615e491b7e676ce5eb7f0/anndata%2Fexperimental%2Fmerge.py",
            "contents_url": "https://api.github.com/repos/scverse/anndata/contents/anndata%2Fexperimental%2Fmerge.py?ref=f88f7bd4250b963752d615e491b7e676ce5eb7f0",
            "patch": "@@ -32,7 +32,7 @@\n )\n from .._core.sparse_dataset import BaseCompressedSparseDataset, sparse_dataset\n from .._io.specs import read_elem, write_elem\n-from ..compat import H5Array, H5Group, ZarrArray, ZarrGroup\n+from ..compat import H5Array, H5Group, ZarrArray, ZarrGroup, _map_cat_to_str\n from . import read_dispatched\n \n SPARSE_MATRIX = {\"csc_matrix\", \"csr_matrix\"}\n@@ -172,7 +172,10 @@ def write_concat_dense(\n     \"\"\"\n     import dask.array as da\n \n-    darrays = (da.from_array(a, chunks=\"auto\") for a in arrays)\n+    darrays = (\n+        da.from_array(a, chunks=\"auto\" if a.chunks is None else a.chunks)\n+        for a in arrays\n+    )\n \n     res = da.concatenate(\n         [\n@@ -381,7 +384,7 @@ def _write_alt_annot(groups, output_group, alt_dim, alt_indices, merge):\n \n def _write_dim_annot(groups, output_group, dim, concat_indices, label, label_col, join):\n     concat_annot = pd.concat(\n-        unify_dtypes([read_elem(g[dim]) for g in groups]),\n+        unify_dtypes(read_elem(g[dim]) for g in groups),\n         join=join,\n         ignore_index=True,\n     )\n@@ -520,7 +523,7 @@ def concat_on_disk(\n     >>> adata = ad.read_h5ad('merged.h5ad', backed=True)\n     >>> adata.X\n     CSRDataset: backend hdf5, shape (490, 15585), data_dtype float32\n-    >>> adata.obs['dataset'].value_counts()\n+    >>> adata.obs['dataset'].value_counts()  # doctest: +SKIP\n     dataset\n     fetal      344\n     b_cells    146\n@@ -593,7 +596,9 @@ def concat_on_disk(\n         [pd.Series(_df_index(g[dim])) for g in groups], ignore_index=True\n     )\n     if index_unique is not None:\n-        concat_indices = concat_indices.str.cat(label_col.map(str), sep=index_unique)\n+        concat_indices = concat_indices.str.cat(\n+            _map_cat_to_str(label_col), sep=index_unique\n+        )\n \n     # Resulting indices for {dim} and {alt_dim}\n     concat_indices = pd.Index(concat_indices)"
        },
        {
            "sha": "09533522a62d5b54123120e07801a9a012cd4c5b",
            "filename": "anndata/experimental/multi_files/_anncollection.py",
            "status": "modified",
            "additions": 15,
            "deletions": 8,
            "changes": 23,
            "blob_url": "https://github.com/scverse/anndata/blob/f88f7bd4250b963752d615e491b7e676ce5eb7f0/anndata%2Fexperimental%2Fmulti_files%2F_anncollection.py",
            "raw_url": "https://github.com/scverse/anndata/raw/f88f7bd4250b963752d615e491b7e676ce5eb7f0/anndata%2Fexperimental%2Fmulti_files%2F_anncollection.py",
            "contents_url": "https://api.github.com/repos/scverse/anndata/contents/anndata%2Fexperimental%2Fmulti_files%2F_anncollection.py?ref=f88f7bd4250b963752d615e491b7e676ce5eb7f0",
            "patch": "@@ -15,6 +15,7 @@\n from ..._core.merge import concat_arrays, inner_concat_aligned_mapping\n from ..._core.sparse_dataset import BaseCompressedSparseDataset\n from ..._core.views import _resolve_idx\n+from ...compat import _map_cat_to_str\n \n ATTRS = [\"obs\", \"obsm\", \"layers\"]\n \n@@ -208,7 +209,7 @@ def __getitem__(self, key, use_convert=True):\n             else:\n                 if vidx is not None:\n                     idx = np.ix_(*idx) if not isinstance(idx[1], slice) else idx\n-                arrs.append(arr[idx])\n+                arrs.append(arr.iloc[idx] if isinstance(arr, pd.Series) else arr[idx])\n \n         if len(arrs) > 1:\n             _arr = _merge(arrs)\n@@ -492,9 +493,12 @@ def convert(self):\n         ::\n \n             {\n-                'X': lambda a: a.toarray() if issparse(a) else a, # densify .X\n-                'obsm': lambda a: np.asarray(a, dtype='float32'), # change dtype for all keys of .obsm\n-                'obs': dict(key1 = lambda c: c.astype(str)) # change type only for one key of .obs\n+                # densify .X\n+                \"X\": lambda a: a.toarray() if issparse(a) else a,\n+                # change dtype for all keys of .obsm\n+                \"obsm\": lambda a: np.asarray(a, dtype=\"float32\"),\n+                # change type only for one key of .obs\n+                \"obs\": dict(key1=lambda c: c.astype(str)),\n             }\n         \"\"\"\n         return self._convert\n@@ -721,7 +725,7 @@ def __init__(\n         )\n         if index_unique is not None:\n             concat_indices = concat_indices.str.cat(\n-                label_col.map(str), sep=index_unique\n+                _map_cat_to_str(label_col), sep=index_unique\n             )\n         self.obs_names = pd.Index(concat_indices)\n \n@@ -816,9 +820,12 @@ def convert(self):\n         ::\n \n             {\n-                'X': lambda a: a.toarray() if issparse(a) else a, # densify .X\n-                'obsm': lambda a: np.asarray(a, dtype='float32'), # change dtype for all keys of .obsm\n-                'obs': dict(key1 = lambda c: c.astype(str)) # change type only for one key of .obs\n+                # densify .X\n+                \"X\": lambda a: a.toarray() if issparse(a) else a,\n+                # change dtype for all keys of .obsm\n+                \"obsm\": lambda a: np.asarray(a, dtype=\"float32\"),\n+                # change type only for one key of .obs\n+                \"obs\": dict(key1=lambda c: c.astype(str)),\n             }\n         \"\"\"\n         return self._convert"
        },
        {
            "sha": "f3d07f732ca6278bd369c5a7adffbf8b85de8aa1",
            "filename": "anndata/readwrite.py",
            "status": "removed",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/scverse/anndata/blob/cc6d6ea741ff6c35df3747a95c4869cc3ed5f84e/anndata%2Freadwrite.py",
            "raw_url": "https://github.com/scverse/anndata/raw/cc6d6ea741ff6c35df3747a95c4869cc3ed5f84e/anndata%2Freadwrite.py",
            "contents_url": "https://api.github.com/repos/scverse/anndata/contents/anndata%2Freadwrite.py?ref=cc6d6ea741ff6c35df3747a95c4869cc3ed5f84e",
            "patch": "@@ -1,7 +0,0 @@\n-from __future__ import annotations\n-\n-from warnings import warn\n-\n-warn(\"Please only import from anndata, not anndata.readwrite\", DeprecationWarning)\n-\n-from ._io import *  # noqa: F403, E402"
        },
        {
            "sha": "4fb33c039e6d04c73f87ee3e63c4a2fd5c279bd6",
            "filename": "anndata/tests/helpers.py",
            "status": "modified",
            "additions": 57,
            "deletions": 13,
            "changes": 70,
            "blob_url": "https://github.com/scverse/anndata/blob/f88f7bd4250b963752d615e491b7e676ce5eb7f0/anndata%2Ftests%2Fhelpers.py",
            "raw_url": "https://github.com/scverse/anndata/raw/f88f7bd4250b963752d615e491b7e676ce5eb7f0/anndata%2Ftests%2Fhelpers.py",
            "contents_url": "https://api.github.com/repos/scverse/anndata/contents/anndata%2Ftests%2Fhelpers.py?ref=f88f7bd4250b963752d615e491b7e676ce5eb7f0",
            "patch": "@@ -15,7 +15,7 @@\n from pandas.api.types import is_numeric_dtype\n from scipy import sparse\n \n-from anndata import AnnData, Raw\n+from anndata import AnnData, ExperimentalFeatureWarning, Raw\n from anndata._core.aligned_mapping import AlignedMapping\n from anndata._core.sparse_dataset import BaseCompressedSparseDataset\n from anndata._core.views import ArrayView\n@@ -256,17 +256,19 @@ def gen_adata(\n         awkward_ragged=gen_awkward((12, None, None)),\n         # U_recarray=gen_vstr_recarray(N, 5, \"U4\")\n     )\n-    adata = AnnData(\n-        X=X,\n-        obs=obs,\n-        var=var,\n-        obsm=obsm,\n-        varm=varm,\n-        layers=layers,\n-        obsp=obsp,\n-        varp=varp,\n-        uns=uns,\n-    )\n+    with warnings.catch_warnings():\n+        warnings.simplefilter(\"ignore\", ExperimentalFeatureWarning)\n+        adata = AnnData(\n+            X=X,\n+            obs=obs,\n+            var=var,\n+            obsm=obsm,\n+            varm=varm,\n+            layers=layers,\n+            obsp=obsp,\n+            varp=varp,\n+            uns=uns,\n+        )\n     return adata\n \n \n@@ -281,6 +283,10 @@ def array_bool_subset(index, min_size=2):\n     return b\n \n \n+def list_bool_subset(index, min_size=2):\n+    return array_bool_subset(index, min_size=min_size).tolist()\n+\n+\n def matrix_bool_subset(index, min_size=2):\n     with warnings.catch_warnings():\n         warnings.simplefilter(\"ignore\", PendingDeprecationWarning)\n@@ -318,6 +324,10 @@ def array_int_subset(index, min_size=2):\n     )\n \n \n+def list_int_subset(index, min_size=2):\n+    return array_int_subset(index, min_size=min_size).tolist()\n+\n+\n def slice_subset(index, min_size=2):\n     while True:\n         points = np.random.choice(np.arange(len(index) + 1), size=2, replace=False)\n@@ -337,7 +347,9 @@ def single_subset(index):\n         slice_subset,\n         single_subset,\n         array_int_subset,\n+        list_int_subset,\n         array_bool_subset,\n+        list_bool_subset,\n         matrix_bool_subset,\n         spmatrix_bool_subset,\n     ]\n@@ -410,7 +422,11 @@ def assert_equal_ndarray(a, b, exact=False, elem_name=None):\n         and len(a.dtype) > 1\n         and len(b.dtype) > 0\n     ):\n-        assert_equal(pd.DataFrame(a), pd.DataFrame(b), exact, elem_name)\n+        # Reshaping to allow >2d arrays\n+        assert a.shape == b.shape, format_msg(elem_name)\n+        assert_equal(\n+            pd.DataFrame(a.reshape(-1)), pd.DataFrame(b.reshape(-1)), exact, elem_name\n+        )\n     else:\n         assert np.all(a == b), format_msg(elem_name)\n \n@@ -741,3 +757,31 @@ def shares_memory_sparse(x, y):\n         marks=pytest.mark.gpu,\n     ),\n ]\n+\n+try:\n+    import zarr\n+\n+    class AccessTrackingStore(zarr.DirectoryStore):\n+        def __init__(self, *args, **kwargs):\n+            super().__init__(*args, **kwargs)\n+            self._access_count = {}\n+\n+        def __getitem__(self, key):\n+            for tracked in self._access_count:\n+                if tracked in key:\n+                    self._access_count[tracked] += 1\n+            return super().__getitem__(key)\n+\n+        def get_access_count(self, key):\n+            return self._access_count[key]\n+\n+        def set_key_trackers(self, keys_to_track):\n+            for k in keys_to_track:\n+                self._access_count[k] = 0\n+except ImportError:\n+\n+    class AccessTrackingStore:\n+        def __init__(self, *_args, **_kwargs) -> None:\n+            raise ImportError(\n+                \"zarr must be imported to create an `AccessTrackingStore` instance.\"\n+            )"
        },
        {
            "sha": "2e996bfc66fa03b3710a5c7f54516e7fc6a26bbd",
            "filename": "anndata/tests/test_awkward.py",
            "status": "modified",
            "additions": 15,
            "deletions": 5,
            "changes": 20,
            "blob_url": "https://github.com/scverse/anndata/blob/f88f7bd4250b963752d615e491b7e676ce5eb7f0/anndata%2Ftests%2Ftest_awkward.py",
            "raw_url": "https://github.com/scverse/anndata/raw/f88f7bd4250b963752d615e491b7e676ce5eb7f0/anndata%2Ftests%2Ftest_awkward.py",
            "contents_url": "https://api.github.com/repos/scverse/anndata/contents/anndata%2Ftests%2Ftest_awkward.py?ref=f88f7bd4250b963752d615e491b7e676ce5eb7f0",
            "patch": "@@ -1,13 +1,19 @@\n \"\"\"Tests related to awkward arrays\"\"\"\n from __future__ import annotations\n \n+import warnings\n+\n import numpy as np\n import numpy.testing as npt\n import pandas as pd\n import pytest\n \n import anndata\n-from anndata import AnnData, ImplicitModificationWarning, read_h5ad\n+from anndata import (\n+    AnnData,\n+    ImplicitModificationWarning,\n+    read_h5ad,\n+)\n from anndata.compat import awkward as ak\n from anndata.tests.helpers import assert_equal, gen_adata, gen_awkward\n from anndata.utils import dim_len\n@@ -196,8 +202,8 @@ def reversed(self):\n             ]\n         ),\n         # categorical array\n-        ak.to_categorical(ak.Array([[\"a\", \"b\", \"c\"], [\"a\", \"b\"]])),\n-        ak.to_categorical(ak.Array([[1, 1, 2], [3, 3]])),\n+        ak.str.to_categorical(ak.Array([[\"a\", \"b\", \"c\"], [\"a\", \"b\"]])),\n+        ak.str.to_categorical(ak.Array([[1, 1, 2], [3, 3]])),\n         # tyical record type with AIRR data consisting of different dtypes\n         ak.Array(\n             [\n@@ -375,10 +381,14 @@ def test_concat_mixed_types(key, arrays, expected, join):\n         to_concat.append(tmp_adata)\n \n     if isinstance(expected, type) and issubclass(expected, Exception):\n-        with pytest.raises(expected):\n+        with pytest.raises(expected), warnings.catch_warnings():\n+            warnings.filterwarnings(\n+                \"ignore\",\n+                r\"The behavior of DataFrame concatenation with empty or all-NA entries is deprecated\",\n+                FutureWarning,\n+            )\n             anndata.concat(to_concat, axis=axis, join=join)\n     else:\n-        print(to_concat)\n         result_adata = anndata.concat(to_concat, axis=axis, join=join)\n         result = getattr(result_adata, key).get(\"test\", None)\n         assert_equal(expected, result, exact=True)"
        },
        {
            "sha": "7ce6860d16e36f8cc97e8258104144189cd96d1c",
            "filename": "anndata/tests/test_backed_sparse.py",
            "status": "modified",
            "additions": 231,
            "deletions": 10,
            "changes": 241,
            "blob_url": "https://github.com/scverse/anndata/blob/f88f7bd4250b963752d615e491b7e676ce5eb7f0/anndata%2Ftests%2Ftest_backed_sparse.py",
            "raw_url": "https://github.com/scverse/anndata/raw/f88f7bd4250b963752d615e491b7e676ce5eb7f0/anndata%2Ftests%2Ftest_backed_sparse.py",
            "contents_url": "https://api.github.com/repos/scverse/anndata/contents/anndata%2Ftests%2Ftest_backed_sparse.py?ref=f88f7bd4250b963752d615e491b7e676ce5eb7f0",
            "patch": "@@ -1,5 +1,8 @@\n from __future__ import annotations\n \n+from functools import partial\n+from typing import TYPE_CHECKING, Callable, Literal\n+\n import h5py\n import numpy as np\n import pytest\n@@ -10,7 +13,13 @@\n from anndata._core.anndata import AnnData\n from anndata._core.sparse_dataset import sparse_dataset\n from anndata.experimental import read_dispatched\n-from anndata.tests.helpers import assert_equal, subset_func\n+from anndata.tests.helpers import AccessTrackingStore, assert_equal, subset_func\n+\n+if TYPE_CHECKING:\n+    from pathlib import Path\n+\n+    from numpy.typing import ArrayLike\n+    from pytest_mock import MockerFixture\n \n subset_func2 = subset_func\n \n@@ -20,15 +29,21 @@ def diskfmt(request):\n     return request.param\n \n \n+M = 50\n+N = 50\n+\n+\n @pytest.fixture(scope=\"function\")\n-def ondisk_equivalent_adata(tmp_path, diskfmt):\n+def ondisk_equivalent_adata(\n+    tmp_path: Path, diskfmt: Literal[\"h5ad\", \"zarr\"]\n+) -> tuple[AnnData, AnnData, AnnData, AnnData]:\n     csr_path = tmp_path / f\"csr.{diskfmt}\"\n     csc_path = tmp_path / f\"csc.{diskfmt}\"\n     dense_path = tmp_path / f\"dense.{diskfmt}\"\n \n     write = lambda x, pth, **kwargs: getattr(x, f\"write_{diskfmt}\")(pth, **kwargs)\n \n-    csr_mem = ad.AnnData(X=sparse.random(50, 50, format=\"csr\", density=0.1))\n+    csr_mem = ad.AnnData(X=sparse.random(M, N, format=\"csr\", density=0.1))\n     csc_mem = ad.AnnData(X=csr_mem.X.tocsc())\n     dense_mem = ad.AnnData(X=csr_mem.X.toarray())\n \n@@ -54,7 +69,7 @@ def callback(func, elem_name, elem, iospec):\n                         **{k: read_dispatched(v, callback) for k, v in elem.items()}\n                     )\n                 if iospec.encoding_type in {\"csc_matrix\", \"csr_matrix\"}:\n-                    return sparse_dataset(elem)._to_backed()\n+                    return sparse_dataset(elem)\n                 return func(elem)\n \n             adata = read_dispatched(f, callback=callback)\n@@ -68,26 +83,165 @@ def callback(func, elem_name, elem, iospec):\n     return csr_mem, csr_disk, csc_disk, dense_disk\n \n \n-def test_backed_indexing(ondisk_equivalent_adata, subset_func, subset_func2):\n+@pytest.mark.parametrize(\n+    \"empty_mask\", [[], np.zeros(M, dtype=bool)], ids=[\"empty_list\", \"empty_bool_mask\"]\n+)\n+def test_empty_backed_indexing(\n+    ondisk_equivalent_adata: tuple[AnnData, AnnData, AnnData, AnnData],\n+    empty_mask,\n+):\n+    csr_mem, csr_disk, csc_disk, _ = ondisk_equivalent_adata\n+\n+    assert_equal(csr_mem.X[empty_mask], csr_disk.X[empty_mask])\n+    assert_equal(csr_mem.X[:, empty_mask], csc_disk.X[:, empty_mask])\n+\n+    # The following do not work because of https://github.com/scipy/scipy/issues/19919\n+    # Our implementation returns a (0,0) sized matrix but scipy does (1,0).\n+\n+    # assert_equal(csr_mem.X[empty_mask, empty_mask], csr_disk.X[empty_mask, empty_mask])\n+    # assert_equal(csr_mem.X[empty_mask, empty_mask], csc_disk.X[empty_mask, empty_mask])\n+\n+\n+def test_backed_indexing(\n+    ondisk_equivalent_adata: tuple[AnnData, AnnData, AnnData, AnnData],\n+    subset_func,\n+    subset_func2,\n+):\n     csr_mem, csr_disk, csc_disk, dense_disk = ondisk_equivalent_adata\n \n     obs_idx = subset_func(csr_mem.obs_names)\n     var_idx = subset_func2(csr_mem.var_names)\n \n     assert_equal(csr_mem[obs_idx, var_idx].X, csr_disk[obs_idx, var_idx].X)\n     assert_equal(csr_mem[obs_idx, var_idx].X, csc_disk[obs_idx, var_idx].X)\n+    assert_equal(csr_mem.X[...], csc_disk.X[...])\n     assert_equal(csr_mem[obs_idx, :].X, dense_disk[obs_idx, :].X)\n+    assert_equal(csr_mem[obs_idx].X, csr_disk[obs_idx].X)\n     assert_equal(csr_mem[:, var_idx].X, dense_disk[:, var_idx].X)\n \n \n+def make_randomized_mask(size: int) -> np.ndarray:\n+    randomized_mask = np.zeros(size, dtype=bool)\n+    inds = np.random.choice(size, 20, replace=False)\n+    inds.sort()\n+    for i in range(0, len(inds) - 1, 2):\n+        randomized_mask[inds[i] : inds[i + 1]] = True\n+    return randomized_mask\n+\n+\n+def make_alternating_mask(size: int, step: int) -> np.ndarray:\n+    mask_alternating = np.ones(size, dtype=bool)\n+    for i in range(0, size, step):  # 5 is too low to trigger new behavior\n+        mask_alternating[i] = False\n+    return mask_alternating\n+\n+\n+# non-random indices, with alternating one false and n true\n+make_alternating_mask_5 = partial(make_alternating_mask, step=5)\n+make_alternating_mask_15 = partial(make_alternating_mask, step=15)\n+\n+\n+def make_one_group_mask(size: int) -> np.ndarray:\n+    one_group_mask = np.zeros(size, dtype=bool)\n+    one_group_mask[1 : size // 2] = True\n+    return one_group_mask\n+\n+\n+def make_one_elem_mask(size: int) -> np.ndarray:\n+    one_elem_mask = np.zeros(size, dtype=bool)\n+    one_elem_mask[size // 4] = True\n+    return one_elem_mask\n+\n+\n+# test behavior from https://github.com/scverse/anndata/pull/1233\n+@pytest.mark.parametrize(\n+    \"make_bool_mask,should_trigger_optimization\",\n+    [\n+        (make_randomized_mask, None),\n+        (make_alternating_mask_15, True),\n+        (make_alternating_mask_5, False),\n+        (make_one_group_mask, True),\n+        (make_one_elem_mask, False),\n+    ],\n+    ids=[\"randomized\", \"alternating_15\", \"alternating_5\", \"one_group\", \"one_elem\"],\n+)\n+def test_consecutive_bool(\n+    mocker: MockerFixture,\n+    ondisk_equivalent_adata: tuple[AnnData, AnnData, AnnData, AnnData],\n+    make_bool_mask: Callable[[int], np.ndarray],\n+    should_trigger_optimization: bool | None,\n+):\n+    \"\"\"Tests for optimization from https://github.com/scverse/anndata/pull/1233\n+\n+    Parameters\n+    ----------\n+    mocker\n+        Mocker object\n+    ondisk_equivalent_adata\n+        AnnData objects with sparse X for testing\n+    make_bool_mask\n+        Function for creating a boolean mask.\n+    should_trigger_optimization\n+        Whether or not a given mask should trigger the optimized behavior.\n+    \"\"\"\n+    _, csr_disk, csc_disk, _ = ondisk_equivalent_adata\n+    mask = make_bool_mask(csr_disk.shape[0])\n+\n+    # indexing needs to be on `X` directly to trigger the optimization.\n+\n+    # `_normalize_indices`, which is used by `AnnData`, converts bools to ints with `np.where`\n+    from anndata._core import sparse_dataset\n+\n+    spy = mocker.spy(sparse_dataset, \"get_compressed_vectors_for_slices\")\n+    assert_equal(csr_disk.X[mask, :], csr_disk.X[np.where(mask)])\n+    if should_trigger_optimization is not None:\n+        assert (\n+            spy.call_count == 1 if should_trigger_optimization else not spy.call_count\n+        )\n+    assert_equal(csc_disk.X[:, mask], csc_disk.X[:, np.where(mask)[0]])\n+    if should_trigger_optimization is not None:\n+        assert (\n+            spy.call_count == 2 if should_trigger_optimization else not spy.call_count\n+        )\n+    assert_equal(csr_disk[mask, :], csr_disk[np.where(mask)])\n+    if should_trigger_optimization is not None:\n+        assert (\n+            spy.call_count == 3 if should_trigger_optimization else not spy.call_count\n+        )\n+    subset = csc_disk[:, mask]\n+    assert_equal(subset, csc_disk[:, np.where(mask)[0]])\n+    if should_trigger_optimization is not None:\n+        assert (\n+            spy.call_count == 4 if should_trigger_optimization else not spy.call_count\n+        )\n+    if should_trigger_optimization is not None and not csc_disk.isbacked:\n+        size = subset.shape[1]\n+        if should_trigger_optimization:\n+            subset_subset_mask = np.ones(size).astype(\"bool\")\n+            subset_subset_mask[size // 2] = False\n+        else:\n+            subset_subset_mask = make_one_elem_mask(size)\n+        assert_equal(\n+            subset[:, subset_subset_mask], subset[:, np.where(subset_subset_mask)[0]]\n+        )\n+        assert (\n+            spy.call_count == 5 if should_trigger_optimization else not spy.call_count\n+        ), f\"Actual count: {spy.call_count}\"\n+\n+\n @pytest.mark.parametrize(\n     [\"sparse_format\", \"append_method\"],\n     [\n         pytest.param(sparse.csr_matrix, sparse.vstack),\n         pytest.param(sparse.csc_matrix, sparse.hstack),\n     ],\n )\n-def test_dataset_append_memory(tmp_path, sparse_format, append_method, diskfmt):\n+def test_dataset_append_memory(\n+    tmp_path: Path,\n+    sparse_format: Callable[[ArrayLike], sparse.spmatrix],\n+    append_method: Callable[[list[sparse.spmatrix]], sparse.spmatrix],\n+    diskfmt: Literal[\"h5ad\", \"zarr\"],\n+):\n     path = (\n         tmp_path / f\"test.{diskfmt.replace('ad', '')}\"\n     )  # diskfmt is either h5ad or zarr\n@@ -115,7 +269,12 @@ def test_dataset_append_memory(tmp_path, sparse_format, append_method, diskfmt):\n         pytest.param(sparse.csc_matrix, sparse.hstack),\n     ],\n )\n-def test_dataset_append_disk(tmp_path, sparse_format, append_method, diskfmt):\n+def test_dataset_append_disk(\n+    tmp_path: Path,\n+    sparse_format: Callable[[ArrayLike], sparse.spmatrix],\n+    append_method: Callable[[list[sparse.spmatrix]], sparse.spmatrix],\n+    diskfmt: Literal[\"h5ad\", \"zarr\"],\n+):\n     path = (\n         tmp_path / f\"test.{diskfmt.replace('ad', '')}\"\n     )  # diskfmt is either h5ad or zarr\n@@ -139,14 +298,48 @@ def test_dataset_append_disk(tmp_path, sparse_format, append_method, diskfmt):\n     assert_equal(fromdisk, frommem)\n \n \n+@pytest.mark.parametrize(\n+    [\"sparse_format\"],\n+    [\n+        pytest.param(sparse.csr_matrix),\n+        pytest.param(sparse.csc_matrix),\n+    ],\n+)\n+def test_indptr_cache(\n+    tmp_path: Path,\n+    sparse_format: Callable[[ArrayLike], sparse.spmatrix],\n+):\n+    path = tmp_path / \"test.zarr\"  # diskfmt is either h5ad or zarr\n+    a = sparse_format(sparse.random(10, 10))\n+    f = zarr.open_group(path, \"a\")\n+    ad._io.specs.write_elem(f, \"X\", a)\n+    store = AccessTrackingStore(path)\n+    store.set_key_trackers([\"X/indptr\"])\n+    f = zarr.open_group(store, \"a\")\n+    a_disk = sparse_dataset(f[\"X\"])\n+    a_disk[:1]\n+    a_disk[3:5]\n+    a_disk[6:7]\n+    a_disk[8:9]\n+    assert (\n+        store.get_access_count(\"X/indptr\") == 2\n+    )  # one each for .zarray and actual access\n+\n+\n @pytest.mark.parametrize(\n     [\"sparse_format\", \"a_shape\", \"b_shape\"],\n     [\n         pytest.param(\"csr\", (100, 100), (100, 200)),\n         pytest.param(\"csc\", (100, 100), (200, 100)),\n     ],\n )\n-def test_wrong_shape(tmp_path, sparse_format, a_shape, b_shape, diskfmt):\n+def test_wrong_shape(\n+    tmp_path: Path,\n+    sparse_format: Literal[\"csr\", \"csc\"],\n+    a_shape: tuple[int, int],\n+    b_shape: tuple[int, int],\n+    diskfmt: Literal[\"h5ad\", \"zarr\"],\n+):\n     path = (\n         tmp_path / f\"test.{diskfmt.replace('ad', '')}\"\n     )  # diskfmt is either h5ad or zarr\n@@ -167,7 +360,22 @@ def test_wrong_shape(tmp_path, sparse_format, a_shape, b_shape, diskfmt):\n         a_disk.append(b_disk)\n \n \n-def test_wrong_formats(tmp_path, diskfmt):\n+def test_reset_group(tmp_path: Path):\n+    path = tmp_path / \"test.zarr\"  # diskfmt is either h5ad or zarr\n+    base = sparse.random(100, 100, format=\"csr\")\n+\n+    if diskfmt == \"zarr\":\n+        f = zarr.open_group(path, \"a\")\n+    else:\n+        f = h5py.File(path, \"a\")\n+\n+    ad._io.specs.write_elem(f, \"base\", base)\n+    disk_mtx = sparse_dataset(f[\"base\"])\n+    with pytest.raises(AttributeError):\n+        disk_mtx.group = f\n+\n+\n+def test_wrong_formats(tmp_path: Path, diskfmt: Literal[\"h5ad\", \"zarr\"]):\n     path = (\n         tmp_path / f\"test.{diskfmt.replace('ad', '')}\"\n     )  # diskfmt is either h5ad or zarr\n@@ -198,7 +406,7 @@ def test_wrong_formats(tmp_path, diskfmt):\n     assert not np.any((pre_checks != post_checks).toarray())\n \n \n-def test_anndata_sparse_compat(tmp_path, diskfmt):\n+def test_anndata_sparse_compat(tmp_path: Path, diskfmt: Literal[\"h5ad\", \"zarr\"]):\n     path = (\n         tmp_path / f\"test.{diskfmt.replace('ad', '')}\"\n     )  # diskfmt is either h5ad or zarr\n@@ -212,3 +420,16 @@ def test_anndata_sparse_compat(tmp_path, diskfmt):\n     ad._io.specs.write_elem(f, \"/\", base)\n     adata = ad.AnnData(sparse_dataset(f[\"/\"]))\n     assert_equal(adata.X, base)\n+\n+\n+def test_backed_sizeof(\n+    ondisk_equivalent_adata: tuple[AnnData, AnnData, AnnData, AnnData],\n+    diskfmt: Literal[\"h5ad\", \"zarr\"],\n+):\n+    csr_mem, csr_disk, csc_disk, _ = ondisk_equivalent_adata\n+\n+    assert csr_mem.__sizeof__() == csr_disk.__sizeof__(with_disk=True)\n+    assert csr_mem.__sizeof__() == csc_disk.__sizeof__(with_disk=True)\n+    assert csr_disk.__sizeof__(with_disk=True) == csc_disk.__sizeof__(with_disk=True)\n+    assert csr_mem.__sizeof__() > csr_disk.__sizeof__()\n+    assert csr_mem.__sizeof__() > csc_disk.__sizeof__()"
        },
        {
            "sha": "9a47e983e1742261e8a0f5ef287ec034674c2496",
            "filename": "anndata/tests/test_base.py",
            "status": "modified",
            "additions": 10,
            "deletions": 0,
            "changes": 10,
            "blob_url": "https://github.com/scverse/anndata/blob/f88f7bd4250b963752d615e491b7e676ce5eb7f0/anndata%2Ftests%2Ftest_base.py",
            "raw_url": "https://github.com/scverse/anndata/raw/f88f7bd4250b963752d615e491b7e676ce5eb7f0/anndata%2Ftests%2Ftest_base.py",
            "contents_url": "https://api.github.com/repos/scverse/anndata/contents/anndata%2Ftests%2Ftest_base.py?ref=f88f7bd4250b963752d615e491b7e676ce5eb7f0",
            "patch": "@@ -12,6 +12,7 @@\n from scipy.sparse import csr_matrix, issparse\n \n from anndata import AnnData\n+from anndata._settings import settings\n from anndata.tests.helpers import assert_equal, gen_adata\n \n # some test objects that we use below\n@@ -399,6 +400,15 @@ def test_slicing_remove_unused_categories():\n     assert adata[2:4].obs[\"k\"].cat.categories.tolist() == [\"b\"]\n \n \n+def test_slicing_dont_remove_unused_categories():\n+    with settings.override(remove_unused_categories=False):\n+        adata = AnnData(\n+            np.array([[1, 2], [3, 4], [5, 6], [7, 8]]), dict(k=[\"a\", \"a\", \"b\", \"b\"])\n+        )\n+        adata._sanitize()\n+        assert adata[2:4].obs[\"k\"].cat.categories.tolist() == [\"a\", \"b\"]\n+\n+\n def test_get_subset_annotation():\n     adata = AnnData(\n         np.array([[1, 2, 3], [4, 5, 6]]),"
        },
        {
            "sha": "e244314367bb06f3cd3370699852d3259136e3c7",
            "filename": "anndata/tests/test_concatenate.py",
            "status": "modified",
            "additions": 98,
            "deletions": 11,
            "changes": 109,
            "blob_url": "https://github.com/scverse/anndata/blob/f88f7bd4250b963752d615e491b7e676ce5eb7f0/anndata%2Ftests%2Ftest_concatenate.py",
            "raw_url": "https://github.com/scverse/anndata/raw/f88f7bd4250b963752d615e491b7e676ce5eb7f0/anndata%2Ftests%2Ftest_concatenate.py",
            "contents_url": "https://api.github.com/repos/scverse/anndata/contents/anndata%2Ftests%2Ftest_concatenate.py?ref=f88f7bd4250b963752d615e491b7e676ce5eb7f0",
            "patch": "@@ -2,16 +2,18 @@\n \n import warnings\n from collections.abc import Hashable\n+from contextlib import nullcontext\n from copy import deepcopy\n from functools import partial, singledispatch\n from itertools import chain, permutations, product\n-from typing import Any, Callable\n+from typing import Any, Callable, Literal\n \n import numpy as np\n import pandas as pd\n import pytest\n from boltons.iterutils import default_exit, remap, research\n from numpy import ma\n+from packaging.version import Version\n from scipy import sparse\n \n from anndata import AnnData, Raw, concat\n@@ -27,9 +29,14 @@\n     as_dense_dask_array,\n     assert_equal,\n     gen_adata,\n+    gen_vstr_recarray,\n )\n from anndata.utils import asarray\n \n+mark_legacy_concatenate = pytest.mark.filterwarnings(\n+    r\"ignore:.*AnnData\\.concatenate is deprecated:FutureWarning\"\n+)\n+\n \n @singledispatch\n def filled_like(a, fill_value=None):\n@@ -93,7 +100,7 @@ def fill_val(request):\n \n \n @pytest.fixture(params=[0, 1])\n-def axis(request):\n+def axis(request) -> Literal[0, 1]:\n     return request.param\n \n \n@@ -145,6 +152,7 @@ def test_concat_interface_errors():\n         concat([])\n \n \n+@mark_legacy_concatenate\n @pytest.mark.parametrize(\n     [\"concat_func\", \"backwards_compat\"],\n     [\n@@ -173,6 +181,7 @@ def test_concatenate_roundtrip(join_type, array_type, concat_func, backwards_com\n     assert_equal(result[orig.obs_names].copy(), orig)\n \n \n+@mark_legacy_concatenate\n def test_concatenate_dense():\n     # dense data\n     X1 = np.array([[1, 2, 3], [4, 5, 6]])\n@@ -248,6 +257,7 @@ def test_concatenate_dense():\n     assert np.allclose(var_ma.compressed(), var_ma_ref.compressed())\n \n \n+@mark_legacy_concatenate\n def test_concatenate_layers(array_type, join_type):\n     adatas = []\n     for _ in range(5):\n@@ -307,6 +317,7 @@ def gen_index(n):\n     ]\n \n \n+@mark_legacy_concatenate\n def test_concatenate_obsm_inner(obsm_adatas):\n     adata = obsm_adatas[0].concatenate(obsm_adatas[1:], join=\"inner\")\n \n@@ -336,6 +347,7 @@ def test_concatenate_obsm_inner(obsm_adatas):\n     pd.testing.assert_frame_equal(true_df, cur_df)\n \n \n+@mark_legacy_concatenate\n def test_concatenate_obsm_outer(obsm_adatas, fill_val):\n     outer = obsm_adatas[0].concatenate(\n         obsm_adatas[1:], join=\"outer\", fill_value=fill_val\n@@ -406,6 +418,7 @@ def test_concat_annot_join(obsm_adatas, join_type):\n     )\n \n \n+@mark_legacy_concatenate\n def test_concatenate_layers_misaligned(array_type, join_type):\n     adatas = []\n     for _ in range(5):\n@@ -419,6 +432,7 @@ def test_concatenate_layers_misaligned(array_type, join_type):\n     assert_equal(merged.X, merged.layers[\"a\"])\n \n \n+@mark_legacy_concatenate\n def test_concatenate_layers_outer(array_type, fill_val):\n     # Testing that issue #368 is fixed\n     a = AnnData(\n@@ -434,6 +448,7 @@ def test_concatenate_layers_outer(array_type, fill_val):\n     )\n \n \n+@mark_legacy_concatenate\n def test_concatenate_fill_value(fill_val):\n     def get_obs_els(adata):\n         return {\n@@ -479,6 +494,7 @@ def get_obs_els(adata):\n         ptr += orig.n_obs\n \n \n+@mark_legacy_concatenate\n def test_concatenate_dense_duplicates():\n     X1 = np.array([[1, 2, 3], [4, 5, 6]])\n     X2 = np.array([[1, 2, 3], [4, 5, 6]])\n@@ -530,6 +546,7 @@ def test_concatenate_dense_duplicates():\n     ]\n \n \n+@mark_legacy_concatenate\n def test_concatenate_sparse():\n     # sparse data\n     from scipy.sparse import csr_matrix\n@@ -575,6 +592,7 @@ def test_concatenate_sparse():\n     ]\n \n \n+@mark_legacy_concatenate\n def test_concatenate_mixed():\n     X1 = sparse.csr_matrix(np.array([[1, 2, 0], [4, 0, 6], [0, 0, 9]]))\n     X2 = sparse.csr_matrix(np.array([[0, 2, 3], [4, 0, 0], [7, 0, 9]]))\n@@ -610,6 +628,7 @@ def test_concatenate_mixed():\n     assert isinstance(adata_all.layers[\"counts\"], sparse.csr_matrix)\n \n \n+@mark_legacy_concatenate\n def test_concatenate_with_raw():\n     # dense data\n     X1 = np.array([[1, 2, 3], [4, 5, 6]])\n@@ -814,7 +833,8 @@ def gen_dim_array(m):\n \n     # Check values of included elements\n     full_inds = np.arange(w_pairwise.shape[axis])\n-    groups = getattr(w_pairwise, dim).groupby(\"orig\").indices\n+    obs_var: pd.DataFrame = getattr(w_pairwise, dim)\n+    groups = obs_var.groupby(\"orig\", observed=True).indices\n     for k, inds in groups.items():\n         orig_arr = getattr(adatas[k], dim_attr)[\"arr\"]\n         full_arr = getattr(w_pairwise, dim_attr)[\"arr\"]\n@@ -1000,6 +1020,15 @@ def gen_something(n):\n     return np.random.choice(options)(n)\n \n \n+def gen_3d_numeric_array(n):\n+    return np.random.randn(n, n, n)\n+\n+\n+def gen_3d_recarray(_):\n+    # Ignoring n as it can get quite slow\n+    return gen_vstr_recarray(8, 3).reshape(2, 2, 2)\n+\n+\n def gen_concat_params(unss, compat2result):\n     value_generators = [\n         lambda x: x,\n@@ -1008,6 +1037,8 @@ def gen_concat_params(unss, compat2result):\n         gen_list,\n         gen_sparse,\n         gen_something,\n+        gen_3d_numeric_array,\n+        gen_3d_recarray,\n     ]\n     for gen, (mode, result) in product(value_generators, compat2result.items()):\n         yield pytest.param(unss, mode, result, gen)\n@@ -1089,7 +1120,7 @@ def test_concatenate_uns(unss, merge_strategy, result, value_gen):\n     print(merge_strategy, \"\\n\", unss, \"\\n\", result)\n     result, *unss = permute_nested_values([result] + unss, value_gen)\n     adatas = [uns_ad(uns) for uns in unss]\n-    with pytest.warns(FutureWarning, match=r\"concatenate method is deprecated\"):\n+    with pytest.warns(FutureWarning, match=r\"concatenate is deprecated\"):\n         merged = AnnData.concatenate(*adatas, uns_merge=merge_strategy).uns\n     assert_equal(merged, result, elem_name=\"uns\")\n \n@@ -1219,6 +1250,32 @@ def test_concat_ordered_categoricals_retained():\n     assert c.obs[\"cat_ordered\"].cat.ordered\n \n \n+def test_concat_categorical_dtype_promotion():\n+    \"\"\"https://github.com/scverse/anndata/issues/1170\n+\n+    When concatenating categorical with other dtype, defer to pandas.\n+    \"\"\"\n+    a = AnnData(\n+        np.ones((3, 3)),\n+        obs=pd.DataFrame(\n+            {\"col\": pd.Categorical([\"a\", \"a\", \"b\"])},\n+            index=[f\"cell_{i:02d}\" for i in range(3)],\n+        ),\n+    )\n+    b = AnnData(\n+        np.ones((3, 3)),\n+        obs=pd.DataFrame(\n+            {\"col\": [\"c\", \"c\", \"c\"]},\n+            index=[f\"cell_{i:02d}\" for i in range(3, 6)],\n+        ),\n+    )\n+\n+    result = concat([a, b])\n+    expected = pd.concat([a.obs, b.obs])\n+\n+    assert_equal(result.obs, expected)\n+\n+\n def test_bool_promotion():\n     np_bool = AnnData(\n         np.ones((5, 1)),\n@@ -1288,14 +1345,24 @@ def test_concat_size_0_dim(axis, join_type, merge_strategy, shape):\n     dim = (\"obs\", \"var\")[axis]\n \n     expected_size = expected_shape(a, b, axis=axis, join=join_type)\n-    result = concat(\n-        {\"a\": a, \"b\": b},\n-        axis=axis,\n-        join=join_type,\n-        merge=merge_strategy,\n-        pairwise=True,\n-        index_unique=\"-\",\n+\n+    ctx_concat_empty = (\n+        pytest.warns(\n+            FutureWarning,\n+            match=r\"The behavior of DataFrame concatenation with empty or all-NA entries is deprecated\",\n+        )\n+        if shape[axis] == 0 and Version(pd.__version__) >= Version(\"2.1\")\n+        else nullcontext()\n     )\n+    with ctx_concat_empty:\n+        result = concat(\n+            {\"a\": a, \"b\": b},\n+            axis=axis,\n+            join=join_type,\n+            merge=merge_strategy,\n+            pairwise=True,\n+            index_unique=\"-\",\n+        )\n     assert result.shape == expected_size\n \n     if join_type == \"outer\":\n@@ -1344,6 +1411,7 @@ def test_concat_outer_aligned_mapping(elem):\n     check_filled_like(result, elem_name=f\"obsm/{elem}\")\n \n \n+@mark_legacy_concatenate\n def test_concatenate_size_0_dim():\n     # https://github.com/scverse/anndata/issues/526\n \n@@ -1495,3 +1563,22 @@ def test_error_on_mixed_device():\n \n     for p in permutations([cp_adata, cp_sparse_adata]):\n         concat(p)\n+\n+\n+def test_concat_on_var_outer_join(array_type):\n+    # https://github.com/scverse/anndata/issues/1286\n+    a = AnnData(\n+        obs=pd.DataFrame(index=[f\"cell_{i:02d}\" for i in range(10)]),\n+        var=pd.DataFrame(index=[f\"gene_{i:02d}\" for i in range(10)]),\n+        layers={\n+            \"X\": array_type(np.ones((10, 10))),\n+        },\n+    )\n+    b = AnnData(\n+        obs=pd.DataFrame(index=[f\"cell_{i:02d}\" for i in range(10)]),\n+        var=pd.DataFrame(index=[f\"gene_{i:02d}\" for i in range(10, 20)]),\n+    )\n+\n+    # This shouldn't error\n+    # TODO: specify expected result while accounting for null value\n+    _ = concat([a, b], join=\"outer\", axis=1)"
        },
        {
            "sha": "659fb98cf0c82960310a3cedaf060ed99f71b880",
            "filename": "anndata/tests/test_concatenate_disk.py",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/scverse/anndata/blob/f88f7bd4250b963752d615e491b7e676ce5eb7f0/anndata%2Ftests%2Ftest_concatenate_disk.py",
            "raw_url": "https://github.com/scverse/anndata/raw/f88f7bd4250b963752d615e491b7e676ce5eb7f0/anndata%2Ftests%2Ftest_concatenate_disk.py",
            "contents_url": "https://api.github.com/repos/scverse/anndata/contents/anndata%2Ftests%2Ftest_concatenate_disk.py?ref=f88f7bd4250b963752d615e491b7e676ce5eb7f0",
            "patch": "@@ -109,7 +109,7 @@ def test_anndatas_without_reindex(\n     M = 50\n     sparse_fmt = \"csr\"\n     adatas = []\n-    for _ in range(5):\n+    for i in range(5):\n         if axis == 0:\n             M = np.random.randint(1, 100)\n         else:\n@@ -122,6 +122,10 @@ def test_anndatas_without_reindex(\n             sparse_fmt=sparse_fmt,\n             **GEN_ADATA_OOC_CONCAT_ARGS,\n         )\n+        if axis == 0:\n+            a.obs_names = f\"{i}-\" + a.obs_names\n+        else:\n+            a.var_names = f\"{i}-\" + a.var_names\n         adatas.append(a)\n \n     assert_eq_concat_on_disk("
        },
        {
            "sha": "56cb0f8c8a83f9bab918e157cb4e1ece93538a26",
            "filename": "anndata/tests/test_dask.py",
            "status": "modified",
            "additions": 14,
            "deletions": 15,
            "changes": 29,
            "blob_url": "https://github.com/scverse/anndata/blob/f88f7bd4250b963752d615e491b7e676ce5eb7f0/anndata%2Ftests%2Ftest_dask.py",
            "raw_url": "https://github.com/scverse/anndata/raw/f88f7bd4250b963752d615e491b7e676ce5eb7f0/anndata%2Ftests%2Ftest_dask.py",
            "contents_url": "https://api.github.com/repos/scverse/anndata/contents/anndata%2Ftests%2Ftest_dask.py?ref=f88f7bd4250b963752d615e491b7e676ce5eb7f0",
            "patch": "@@ -107,21 +107,20 @@ def test_dask_distributed_write(adata, tmp_path, diskfmt):\n     pth = tmp_path / f\"test_write.{diskfmt}\"\n     g = as_group(pth, mode=\"w\")\n \n-    with dd.LocalCluster(n_workers=1, threads_per_worker=1, processes=False) as cluster:\n-        with dd.Client(cluster):\n-            M, N = adata.X.shape\n-            adata.obsm[\"a\"] = da.random.random((M, 10))\n-            adata.obsm[\"b\"] = da.random.random((M, 10))\n-            adata.varm[\"a\"] = da.random.random((N, 10))\n-            orig = adata\n-            if diskfmt == \"h5ad\":\n-                with pytest.raises(\n-                    ValueError, match=\"Cannot write dask arrays to hdf5\"\n-                ):\n-                    write_elem(g, \"\", orig)\n-                return\n-            write_elem(g, \"\", orig)\n-            curr = read_elem(g)\n+    with dd.LocalCluster(\n+        n_workers=1, threads_per_worker=1, processes=False\n+    ) as cluster, dd.Client(cluster):\n+        M, N = adata.X.shape\n+        adata.obsm[\"a\"] = da.random.random((M, 10))\n+        adata.obsm[\"b\"] = da.random.random((M, 10))\n+        adata.varm[\"a\"] = da.random.random((N, 10))\n+        orig = adata\n+        if diskfmt == \"h5ad\":\n+            with pytest.raises(ValueError, match=\"Cannot write dask arrays to hdf5\"):\n+                write_elem(g, \"\", orig)\n+            return\n+        write_elem(g, \"\", orig)\n+        curr = read_elem(g)\n \n     with pytest.raises(Exception):\n         assert_equal(curr.obsm[\"a\"], curr.obsm[\"b\"])"
        },
        {
            "sha": "8ce952ad60bd0ff7ef564986c501259f576fcb6a",
            "filename": "anndata/tests/test_dask_view_mem.py",
            "status": "modified",
            "additions": 12,
            "deletions": 3,
            "changes": 15,
            "blob_url": "https://github.com/scverse/anndata/blob/f88f7bd4250b963752d615e491b7e676ce5eb7f0/anndata%2Ftests%2Ftest_dask_view_mem.py",
            "raw_url": "https://github.com/scverse/anndata/raw/f88f7bd4250b963752d615e491b7e676ce5eb7f0/anndata%2Ftests%2Ftest_dask_view_mem.py",
            "contents_url": "https://api.github.com/repos/scverse/anndata/contents/anndata%2Ftests%2Ftest_dask_view_mem.py?ref=f88f7bd4250b963752d615e491b7e676ce5eb7f0",
            "patch": "@@ -1,9 +1,14 @@\n from __future__ import annotations\n \n+from typing import TYPE_CHECKING\n+\n import pytest\n \n import anndata as ad\n \n+if TYPE_CHECKING:\n+    import pandas as pd\n+\n pytest.importorskip(\"pytest_memray\")\n \n # ------------------------------------------------------------------------------\n@@ -130,7 +135,11 @@ def test_modify_view_X_memory(mapping_name, give_chunks):\n     subset = adata[:N, :N]\n     assert subset.is_view\n     m = subset.X\n-    m[0, 0] = 100\n+    with pytest.warns(\n+        ad.ImplicitModificationWarning,\n+        match=\"Trying to modify attribute `.X` of view, initializing view as actual.\",\n+    ):\n+        m[0, 0] = 100\n \n \n # Normally should expect something around 90 kbs\n@@ -155,5 +164,5 @@ def test_modify_view_mapping_obs_var_memory(attr_name, give_chunks):\n     )\n     subset = adata[:N, :N]\n     assert subset.is_view\n-    m = getattr(subset, attr_name)[\"m\"]\n-    m[0] = 100\n+    m: pd.Series = getattr(subset, attr_name)[\"m\"]\n+    m.iloc[0] = 100"
        },
        {
            "sha": "39176e315219f38e7e48a836b3b5571b7667bdd3",
            "filename": "anndata/tests/test_deprecations.py",
            "status": "modified",
            "additions": 34,
            "deletions": 19,
            "changes": 53,
            "blob_url": "https://github.com/scverse/anndata/blob/f88f7bd4250b963752d615e491b7e676ce5eb7f0/anndata%2Ftests%2Ftest_deprecations.py",
            "raw_url": "https://github.com/scverse/anndata/raw/f88f7bd4250b963752d615e491b7e676ce5eb7f0/anndata%2Ftests%2Ftest_deprecations.py",
            "contents_url": "https://api.github.com/repos/scverse/anndata/contents/anndata%2Ftests%2Ftest_deprecations.py?ref=f88f7bd4250b963752d615e491b7e676ce5eb7f0",
            "patch": "@@ -10,10 +10,12 @@\n import h5py\n import numpy as np\n import pytest\n+import zarr\n from scipy import sparse\n \n import anndata as ad\n from anndata import AnnData\n+from anndata.experimental import CSRDataset, write_elem\n from anndata.tests.helpers import assert_equal\n \n \n@@ -38,26 +40,24 @@ def test_get_obsvar_array_warn(adata):\n         adata._get_var_array(\"s1\")\n \n \n-# TODO: Why doesn’t this mark work?\n-# @pytest.mark.filterwarnings(\"ignore::DeprecationWarning\")\n+@pytest.mark.filterwarnings(\"ignore::DeprecationWarning\")\n def test_get_obsvar_array(adata):\n-    with pytest.warns(DeprecationWarning):  # Just to hide warnings\n-        assert np.allclose(adata._get_obs_array(\"a\"), adata.obs_vector(\"a\"))\n-        assert np.allclose(\n-            adata._get_obs_array(\"a\", layer=\"x2\"),\n-            adata.obs_vector(\"a\", layer=\"x2\"),\n-        )\n-        assert np.allclose(\n-            adata._get_obs_array(\"a\", use_raw=True), adata.raw.obs_vector(\"a\")\n-        )\n-        assert np.allclose(adata._get_var_array(\"s1\"), adata.var_vector(\"s1\"))\n-        assert np.allclose(\n-            adata._get_var_array(\"s1\", layer=\"x2\"),\n-            adata.var_vector(\"s1\", layer=\"x2\"),\n-        )\n-        assert np.allclose(\n-            adata._get_var_array(\"s1\", use_raw=True), adata.raw.var_vector(\"s1\")\n-        )\n+    assert np.allclose(adata._get_obs_array(\"a\"), adata.obs_vector(\"a\"))\n+    assert np.allclose(\n+        adata._get_obs_array(\"a\", layer=\"x2\"),\n+        adata.obs_vector(\"a\", layer=\"x2\"),\n+    )\n+    assert np.allclose(\n+        adata._get_obs_array(\"a\", use_raw=True), adata.raw.obs_vector(\"a\")\n+    )\n+    assert np.allclose(adata._get_var_array(\"s1\"), adata.var_vector(\"s1\"))\n+    assert np.allclose(\n+        adata._get_var_array(\"s1\", layer=\"x2\"),\n+        adata.var_vector(\"s1\", layer=\"x2\"),\n+    )\n+    assert np.allclose(\n+        adata._get_var_array(\"s1\", use_raw=True), adata.raw.var_vector(\"s1\")\n+    )\n \n \n def test_obsvar_vector_Xlayer(adata):\n@@ -144,3 +144,18 @@ def test_deprecated_sparse_dataset_values():\n \n     with pytest.warns(FutureWarning, match=\"Please use .format\"):\n         mtx_backed.format_str\n+\n+\n+def test_deprecated_sparse_dataset():\n+    from anndata._core.sparse_dataset import SparseDataset\n+\n+    mem_X = sparse.random(50, 50, format=\"csr\")\n+    g = zarr.group()\n+    write_elem(g, \"X\", mem_X)\n+    with pytest.warns(FutureWarning, match=\"SparseDataset is deprecated\"):\n+        X = SparseDataset(g[\"X\"])\n+\n+    assert isinstance(X, CSRDataset)\n+\n+    with pytest.warns(FutureWarning, match=\"SparseDataset is deprecated\"):\n+        assert isinstance(X, SparseDataset)"
        },
        {
            "sha": "f7791de62e43af6462b76d097766233108eaafa2",
            "filename": "anndata/tests/test_hdf5_backing.py",
            "status": "modified",
            "additions": 12,
            "deletions": 8,
            "changes": 20,
            "blob_url": "https://github.com/scverse/anndata/blob/f88f7bd4250b963752d615e491b7e676ce5eb7f0/anndata%2Ftests%2Ftest_hdf5_backing.py",
            "raw_url": "https://github.com/scverse/anndata/raw/f88f7bd4250b963752d615e491b7e676ce5eb7f0/anndata%2Ftests%2Ftest_hdf5_backing.py",
            "contents_url": "https://api.github.com/repos/scverse/anndata/contents/anndata%2Ftests%2Ftest_hdf5_backing.py?ref=f88f7bd4250b963752d615e491b7e676ce5eb7f0",
            "patch": "@@ -81,6 +81,7 @@ def as_dense(request):\n \n \n # TODO: Check to make sure obs, obsm, layers, ... are written and read correctly as well\n+@pytest.mark.filterwarnings(\"error\")\n def test_read_write_X(tmp_path, mtx_format, backed_mode, as_dense):\n     base_pth = Path(tmp_path)\n     orig_pth = base_pth / \"orig.h5ad\"\n@@ -89,11 +90,11 @@ def test_read_write_X(tmp_path, mtx_format, backed_mode, as_dense):\n     orig = ad.AnnData(mtx_format(asarray(sparse.random(10, 10, format=\"csr\"))))\n     orig.write(orig_pth)\n \n-    backed = ad.read(orig_pth, backed=backed_mode)\n+    backed = ad.read_h5ad(orig_pth, backed=backed_mode)\n     backed.write(backed_pth, as_dense=as_dense)\n     backed.file.close()\n \n-    from_backed = ad.read(backed_pth)\n+    from_backed = ad.read_h5ad(backed_pth)\n     assert np.all(asarray(orig.X) == asarray(from_backed.X))\n \n \n@@ -192,8 +193,8 @@ def test_backed_raw_subset(tmp_path, array_type, subset_func, subset_func2):\n     var_idx = subset_func2(mem_adata.var_names)\n     if (\n         array_type is asarray\n-        and isinstance(obs_idx, (np.ndarray, sparse.spmatrix))\n-        and isinstance(var_idx, (np.ndarray, sparse.spmatrix))\n+        and isinstance(obs_idx, (list, np.ndarray, sparse.spmatrix))\n+        and isinstance(var_idx, (list, np.ndarray, sparse.spmatrix))\n     ):\n         pytest.xfail(\n             \"Fancy indexing does not work with multiple arrays on a h5py.Dataset\"\n@@ -303,10 +304,13 @@ def test_backed_modification_sparse(adata, backing_h5ad, sparse_format):\n     assert adata.filename == backing_h5ad\n     assert adata.isbacked\n \n-    adata.X[0, [0, 2]] = 10\n-    adata.X[1, [0, 2]] = [11, 12]\n-    with pytest.raises(ValueError):\n-        adata.X[2, 1] = 13\n+    with pytest.warns(\n+        PendingDeprecationWarning, match=r\"__setitem__ will likely be removed\"\n+    ):\n+        adata.X[0, [0, 2]] = 10\n+        adata.X[1, [0, 2]] = [11, 12]\n+        with pytest.raises(ValueError):\n+            adata.X[2, 1] = 13\n \n     assert adata.isbacked\n "
        },
        {
            "sha": "7f7dac4dd220daf24075772bc1d8742842130cba",
            "filename": "anndata/tests/test_io_elementwise.py",
            "status": "modified",
            "additions": 31,
            "deletions": 1,
            "changes": 32,
            "blob_url": "https://github.com/scverse/anndata/blob/f88f7bd4250b963752d615e491b7e676ce5eb7f0/anndata%2Ftests%2Ftest_io_elementwise.py",
            "raw_url": "https://github.com/scverse/anndata/raw/f88f7bd4250b963752d615e491b7e676ce5eb7f0/anndata%2Ftests%2Ftest_io_elementwise.py",
            "contents_url": "https://api.github.com/repos/scverse/anndata/contents/anndata%2Ftests%2Ftest_io_elementwise.py?ref=f88f7bd4250b963752d615e491b7e676ce5eb7f0",
            "patch": "@@ -298,4 +298,34 @@ def test_read_zarr_from_group(tmp_path, consolidated):\n         read_func = zarr.open\n \n     with read_func(pth) as z:\n-        assert_equal(ad.read_zarr(z[\"table/table\"]), adata)\n+        expected = ad.read_zarr(z[\"table/table\"])\n+    assert_equal(adata, expected)\n+\n+\n+def test_dataframe_column_uniqueness(store):\n+    repeated_cols = pd.DataFrame(np.ones((3, 2)), columns=[\"a\", \"a\"])\n+\n+    with pytest_8_raises(\n+        ValueError,\n+        match=r\"Found repeated column names: \\['a'\\]\\. Column names must be unique\\.\",\n+    ):\n+        write_elem(store, \"repeated_cols\", repeated_cols)\n+\n+    index_shares_col_name = pd.DataFrame(\n+        {\"col_name\": [1, 2, 3]}, index=pd.Index([1, 3, 2], name=\"col_name\")\n+    )\n+\n+    with pytest_8_raises(\n+        ValueError,\n+        match=r\"DataFrame\\.index\\.name \\('col_name'\\) is also used by a column whose values are different\\.\",\n+    ):\n+        write_elem(store, \"index_shares_col_name\", index_shares_col_name)\n+\n+    index_shared_okay = pd.DataFrame(\n+        {\"col_name\": [1, 2, 3]}, index=pd.Index([1, 2, 3], name=\"col_name\")\n+    )\n+\n+    write_elem(store, \"index_shared_okay\", index_shared_okay)\n+    result = read_elem(store[\"index_shared_okay\"])\n+\n+    assert_equal(result, index_shared_okay)"
        },
        {
            "sha": "803a4ad72b24be91f0c9f999a17af8c346baa896",
            "filename": "anndata/tests/test_io_utils.py",
            "status": "modified",
            "additions": 27,
            "deletions": 10,
            "changes": 37,
            "blob_url": "https://github.com/scverse/anndata/blob/f88f7bd4250b963752d615e491b7e676ce5eb7f0/anndata%2Ftests%2Ftest_io_utils.py",
            "raw_url": "https://github.com/scverse/anndata/raw/f88f7bd4250b963752d615e491b7e676ce5eb7f0/anndata%2Ftests%2Ftest_io_utils.py",
            "contents_url": "https://api.github.com/repos/scverse/anndata/contents/anndata%2Ftests%2Ftest_io_utils.py?ref=f88f7bd4250b963752d615e491b7e676ce5eb7f0",
            "patch": "@@ -1,6 +1,7 @@\n from __future__ import annotations\n \n-from contextlib import suppress\n+from contextlib import AbstractContextManager, suppress\n+from typing import TYPE_CHECKING\n \n import h5py\n import pandas as pd\n@@ -9,13 +10,15 @@\n \n import anndata as ad\n from anndata._io.specs.registry import IORegistryError\n-from anndata._io.utils import (\n-    report_read_key_on_error,\n-)\n+from anndata._io.utils import report_read_key_on_error\n from anndata.compat import _clean_uns\n from anndata.experimental import read_elem, write_elem\n from anndata.tests.helpers import pytest_8_raises\n \n+if TYPE_CHECKING:\n+    from collections.abc import Callable\n+    from pathlib import Path\n+\n \n @pytest.fixture(params=[\"h5ad\", \"zarr\"])\n def diskfmt(request):\n@@ -29,20 +32,32 @@ def diskfmt(request):\n         pytest.param(lambda p: h5py.File(p / \"test.h5\", mode=\"a\"), id=\"h5py\"),\n     ],\n )\n-def test_key_error(tmp_path, group_fn):\n+@pytest.mark.parametrize(\"nested\", [True, False], ids=[\"nested\", \"root\"])\n+def test_key_error(\n+    tmp_path, group_fn: Callable[[Path], zarr.Group | h5py.Group], nested: bool\n+):\n     @report_read_key_on_error\n     def read_attr(_):\n         raise NotImplementedError()\n \n     group = group_fn(tmp_path)\n-    with group if hasattr(group, \"__enter__\") else suppress():\n+    with group if isinstance(group, AbstractContextManager) else suppress():\n+        if nested:\n+            group = group.create_group(\"nested\")\n+            path = \"/nested\"\n+        else:\n+            path = \"/\"\n         group[\"X\"] = [1, 2, 3]\n         group.create_group(\"group\")\n \n-        with pytest_8_raises(NotImplementedError, match=r\"/X\"):\n+        with pytest_8_raises(\n+            NotImplementedError, match=rf\"reading key 'X'.*from {path}$\"\n+        ):\n             read_attr(group[\"X\"])\n \n-        with pytest_8_raises(NotImplementedError, match=r\"/group\"):\n+        with pytest_8_raises(\n+            NotImplementedError, match=rf\"reading key 'group'.*from {path}$\"\n+        ):\n             read_attr(group[\"group\"])\n \n \n@@ -53,7 +68,9 @@ def test_write_error_info(diskfmt, tmp_path):\n     # Assuming we don't define a writer for tuples\n     a = ad.AnnData(uns={\"a\": {\"b\": {\"c\": (1, 2, 3)}}})\n \n-    with pytest_8_raises(IORegistryError, match=r\"Error raised while writing key 'c'\"):\n+    with pytest_8_raises(\n+        IORegistryError, match=r\"Error raised while writing key 'c'.*to /uns/a/b\"\n+    ):\n         write(a)\n \n \n@@ -89,7 +106,7 @@ class Foo:\n     # (?!...) is a negative lookahead\n     # (?s) enables the dot to match newlines\n     # https://stackoverflow.com/a/406408/130164 <- copilot suggested lol\n-    pattern = r\"(?s)((?!Error raised while writing key '/?a').)*$\"\n+    pattern = r\"(?s)^((?!Error raised while writing key '/?a').)*$\"\n \n     with pytest_8_raises(IORegistryError, match=pattern):\n         write_elem(group, \"/\", {\"a\": {\"b\": Foo()}})"
        },
        {
            "sha": "29ab2d9630b64ecd3311d559c93d99dae5059d99",
            "filename": "anndata/tests/test_io_warnings.py",
            "status": "modified",
            "additions": 29,
            "deletions": 3,
            "changes": 32,
            "blob_url": "https://github.com/scverse/anndata/blob/f88f7bd4250b963752d615e491b7e676ce5eb7f0/anndata%2Ftests%2Ftest_io_warnings.py",
            "raw_url": "https://github.com/scverse/anndata/raw/f88f7bd4250b963752d615e491b7e676ce5eb7f0/anndata%2Ftests%2Ftest_io_warnings.py",
            "contents_url": "https://api.github.com/repos/scverse/anndata/contents/anndata%2Ftests%2Ftest_io_warnings.py?ref=f88f7bd4250b963752d615e491b7e676ce5eb7f0",
            "patch": "@@ -1,10 +1,13 @@\n from __future__ import annotations\n \n+import re\n import warnings\n from importlib.util import find_spec\n from pathlib import Path\n \n+import h5py\n import pytest\n+from packaging.version import Version\n \n import anndata as ad\n from anndata.tests.helpers import gen_adata\n@@ -14,18 +17,41 @@\n def test_old_format_warning_thrown():\n     import scanpy as sc\n \n-    with pytest.warns(ad._warnings.OldFormatWarning):\n-        pth = Path(sc.datasets.__file__).parent / \"10x_pbmc68k_reduced.h5ad\"\n+    pth = Path(sc.datasets.__file__).parent / \"10x_pbmc68k_reduced.h5ad\"\n+    # TODO: with Pytest 8, all this can be a\n+    #       `with pytest.warns(...), pytest.warns(...):`\n+    with warnings.catch_warnings(record=True) as record:\n+        warnings.simplefilter(\"always\", ad.OldFormatWarning)\n+        warnings.simplefilter(\"always\", FutureWarning)\n         ad.read_h5ad(pth)\n \n+    assert any(issubclass(w.category, ad.OldFormatWarning) for w in record), [\n+        w.message for w in record if not issubclass(w.category, FutureWarning)\n+    ]\n+    assert any(\n+        issubclass(w.category, FutureWarning)\n+        and re.match(\n+            r\"Moving element from \\.uns\\['neighbors']\\['distances'] to \\.obsp\\['distances']\\.\",\n+            str(w.message),\n+        )\n+        for w in record\n+    ), [w.message for w in record if not issubclass(w.category, ad.OldFormatWarning)]\n+\n \n def test_old_format_warning_not_thrown(tmp_path):\n     pth = tmp_path / \"current.h5ad\"\n     adata = gen_adata((20, 10))\n     adata.write_h5ad(pth)\n \n     with warnings.catch_warnings(record=True) as record:\n-        warnings.simplefilter(\"always\", ad._warnings.OldFormatWarning)\n+        warnings.simplefilter(\"always\", ad.OldFormatWarning)\n+        if Version(h5py.__version__) < Version(\"3.2\"):\n+            # https://github.com/h5py/h5py/issues/1808\n+            warnings.filterwarnings(\n+                \"ignore\",\n+                r\"Passing None into shape arguments as an alias for \\(\\) is deprecated\\.\",\n+                category=DeprecationWarning,\n+            )\n \n         ad.read_h5ad(pth)\n "
        },
        {
            "sha": "0eadffdeba49f69908075461ecc25ff9066cc84a",
            "filename": "anndata/tests/test_layers.py",
            "status": "modified",
            "additions": 23,
            "deletions": 1,
            "changes": 24,
            "blob_url": "https://github.com/scverse/anndata/blob/f88f7bd4250b963752d615e491b7e676ce5eb7f0/anndata%2Ftests%2Ftest_layers.py",
            "raw_url": "https://github.com/scverse/anndata/raw/f88f7bd4250b963752d615e491b7e676ce5eb7f0/anndata%2Ftests%2Ftest_layers.py",
            "contents_url": "https://api.github.com/repos/scverse/anndata/contents/anndata%2Ftests%2Ftest_layers.py?ref=f88f7bd4250b963752d615e491b7e676ce5eb7f0",
            "patch": "@@ -6,6 +6,7 @@\n import numpy as np\n import pandas as pd\n import pytest\n+from numba.core.errors import NumbaDeprecationWarning\n \n from anndata import AnnData, read_h5ad, read_loom\n from anndata.tests.helpers import gen_typed_df_t2_size\n@@ -78,7 +79,15 @@ def test_readwrite(backing_h5ad):\n def test_readwrite_loom(tmp_path):\n     loom_path = tmp_path / \"test.loom\"\n     adata = AnnData(X=X, layers=dict(L=L.copy()))\n-    adata.write_loom(loom_path)\n+\n+    with warnings.catch_warnings():\n+        warnings.filterwarnings(\"ignore\", category=NumbaDeprecationWarning)\n+        warnings.filterwarnings(\n+            \"ignore\",\n+            message=r\"datetime.datetime.utcnow\\(\\) is deprecated\",\n+            category=DeprecationWarning,\n+        )\n+        adata.write_loom(loom_path)\n     adata_read = read_loom(loom_path, X_name=\"\")\n \n     assert adata.layers.keys() == adata_read.layers.keys()\n@@ -95,3 +104,16 @@ def test_copy():\n     bdata = adata.copy()\n     adata.layers[\"L\"] += 10\n     assert np.all(adata.layers[\"L\"] != bdata.layers[\"L\"])  # 201\n+\n+\n+def test_shape_error():\n+    adata = AnnData(X=X)\n+    with pytest.raises(\n+        ValueError,\n+        match=(\n+            r\"Value passed for key 'L' is of incorrect shape\\. \"\n+            r\"Values of layers must match dimensions \\('obs', 'var'\\) of parent\\. \"\n+            r\"Value had shape \\(4, 3\\) while it should have had \\(3, 3\\)\\.\"\n+        ),\n+    ):\n+        adata.layers[\"L\"] = np.zeros((X.shape[0] + 1, X.shape[1]))"
        },
        {
            "sha": "c995cc12a0b0d032d095f3b3719f2a204b135147",
            "filename": "anndata/tests/test_obsmvarm.py",
            "status": "modified",
            "additions": 19,
            "deletions": 7,
            "changes": 26,
            "blob_url": "https://github.com/scverse/anndata/blob/f88f7bd4250b963752d615e491b7e676ce5eb7f0/anndata%2Ftests%2Ftest_obsmvarm.py",
            "raw_url": "https://github.com/scverse/anndata/raw/f88f7bd4250b963752d615e491b7e676ce5eb7f0/anndata%2Ftests%2Ftest_obsmvarm.py",
            "contents_url": "https://api.github.com/repos/scverse/anndata/contents/anndata%2Ftests%2Ftest_obsmvarm.py?ref=f88f7bd4250b963752d615e491b7e676ce5eb7f0",
            "patch": "@@ -6,7 +6,7 @@\n import pytest\n from scipy import sparse\n \n-import anndata\n+from anndata import AnnData\n \n M, N = (100, 100)\n \n@@ -19,10 +19,10 @@ def adata():\n         index=[f\"cell{i:03d}\" for i in range(N)],\n     )\n     var = pd.DataFrame(index=[f\"gene{i:03d}\" for i in range(N)])\n-    return anndata.AnnData(X, obs=obs, var=var)\n+    return AnnData(X, obs=obs, var=var)\n \n \n-def test_assignment_dict(adata):\n+def test_assignment_dict(adata: AnnData):\n     d_obsm = dict(\n         a=pd.DataFrame(\n             dict(a1=np.ones(M), a2=[f\"a{i}\" for i in range(M)]),\n@@ -45,7 +45,7 @@ def test_assignment_dict(adata):\n         assert np.all(adata.varm[k] == v)\n \n \n-def test_setting_ndarray(adata):\n+def test_setting_ndarray(adata: AnnData):\n     adata.obsm[\"a\"] = np.ones((M, 10))\n     adata.varm[\"a\"] = np.ones((N, 10))\n     assert np.all(adata.obsm[\"a\"] == np.ones((M, 10)))\n@@ -63,7 +63,7 @@ def test_setting_ndarray(adata):\n     assert h == joblib.hash(adata)\n \n \n-def test_setting_dataframe(adata):\n+def test_setting_dataframe(adata: AnnData):\n     obsm_df = pd.DataFrame(dict(b_1=np.ones(M), b_2=[\"a\"] * M), index=adata.obs_names)\n     varm_df = pd.DataFrame(dict(b_1=np.ones(N), b_2=[\"a\"] * N), index=adata.var_names)\n \n@@ -83,7 +83,7 @@ def test_setting_dataframe(adata):\n         adata.varm[\"c\"] = bad_varm_df\n \n \n-def test_setting_sparse(adata):\n+def test_setting_sparse(adata: AnnData):\n     obsm_sparse = sparse.random(M, 100)\n     adata.obsm[\"a\"] = obsm_sparse\n     assert not np.any((adata.obsm[\"a\"] != obsm_sparse).data)\n@@ -105,7 +105,7 @@ def test_setting_sparse(adata):\n     assert h == joblib.hash(adata)\n \n \n-def test_setting_daskarray(adata):\n+def test_setting_daskarray(adata: AnnData):\n     import dask.array as da\n \n     adata.obsm[\"a\"] = da.ones((M, 10))\n@@ -125,3 +125,15 @@ def test_setting_daskarray(adata):\n     with pytest.raises(ValueError):\n         adata.varm[\"b\"] = da.ones((int(N * 2), 10))\n     assert h == joblib.hash(adata)\n+\n+\n+def test_shape_error(adata: AnnData):\n+    with pytest.raises(\n+        ValueError,\n+        match=(\n+            r\"Value passed for key 'b' is of incorrect shape\\. \"\n+            r\"Values of obsm must match dimensions \\('obs',\\) of parent\\. \"\n+            r\"Value had shape \\(101,\\) while it should have had \\(100,\\)\\.\"\n+        ),\n+    ):\n+        adata.obsm[\"b\"] = np.zeros((adata.shape[0] + 1, adata.shape[0]))"
        },
        {
            "sha": "c39782e8d8a8c57d61bbfc23a7c951e48f86c849",
            "filename": "anndata/tests/test_obspvarp.py",
            "status": "modified",
            "additions": 19,
            "deletions": 7,
            "changes": 26,
            "blob_url": "https://github.com/scverse/anndata/blob/f88f7bd4250b963752d615e491b7e676ce5eb7f0/anndata%2Ftests%2Ftest_obspvarp.py",
            "raw_url": "https://github.com/scverse/anndata/raw/f88f7bd4250b963752d615e491b7e676ce5eb7f0/anndata%2Ftests%2Ftest_obspvarp.py",
            "contents_url": "https://api.github.com/repos/scverse/anndata/contents/anndata%2Ftests%2Ftest_obspvarp.py?ref=f88f7bd4250b963752d615e491b7e676ce5eb7f0",
            "patch": "@@ -9,7 +9,7 @@\n import pytest\n from scipy import sparse\n \n-import anndata\n+from anndata import AnnData\n from anndata.tests.helpers import gen_typed_df_t2_size\n from anndata.utils import asarray\n \n@@ -24,10 +24,10 @@ def adata():\n         index=[f\"cell{i:03d}\" for i in range(M)],\n     )\n     var = pd.DataFrame(index=[f\"gene{i:03d}\" for i in range(N)])\n-    return anndata.AnnData(X, obs=obs, var=var)\n+    return AnnData(X, obs=obs, var=var)\n \n \n-def test_assigmnent_dict(adata):\n+def test_assigmnent_dict(adata: AnnData):\n     d_obsp = dict(\n         a=pd.DataFrame(np.ones((M, M)), columns=adata.obs_names, index=adata.obs_names),\n         b=np.zeros((M, M)),\n@@ -46,7 +46,7 @@ def test_assigmnent_dict(adata):\n         assert np.all(asarray(adata.varp[k]) == asarray(v))\n \n \n-def test_setting_ndarray(adata):\n+def test_setting_ndarray(adata: AnnData):\n     adata.obsp[\"a\"] = np.ones((M, M))\n     adata.varp[\"a\"] = np.ones((N, N))\n     assert np.all(adata.obsp[\"a\"] == np.ones((M, M)))\n@@ -64,7 +64,7 @@ def test_setting_ndarray(adata):\n     assert h == joblib.hash(adata)\n \n \n-def test_setting_sparse(adata):\n+def test_setting_sparse(adata: AnnData):\n     obsp_sparse = sparse.random(M, M)\n     adata.obsp[\"a\"] = obsp_sparse\n     assert not np.any((adata.obsp[\"a\"] != obsp_sparse).data)\n@@ -95,7 +95,7 @@ def test_setting_sparse(adata):\n     ],\n     ids=[\"heterogeneous\", \"homogeneous\"],\n )\n-def test_setting_dataframe(adata, field, dim, homogenous, df, dtype):\n+def test_setting_dataframe(adata: AnnData, field, dim, homogenous, df, dtype):\n     if homogenous:\n         with pytest.warns(UserWarning, match=rf\"{field.title()} 'df'.*dtype object\"):\n             getattr(adata, field)[\"df\"] = df(dim)\n@@ -107,7 +107,7 @@ def test_setting_dataframe(adata, field, dim, homogenous, df, dtype):\n     assert np.issubdtype(getattr(adata, field)[\"df\"].dtype, dtype)\n \n \n-def test_setting_daskarray(adata):\n+def test_setting_daskarray(adata: AnnData):\n     import dask.array as da\n \n     adata.obsp[\"a\"] = da.ones((M, M))\n@@ -127,3 +127,15 @@ def test_setting_daskarray(adata):\n     with pytest.raises(ValueError):\n         adata.varp[\"b\"] = da.ones((N, int(N * 2)))\n     assert h == joblib.hash(adata)\n+\n+\n+def test_shape_error(adata: AnnData):\n+    with pytest.raises(\n+        ValueError,\n+        match=(\n+            r\"Value passed for key 'a' is of incorrect shape\\. \"\n+            r\"Values of obsp must match dimensions \\('obs', 'obs'\\) of parent\\. \"\n+            r\"Value had shape \\(201, 200\\) while it should have had \\(200, 200\\)\\.\"\n+        ),\n+    ):\n+        adata.obsp[\"a\"] = np.zeros((adata.shape[0] + 1, adata.shape[0]))"
        },
        {
            "sha": "b51376b9a65f079d569b382098bf05d9024a3d70",
            "filename": "anndata/tests/test_raw.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/scverse/anndata/blob/f88f7bd4250b963752d615e491b7e676ce5eb7f0/anndata%2Ftests%2Ftest_raw.py",
            "raw_url": "https://github.com/scverse/anndata/raw/f88f7bd4250b963752d615e491b7e676ce5eb7f0/anndata%2Ftests%2Ftest_raw.py",
            "contents_url": "https://api.github.com/repos/scverse/anndata/contents/anndata%2Ftests%2Ftest_raw.py?ref=f88f7bd4250b963752d615e491b7e676ce5eb7f0",
            "patch": "@@ -81,7 +81,7 @@ def test_raw_of_view(adata_raw: ad.AnnData):\n \n def test_raw_rw(adata_raw: ad.AnnData, backing_h5ad):\n     adata_raw.write(backing_h5ad)\n-    adata_read = ad.read(backing_h5ad)\n+    adata_read = ad.read_h5ad(backing_h5ad)\n \n     assert_equal(adata_read, adata_raw, exact=True)\n \n@@ -96,7 +96,7 @@ def test_raw_view_rw(adata_raw: ad.AnnData, backing_h5ad):\n     assert_equal(adata_raw_view, adata_raw)\n     with pytest.warns(ImplicitModificationWarning, match=\"initializing view as actual\"):\n         adata_raw_view.write(backing_h5ad)\n-    adata_read = ad.read(backing_h5ad)\n+    adata_read = ad.read_h5ad(backing_h5ad)\n \n     assert_equal(adata_read, adata_raw_view, exact=True)\n "
        },
        {
            "sha": "4521936dd3a2efad953fd5e30eb3e0bfa75bd760",
            "filename": "anndata/tests/test_readwrite.py",
            "status": "modified",
            "additions": 43,
            "deletions": 22,
            "changes": 65,
            "blob_url": "https://github.com/scverse/anndata/blob/f88f7bd4250b963752d615e491b7e676ce5eb7f0/anndata%2Ftests%2Ftest_readwrite.py",
            "raw_url": "https://github.com/scverse/anndata/raw/f88f7bd4250b963752d615e491b7e676ce5eb7f0/anndata%2Ftests%2Ftest_readwrite.py",
            "contents_url": "https://api.github.com/repos/scverse/anndata/contents/anndata%2Ftests%2Ftest_readwrite.py?ref=f88f7bd4250b963752d615e491b7e676ce5eb7f0",
            "patch": "@@ -13,6 +13,7 @@\n import pandas as pd\n import pytest\n import zarr\n+from numba.core.errors import NumbaDeprecationWarning\n from scipy.sparse import csc_matrix, csr_matrix\n \n import anndata as ad\n@@ -88,7 +89,7 @@ def rw(backing_h5ad):\n     M, N = 100, 101\n     orig = gen_adata((M, N))\n     orig.write(backing_h5ad)\n-    curr = ad.read(backing_h5ad)\n+    curr = ad.read_h5ad(backing_h5ad)\n     return curr, orig\n \n \n@@ -139,7 +140,7 @@ def test_readwrite_kitchensink(tmp_path, storage, typ, backing_h5ad, dataset_kwa\n \n     if storage == \"h5ad\":\n         adata_src.write(backing_h5ad, **dataset_kwargs)\n-        adata_mid = ad.read(backing_h5ad)\n+        adata_mid = ad.read_h5ad(backing_h5ad)\n         adata_mid.write(tmp_path / \"mid.h5ad\", **dataset_kwargs)\n         adata = ad.read_h5ad(tmp_path / \"mid.h5ad\")\n     else:\n@@ -179,7 +180,7 @@ def test_readwrite_maintain_X_dtype(typ, backing_h5ad):\n     adata_src = ad.AnnData(X)\n     adata_src.write(backing_h5ad)\n \n-    adata = ad.read(backing_h5ad)\n+    adata = ad.read_h5ad(backing_h5ad)\n     assert adata.X.dtype == adata_src.X.dtype\n \n \n@@ -212,7 +213,7 @@ def test_readwrite_h5ad_one_dimension(typ, backing_h5ad):\n     adata_src = ad.AnnData(X, obs=obs_dict, var=var_dict, uns=uns_dict)\n     adata_one = adata_src[:, 0].copy()\n     adata_one.write(backing_h5ad)\n-    adata = ad.read(backing_h5ad)\n+    adata = ad.read_h5ad(backing_h5ad)\n     assert adata.shape == (3, 1)\n     assert_equal(adata, adata_one)\n \n@@ -224,7 +225,7 @@ def test_readwrite_backed(typ, backing_h5ad):\n     adata_src.filename = backing_h5ad  # change to backed mode\n     adata_src.write()\n \n-    adata = ad.read(backing_h5ad)\n+    adata = ad.read_h5ad(backing_h5ad)\n     assert isinstance(adata.obs[\"oanno1\"].dtype, pd.CategoricalDtype)\n     assert not isinstance(adata.obs[\"oanno2\"].dtype, pd.CategoricalDtype)\n     assert adata.obs.index.tolist() == [\"name1\", \"name2\", \"name3\"]\n@@ -276,7 +277,7 @@ def test_read_full_io_error(tmp_path, name, read, write):\n         store[\"obs\"].attrs[\"encoding-type\"] = \"invalid\"\n     with pytest_8_raises(\n         IORegistryError,\n-        match=r\"raised while reading key '/obs'\",\n+        match=r\"raised while reading key 'obs'.*from /$\",\n     ) as exc_info:\n         read(path)\n     assert re.search(\n@@ -324,7 +325,8 @@ def check_compressed(key, value):\n         msg = \"\\n\\t\".join(not_compressed)\n         raise AssertionError(f\"These elements were not compressed correctly:\\n\\t{msg}\")\n \n-    assert_equal(adata, ad.read_h5ad(pth))\n+    expected = ad.read_h5ad(pth)\n+    assert_equal(adata, expected)\n \n \n def test_zarr_compression(tmp_path):\n@@ -349,7 +351,8 @@ def check_compressed(key, value):\n         msg = \"\\n\\t\".join(not_compressed)\n         raise AssertionError(f\"These elements were not compressed correctly:\\n\\t{msg}\")\n \n-    assert_equal(adata, ad.read_zarr(pth))\n+    expected = ad.read_zarr(pth)\n+    assert_equal(adata, expected)\n \n \n def test_changed_obs_var_names(tmp_path, diskfmt):\n@@ -388,7 +391,14 @@ def test_readwrite_loom(typ, obsm_mapping, varm_mapping, tmp_path):\n     adata_src.obsm[\"X_a\"] = np.zeros((adata_src.n_obs, 2))\n     adata_src.varm[\"X_b\"] = np.zeros((adata_src.n_vars, 3))\n \n-    adata_src.write_loom(tmp_path / \"test.loom\", write_obsm_varm=True)\n+    with warnings.catch_warnings():\n+        warnings.filterwarnings(\"ignore\", category=NumbaDeprecationWarning)\n+        warnings.filterwarnings(\n+            \"ignore\",\n+            message=r\"datetime.datetime.utcnow\\(\\) is deprecated\",\n+            category=DeprecationWarning,\n+        )\n+        adata_src.write_loom(tmp_path / \"test.loom\", write_obsm_varm=True)\n \n     adata = ad.read_loom(\n         tmp_path / \"test.loom\",\n@@ -422,15 +432,23 @@ def test_readwrite_loom(typ, obsm_mapping, varm_mapping, tmp_path):\n def test_readloom_deprecations(tmp_path):\n     loom_pth = tmp_path / \"test.loom\"\n     adata_src = gen_adata((5, 10), obsm_types=[np.ndarray], varm_types=[np.ndarray])\n-    adata_src.write_loom(loom_pth, write_obsm_varm=True)\n+\n+    with warnings.catch_warnings():\n+        warnings.filterwarnings(\"ignore\", category=NumbaDeprecationWarning)\n+        warnings.filterwarnings(\n+            \"ignore\",\n+            message=r\"datetime.datetime.utcnow\\(\\) is deprecated\",\n+            category=DeprecationWarning,\n+        )\n+        adata_src.write_loom(loom_pth, write_obsm_varm=True)\n \n     # obsm_names -> obsm_mapping\n     obsm_mapping = {\"df\": adata_src.obs.columns}\n     with pytest.warns(FutureWarning):\n         depr_result = ad.read_loom(loom_pth, obsm_names=obsm_mapping)\n     actual_result = ad.read_loom(loom_pth, obsm_mapping=obsm_mapping)\n     assert_equal(actual_result, depr_result)\n-    with pytest.raises(ValueError, match=\"ambiguous\"):\n+    with pytest.raises(ValueError, match=\"ambiguous\"), pytest.warns(FutureWarning):\n         ad.read_loom(loom_pth, obsm_mapping=obsm_mapping, obsm_names=obsm_mapping)\n \n     # varm_names -> varm_mapping\n@@ -439,7 +457,7 @@ def test_readloom_deprecations(tmp_path):\n         depr_result = ad.read_loom(loom_pth, varm_names=varm_mapping)\n     actual_result = ad.read_loom(loom_pth, varm_mapping=varm_mapping)\n     assert_equal(actual_result, depr_result)\n-    with pytest.raises(ValueError, match=\"ambiguous\"):\n+    with pytest.raises(ValueError, match=\"ambiguous\"), pytest.warns(FutureWarning):\n         ad.read_loom(loom_pth, varm_mapping=varm_mapping, varm_names=varm_mapping)\n \n     # positional -> keyword\n@@ -521,23 +539,23 @@ def hash_dir_contents(dir: Path) -> dict[str, bytes]:\n             marks=pytest.mark.xfail(reason=\"Loom can’t handle 0×0 matrices\"),\n         ),\n         pytest.param(ad.read_zarr, ad._io.write_zarr, \"test_empty.zarr\"),\n-        pytest.param(\n-            ad.read_zarr,\n-            ad._io.write_zarr,\n-            \"test_empty.zip\",\n-            marks=pytest.mark.xfail(reason=\"Zarr zip storage doesn’t seem to work…\"),\n-        ),\n     ],\n )\n-def test_readwrite_hdf5_empty(read, write, name, tmp_path):\n+def test_readwrite_empty(read, write, name, tmp_path):\n     adata = ad.AnnData(uns=dict(empty=np.array([], dtype=float)))\n     write(tmp_path / name, adata)\n     ad_read = read(tmp_path / name)\n     assert ad_read.uns[\"empty\"].shape == (0,)\n \n \n def test_read_excel():\n-    adata = ad.read_excel(HERE / \"data/excel.xlsx\", \"Sheet1\", dtype=int)\n+    with warnings.catch_warnings():\n+        warnings.filterwarnings(\n+            \"ignore\",\n+            message=r\"datetime.datetime.utcnow\\(\\) is deprecated\",\n+            category=DeprecationWarning,\n+        )\n+        adata = ad.read_excel(HERE / \"data/excel.xlsx\", \"Sheet1\", dtype=int)\n     assert adata.X.tolist() == X_list\n \n \n@@ -728,10 +746,13 @@ def test_scanpy_krumsiek11(tmp_path, diskfmt):\n     filepth = tmp_path / f\"test.{diskfmt}\"\n     import scanpy as sc\n \n-    orig = sc.datasets.krumsiek11()\n+    # TODO: this should be fixed in scanpy instead\n+    with pytest.warns(UserWarning, match=r\"Observation names are not unique\"):\n+        orig = sc.datasets.krumsiek11()\n     del orig.uns[\"highlights\"]  # Can’t write int keys\n     getattr(orig, f\"write_{diskfmt}\")(filepth)\n-    read = getattr(ad, f\"read_{diskfmt}\")(filepth)\n+    with pytest.warns(UserWarning, match=r\"Observation names are not unique\"):\n+        read = getattr(ad, f\"read_{diskfmt}\")(filepth)\n \n     assert_equal(orig, read, exact=True)\n "
        },
        {
            "sha": "478e6122c58efc35a240467da2ea2f7f12203d5c",
            "filename": "anndata/tests/test_settings.py",
            "status": "added",
            "additions": 227,
            "deletions": 0,
            "changes": 227,
            "blob_url": "https://github.com/scverse/anndata/blob/f88f7bd4250b963752d615e491b7e676ce5eb7f0/anndata%2Ftests%2Ftest_settings.py",
            "raw_url": "https://github.com/scverse/anndata/raw/f88f7bd4250b963752d615e491b7e676ce5eb7f0/anndata%2Ftests%2Ftest_settings.py",
            "contents_url": "https://api.github.com/repos/scverse/anndata/contents/anndata%2Ftests%2Ftest_settings.py?ref=f88f7bd4250b963752d615e491b7e676ce5eb7f0",
            "patch": "@@ -0,0 +1,227 @@\n+from __future__ import annotations\n+\n+import os\n+from enum import Enum\n+\n+import pytest\n+\n+from anndata._settings import (\n+    SettingsManager,\n+    check_and_get_bool,\n+    check_and_get_environ_var,\n+    validate_bool,\n+)\n+\n+option = \"test_var\"\n+default_val = False\n+description = \"My doc string!\"\n+\n+option_2 = \"test_var_2\"\n+default_val_2 = False\n+description_2 = \"My doc string 2!\"\n+\n+option_3 = \"test_var_3\"\n+default_val_3 = [1, 2]\n+description_3 = \"My doc string 3!\"\n+type_3 = list[int]\n+\n+\n+def validate_int_list(val) -> bool:\n+    if not isinstance(val, list) or not [isinstance(type(e), int) for e in val]:\n+        raise TypeError(f\"{repr(val)} is not a valid int list\")\n+    return True\n+\n+\n+settings = SettingsManager()\n+settings.register(option, default_val, description, validate_bool)\n+\n+settings.register(option_2, default_val_2, description_2, validate_bool)\n+\n+settings.register(\n+    option_3,\n+    default_val_3,\n+    description_3,\n+    validate_int_list,\n+    type_3,\n+)\n+\n+\n+def test_register_option_default():\n+    assert getattr(settings, option) == default_val\n+    assert description in settings.describe(option)\n+\n+\n+def test_register_with_env(monkeypatch):\n+    with monkeypatch.context() as mp:\n+        option_env = \"test_var_env\"\n+        default_val_env = False\n+        description_env = \"My doc string env!\"\n+        option_env_var = \"ANNDATA_\" + option_env.upper()\n+        mp.setenv(option_env_var, \"1\")\n+\n+        settings.register(\n+            option_env,\n+            default_val_env,\n+            description_env,\n+            validate_bool,\n+            get_from_env=check_and_get_bool,\n+        )\n+\n+        assert settings.test_var_env\n+\n+\n+def test_register_with_env_enum(monkeypatch):\n+    with monkeypatch.context() as mp:\n+        option_env = \"test_var_env\"\n+        default_val_env = False\n+        description_env = \"My doc string env!\"\n+        option_env_var = \"ANNDATA_\" + option_env.upper()\n+        mp.setenv(option_env_var, \"b\")\n+\n+        class TestEnum(Enum):\n+            a = False\n+            b = True\n+\n+        def check_and_get_bool_enum(option, default_value):\n+            return check_and_get_environ_var(\n+                \"ANNDATA_\" + option.upper(), \"a\", cast=TestEnum\n+            ).value\n+\n+        settings.register(\n+            option_env,\n+            default_val_env,\n+            description_env,\n+            validate_bool,\n+            get_from_env=check_and_get_bool_enum,\n+        )\n+\n+        assert settings.test_var_env\n+\n+\n+def test_register_bad_option():\n+    with pytest.raises(TypeError, match=\"'foo' is not a valid int list\"):\n+        settings.register(\n+            \"test_var_4\",\n+            \"foo\",  # should be a list of ints\n+            description_3,\n+            validate_int_list,\n+            type_3,\n+        )\n+\n+\n+def test_set_option():\n+    setattr(settings, option, not default_val)\n+    assert getattr(settings, option) == (not default_val)\n+    settings.reset(option)\n+    assert getattr(settings, option) == default_val\n+\n+\n+def test_dir():\n+    assert {option, option_2, option_3} <= set(dir(settings))\n+    assert dir(settings) == sorted(dir(settings))\n+\n+\n+def test_reset_multiple():\n+    setattr(settings, option, not default_val)\n+    setattr(settings, option_2, not default_val_2)\n+    settings.reset([option, option_2])\n+    assert getattr(settings, option) == default_val\n+    assert getattr(settings, option_2) == default_val_2\n+\n+\n+def test_get_unregistered_option():\n+    with pytest.raises(AttributeError):\n+        setattr(settings, option + \"_different\", default_val)\n+\n+\n+def test_override():\n+    with settings.override(**{option: not default_val}):\n+        assert getattr(settings, option) == (not default_val)\n+    assert getattr(settings, option) == default_val\n+\n+\n+def test_override_multiple():\n+    with settings.override(**{option: not default_val, option_2: not default_val_2}):\n+        assert getattr(settings, option) == (not default_val)\n+        assert getattr(settings, option_2) == (not default_val_2)\n+    assert getattr(settings, option) == default_val\n+    assert getattr(settings, option_2) == default_val_2\n+\n+\n+def test_deprecation():\n+    warning = \"This is a deprecation warning!\"\n+    version = \"0.1.0\"\n+    settings.deprecate(option, version, warning)\n+    described_option = settings.describe(option, print_description=False)\n+    # first line is message, second two from deprecation\n+    default_deprecation_message = f\"{option} will be removed in {version}.*\"\n+    assert described_option.endswith(default_deprecation_message)\n+    described_option = (\n+        described_option.rstrip().removesuffix(default_deprecation_message).rstrip()\n+    )\n+    assert described_option.endswith(warning)\n+    with pytest.warns(\n+        DeprecationWarning,\n+        match=\"'test_var' will be removed in 0.1.0. This is a deprecation warning!\",\n+    ):\n+        assert getattr(settings, option) == default_val\n+\n+\n+def test_deprecation_no_message():\n+    version = \"0.1.0\"\n+    settings.deprecate(option, version)\n+    described_option = settings.describe(option, print_description=False)\n+    # first line is message, second from deprecation version\n+    assert described_option.endswith(f\"{option} will be removed in {version}.*\")\n+\n+\n+def test_option_typing():\n+    assert settings._registered_options[option_3].type == type_3\n+    assert str(type_3) in settings.describe(option_3, print_description=False)\n+\n+\n+def test_check_and_get_environ_var(monkeypatch):\n+    with monkeypatch.context() as mp:\n+        option_env_var = \"ANNDATA_OPTION\"\n+        assert hash(\"foo\") == check_and_get_environ_var(\n+            option_env_var, \"foo\", [\"foo\", \"bar\"], lambda x: hash(x)\n+        )\n+        mp.setenv(option_env_var, \"bar\")\n+        assert hash(\"bar\") == check_and_get_environ_var(\n+            option_env_var, \"foo\", [\"foo\", \"bar\"], lambda x: hash(x)\n+        )\n+        mp.setenv(option_env_var, \"Not foo or bar\")\n+        with pytest.warns(\n+            match=f'Value \"{os.environ[option_env_var]}\" is not in allowed'\n+        ):\n+            check_and_get_environ_var(\n+                option_env_var, \"foo\", [\"foo\", \"bar\"], lambda x: hash(x)\n+            )\n+        assert hash(\"Not foo or bar\") == check_and_get_environ_var(\n+            option_env_var, \"foo\", cast=lambda x: hash(x)\n+        )\n+\n+\n+def test_check_and_get_bool(monkeypatch):\n+    with monkeypatch.context() as mp:\n+        option_env_var = \"ANNDATA_\" + option.upper()\n+        assert not check_and_get_bool(option, default_val)\n+        mp.setenv(option_env_var, \"1\")\n+        assert check_and_get_bool(option, default_val)\n+        mp.setenv(option_env_var, \"Not 0 or 1\")\n+        with pytest.warns(\n+            match=f'Value \"{os.environ[option_env_var]}\" is not in allowed'\n+        ):\n+            check_and_get_bool(option, default_val)\n+\n+\n+def test_check_and_get_bool_enum(monkeypatch):\n+    with monkeypatch.context() as mp:\n+        option_env_var = \"ANNDATA_\" + option.upper()\n+        mp.setenv(option_env_var, \"b\")\n+\n+        class TestEnum(Enum):\n+            a = False\n+            b = True\n+\n+        assert check_and_get_environ_var(option_env_var, \"a\", cast=TestEnum).value"
        },
        {
            "sha": "7ac4cfefcf79feaf85d5af14828b6c476a2eedab",
            "filename": "anndata/tests/test_views.py",
            "status": "modified",
            "additions": 77,
            "deletions": 9,
            "changes": 86,
            "blob_url": "https://github.com/scverse/anndata/blob/f88f7bd4250b963752d615e491b7e676ce5eb7f0/anndata%2Ftests%2Ftest_views.py",
            "raw_url": "https://github.com/scverse/anndata/raw/f88f7bd4250b963752d615e491b7e676ce5eb7f0/anndata%2Ftests%2Ftest_views.py",
            "contents_url": "https://api.github.com/repos/scverse/anndata/contents/anndata%2Ftests%2Ftest_views.py?ref=f88f7bd4250b963752d615e491b7e676ce5eb7f0",
            "patch": "@@ -27,6 +27,10 @@\n )\n from anndata.utils import asarray\n \n+IGNORE_SPARSE_EFFICIENCY_WARNING = pytest.mark.filterwarnings(\n+    \"ignore:Changing the sparsity structure:scipy.sparse.SparseEfficiencyWarning\"\n+)\n+\n # ------------------------------------------------------------------------------\n # Some test data\n # ------------------------------------------------------------------------------\n@@ -103,7 +107,8 @@ def test_views():\n \n     assert adata_subset.is_view\n     # now transition to actual object\n-    adata_subset.obs[\"foo\"] = range(2)\n+    with pytest.warns(ad.ImplicitModificationWarning, match=r\".*\\.obs.*\"):\n+        adata_subset.obs[\"foo\"] = range(2)\n     assert not adata_subset.is_view\n \n     assert adata_subset.obs[\"foo\"].tolist() == list(range(2))\n@@ -134,7 +139,8 @@ def test_modify_view_component(matrix_type, mapping_name):\n     subset = adata[:5, :][:, :5]\n     assert subset.is_view\n     m = getattr(subset, mapping_name)[\"m\"]\n-    m[0, 0] = 100\n+    with pytest.warns(ad.ImplicitModificationWarning, match=rf\".*\\.{mapping_name}.*\"):\n+        m[0, 0] = 100\n     assert not subset.is_view\n     assert getattr(subset, mapping_name)[\"m\"][0, 0] == 100\n \n@@ -267,6 +273,7 @@ def test_set_varm(adata):\n \n # TODO: Determine if this is the intended behavior,\n #       or just the behaviour we’ve had for a while\n+@IGNORE_SPARSE_EFFICIENCY_WARNING\n def test_not_set_subset_X(matrix_type_base, subset_func):\n     adata = ad.AnnData(matrix_type_base(asarray(sparse.random(20, 20))))\n     init_hash = joblib.hash(adata)\n@@ -283,7 +290,8 @@ def test_not_set_subset_X(matrix_type_base, subset_func):\n         subset_func(np.arange(subset.X.shape[1])), subset.var_names\n     )\n     assert subset.is_view\n-    subset.X[:, internal_idx] = 1\n+    with pytest.warns(ad.ImplicitModificationWarning, match=r\".*X.*\"):\n+        subset.X[:, internal_idx] = 1\n     assert not subset.is_view\n     assert not np.any(asarray(adata.X != orig_X_val))\n \n@@ -307,6 +315,7 @@ def tokenize_anndata(adata: ad.AnnData):\n \n # TODO: Determine if this is the intended behavior,\n #       or just the behaviour we’ve had for a while\n+@IGNORE_SPARSE_EFFICIENCY_WARNING\n def test_not_set_subset_X_dask(matrix_type_no_gpu, subset_func):\n     adata = ad.AnnData(matrix_type_no_gpu(asarray(sparse.random(20, 20))))\n     init_hash = tokenize(adata)\n@@ -323,17 +332,19 @@ def test_not_set_subset_X_dask(matrix_type_no_gpu, subset_func):\n         subset_func(np.arange(subset.X.shape[1])), subset.var_names\n     )\n     assert subset.is_view\n-    subset.X[:, internal_idx] = 1\n+    with pytest.warns(ad.ImplicitModificationWarning, match=r\".*X.*\"):\n+        subset.X[:, internal_idx] = 1\n     assert not subset.is_view\n     assert not np.any(asarray(adata.X != orig_X_val))\n \n     assert init_hash == tokenize(adata)\n \n \n+@IGNORE_SPARSE_EFFICIENCY_WARNING\n def test_set_scalar_subset_X(matrix_type, subset_func):\n     adata = ad.AnnData(matrix_type(np.zeros((10, 10))))\n     orig_X_val = adata.X.copy()\n-    subset_idx = slice_subset(adata.obs_names)\n+    subset_idx = subset_func(adata.obs_names)\n \n     adata_subset = adata[subset_idx, :]\n \n@@ -367,7 +378,8 @@ def test_set_subset_obsm(adata, subset_func):\n     )\n \n     assert subset.is_view\n-    subset.obsm[\"o\"][internal_idx] = 1\n+    with pytest.warns(ad.ImplicitModificationWarning, match=r\".*obsm.*\"):\n+        subset.obsm[\"o\"][internal_idx] = 1\n     assert not subset.is_view\n     assert np.all(adata.obsm[\"o\"] == orig_obsm_val)\n \n@@ -389,7 +401,8 @@ def test_set_subset_varm(adata, subset_func):\n     )\n \n     assert subset.is_view\n-    subset.varm[\"o\"][internal_idx] = 1\n+    with pytest.warns(ad.ImplicitModificationWarning, match=r\".*varm.*\"):\n+        subset.varm[\"o\"][internal_idx] = 1\n     assert not subset.is_view\n     assert np.all(adata.varm[\"o\"] == orig_varm_val)\n \n@@ -481,7 +494,8 @@ def test_layers_view():\n     assert real_hash == joblib.hash(real_adata)\n     assert view_hash == joblib.hash(view_adata)\n \n-    view_adata.layers[\"L2\"] = L[1:, 1:] + 2\n+    with pytest.warns(ad.ImplicitModificationWarning, match=r\".*layers.*\"):\n+        view_adata.layers[\"L2\"] = L[1:, 1:] + 2\n \n     assert not view_adata.is_view\n     assert real_hash == joblib.hash(real_adata)\n@@ -537,6 +551,30 @@ def test_double_index(subset_func, subset_func2):\n     assert np.all(v1.var == v2.var)\n \n \n+def test_view_different_type_indices(matrix_type):\n+    orig = gen_adata((30, 30), X_type=matrix_type)\n+    boolean_array_mask = np.random.randint(0, 2, 30).astype(\"bool\")\n+    boolean_list_mask = boolean_array_mask.tolist()\n+    integer_array_mask = np.where(boolean_array_mask)[0]\n+    integer_list_mask = integer_array_mask.tolist()\n+\n+    assert_equal(orig[integer_array_mask, :], orig[boolean_array_mask, :])\n+    assert_equal(orig[integer_list_mask, :], orig[boolean_list_mask, :])\n+    assert_equal(orig[integer_list_mask, :], orig[integer_array_mask, :])\n+    assert_equal(orig[:, integer_array_mask], orig[:, boolean_array_mask])\n+    assert_equal(orig[:, integer_list_mask], orig[:, boolean_list_mask])\n+    assert_equal(orig[:, integer_list_mask], orig[:, integer_array_mask])\n+    # check that X element is same independent of access\n+    assert_equal(orig[:, integer_list_mask].X, orig.X[:, integer_list_mask])\n+    assert_equal(orig[:, boolean_list_mask].X, orig.X[:, boolean_list_mask])\n+    assert_equal(orig[:, integer_array_mask].X, orig.X[:, integer_array_mask])\n+    assert_equal(orig[:, integer_list_mask].X, orig.X[:, integer_list_mask])\n+    assert_equal(orig[integer_list_mask, :].X, orig.X[integer_list_mask, :])\n+    assert_equal(orig[boolean_list_mask, :].X, orig.X[boolean_list_mask, :])\n+    assert_equal(orig[integer_array_mask, :].X, orig.X[integer_array_mask, :])\n+    assert_equal(orig[integer_list_mask, :].X, orig.X[integer_list_mask, :])\n+\n+\n def test_view_retains_ndarray_subclass():\n     adata = ad.AnnData(np.zeros((10, 10)))\n     adata.obsm[\"foo\"] = np.zeros((10, 5)).view(NDArraySubclass)\n@@ -631,7 +669,8 @@ def test_deepcopy_subset(adata, spmat: type):\n def test_view_mixin_copies_data(adata, array_type: type, attr):\n     N = 100\n     adata = ad.AnnData(\n-        obs=pd.DataFrame(index=np.arange(N)), var=pd.DataFrame(index=np.arange(N))\n+        obs=pd.DataFrame(index=np.arange(N).astype(str)),\n+        var=pd.DataFrame(index=np.arange(N).astype(str)),\n     )\n \n     X = array_type(sparse.eye(N, N).multiply(np.arange(1, N + 1)))\n@@ -678,3 +717,32 @@ def test_x_none():\n     new = view.copy()\n     assert new.shape == (2, 0)\n     assert new.obs_names.tolist() == [\"2\", \"3\"]\n+\n+\n+def test_empty_list_subset():\n+    orig = gen_adata((10, 10))\n+    subset = orig[:, []]\n+    assert subset.X.shape == (10, 0)\n+    assert subset.obsm[\"sparse\"].shape == (10, 100)\n+    assert subset.varm[\"sparse\"].shape == (0, 100)\n+\n+\n+# @pytest.mark.parametrize(\"dim\", [\"obs\", \"var\"])\n+# @pytest.mark.parametrize(\n+#     (\"idx\", \"pat\"),\n+#     [\n+#         pytest.param(\n+#             [1, \"cell_c\"], r\"Mixed type list indexers not supported\", id=\"mixed\"\n+#         ),\n+#         pytest.param(\n+#             [[1, 2], [2]], r\"setting an array element with a sequence\", id=\"nested\"\n+#         ),\n+#     ],\n+# )\n+# def test_subset_errors(dim, idx, pat):\n+#     orig = gen_adata((10, 10))\n+#     with pytest.raises(ValueError, match=pat):\n+#         if dim == \"obs\":\n+#             orig[idx, :].X\n+#         elif dim == \"var\":\n+#             orig[:, idx].X"
        },
        {
            "sha": "9c700e28b17e2158dd9bdb3e32596d32b301ecc0",
            "filename": "anndata/utils.py",
            "status": "modified",
            "additions": 46,
            "deletions": 14,
            "changes": 60,
            "blob_url": "https://github.com/scverse/anndata/blob/f88f7bd4250b963752d615e491b7e676ce5eb7f0/anndata%2Futils.py",
            "raw_url": "https://github.com/scverse/anndata/raw/f88f7bd4250b963752d615e491b7e676ce5eb7f0/anndata%2Futils.py",
            "contents_url": "https://api.github.com/repos/scverse/anndata/contents/anndata%2Futils.py?ref=f88f7bd4250b963752d615e491b7e676ce5eb7f0",
            "patch": "@@ -1,5 +1,6 @@\n from __future__ import annotations\n \n+import re\n import warnings\n from functools import singledispatch, wraps\n from typing import TYPE_CHECKING, Any\n@@ -19,6 +20,24 @@\n logger = get_logger(__name__)\n \n \n+def import_name(name: str) -> Any:\n+    from importlib import import_module\n+\n+    parts = name.split(\".\")\n+    obj = import_module(parts[0])\n+    for i, name in enumerate(parts[1:]):\n+        try:\n+            obj = import_module(f\"{obj.__name__}.{name}\")\n+        except ModuleNotFoundError:\n+            break\n+    for name in parts[i + 1 :]:\n+        try:\n+            obj = getattr(obj, name)\n+        except AttributeError:\n+            raise RuntimeError(f\"{parts[:i]}, {parts[i+1:]}, {obj} {name}\")\n+    return obj\n+\n+\n @singledispatch\n def asarray(x):\n     \"\"\"Convert x to a numpy array\"\"\"\n@@ -311,28 +330,40 @@ def convert_dictionary_to_structured_array(source: Mapping[str, Sequence[Any]]):\n     return arr\n \n \n-def deprecated(new_name: str):\n+def warn_once(msg: str, category: type[Warning], stacklevel: int = 1):\n+    warnings.warn(msg, category, stacklevel=stacklevel)\n+    # Prevent from showing up every time an awkward array is used\n+    # You'd think `'once'` works, but it doesn't at the repl and in notebooks\n+    warnings.filterwarnings(\"ignore\", category=category, message=re.escape(msg))\n+\n+\n+def deprecated(\n+    new_name: str,\n+    category: type[Warning] = DeprecationWarning,\n+    add_msg: str = \"\",\n+    hide: bool = True,\n+):\n     \"\"\"\\\n     This is a decorator which can be used to mark functions\n     as deprecated. It will result in a warning being emitted\n     when the function is used.\n     \"\"\"\n \n     def decorator(func):\n+        name = func.__qualname__\n+        msg = (\n+            f\"Use {new_name} instead of {name}, \"\n+            f\"{name} is deprecated and will be removed in the future.\"\n+        )\n+        if add_msg:\n+            msg += f\" {add_msg}\"\n+\n         @wraps(func)\n         def new_func(*args, **kwargs):\n-            # turn off filter\n-            warnings.simplefilter(\"always\", DeprecationWarning)\n-            warnings.warn(\n-                f\"Use {new_name} instead of {func.__name__}, \"\n-                f\"{func.__name__} will be removed in the future.\",\n-                category=DeprecationWarning,\n-                stacklevel=2,\n-            )\n-            warnings.simplefilter(\"default\", DeprecationWarning)  # reset filter\n+            warnings.warn(msg, category=category, stacklevel=2)\n             return func(*args, **kwargs)\n \n-        setattr(new_func, \"__deprecated\", True)\n+        setattr(new_func, \"__deprecated\", (category, msg, hide))\n         return new_func\n \n     return decorator\n@@ -345,13 +376,14 @@ class DeprecationMixinMeta(type):\n     \"\"\"\n \n     def __dir__(cls):\n-        def is_deprecated(attr):\n+        def is_hidden(attr) -> bool:\n             if isinstance(attr, property):\n                 attr = attr.fget\n-            return getattr(attr, \"__deprecated\", False)\n+            _, _, hide = getattr(attr, \"__deprecated\", (None, None, False))\n+            return hide\n \n         return [\n             item\n             for item in type.__dir__(cls)\n-            if not is_deprecated(getattr(cls, item, None))\n+            if not is_hidden(getattr(cls, item, None))\n         ]"
        },
        {
            "sha": "05daf0e8103bf3a0461988473cecc8aec30766c0",
            "filename": "benchmarks/benchmarks/sparse_dataset.py",
            "status": "modified",
            "additions": 22,
            "deletions": 1,
            "changes": 23,
            "blob_url": "https://github.com/scverse/anndata/blob/f88f7bd4250b963752d615e491b7e676ce5eb7f0/benchmarks%2Fbenchmarks%2Fsparse_dataset.py",
            "raw_url": "https://github.com/scverse/anndata/raw/f88f7bd4250b963752d615e491b7e676ce5eb7f0/benchmarks%2Fbenchmarks%2Fsparse_dataset.py",
            "contents_url": "https://api.github.com/repos/scverse/anndata/contents/benchmarks%2Fbenchmarks%2Fsparse_dataset.py?ref=f88f7bd4250b963752d615e491b7e676ce5eb7f0",
            "patch": "@@ -4,16 +4,30 @@\n import zarr\n from scipy import sparse\n \n+from anndata import AnnData\n from anndata.experimental import sparse_dataset, write_elem\n \n \n+def make_alternating_mask(n):\n+    mask_alternating = np.ones(10_000, dtype=bool)\n+    for i in range(0, 10_000, n):\n+        mask_alternating[i] = False\n+    return mask_alternating\n+\n+\n class SparseCSRContiguousSlice:\n     params = (\n         [\n             (10_000, 10_000),\n             # (10_000, 500)\n         ],\n-        [slice(0, 1000), slice(0, 9000), slice(None, 9000, -1), slice(None, None, 2)],\n+        [\n+            slice(0, 1000),\n+            slice(0, 9000),\n+            slice(None, 9000, -1),\n+            slice(None, None, 2),\n+            make_alternating_mask(10),\n+        ],\n     )\n     param_names = [\"shape\", \"slice\"]\n \n@@ -25,9 +39,16 @@ def setup(self, shape, slice):\n         g = zarr.group()\n         write_elem(g, \"X\", X)\n         self.x = sparse_dataset(g[\"X\"])\n+        self.adata = AnnData(self.x)\n \n     def time_getitem(self, shape, slice):\n         self.x[self.slice]\n \n     def peakmem_getitem(self, shape, slice):\n         self.x[self.slice]\n+\n+    def time_getitem_adata(self, shape, slice):\n+        self.adata[self.slice]\n+\n+    def peakmem_getitem_adata(self, shape, slice):\n+        self.adata[self.slice]"
        },
        {
            "sha": "9776ec3d530093ff59ad30e97d571fbc457e31b2",
            "filename": "ci/gpu_ci.yml",
            "status": "added",
            "additions": 12,
            "deletions": 0,
            "changes": 12,
            "blob_url": "https://github.com/scverse/anndata/blob/f88f7bd4250b963752d615e491b7e676ce5eb7f0/ci%2Fgpu_ci.yml",
            "raw_url": "https://github.com/scverse/anndata/raw/f88f7bd4250b963752d615e491b7e676ce5eb7f0/ci%2Fgpu_ci.yml",
            "contents_url": "https://api.github.com/repos/scverse/anndata/contents/ci%2Fgpu_ci.yml?ref=f88f7bd4250b963752d615e491b7e676ce5eb7f0",
            "patch": "@@ -0,0 +1,12 @@\n+name: cupy_env\n+channels:\n+  - nvidia\n+  - conda-forge\n+dependencies:\n+  - python=3.12\n+  - cuda-version=11.8\n+  - cupy\n+  - numba\n+  - pytest\n+  - pytest-cov\n+  - pytest-xdist"
        },
        {
            "sha": "b3f393ea5797e47e2d91d7cd12bb0d5a4b2d7766",
            "filename": "ci/scripts/min-deps.py",
            "status": "added",
            "additions": 99,
            "deletions": 0,
            "changes": 99,
            "blob_url": "https://github.com/scverse/anndata/blob/f88f7bd4250b963752d615e491b7e676ce5eb7f0/ci%2Fscripts%2Fmin-deps.py",
            "raw_url": "https://github.com/scverse/anndata/raw/f88f7bd4250b963752d615e491b7e676ce5eb7f0/ci%2Fscripts%2Fmin-deps.py",
            "contents_url": "https://api.github.com/repos/scverse/anndata/contents/ci%2Fscripts%2Fmin-deps.py?ref=f88f7bd4250b963752d615e491b7e676ce5eb7f0",
            "patch": "@@ -0,0 +1,99 @@\n+#!python3\n+from __future__ import annotations\n+\n+import argparse\n+import sys\n+from collections import deque\n+from pathlib import Path\n+from typing import TYPE_CHECKING\n+\n+if sys.version_info >= (3, 11):\n+    import tomllib\n+else:\n+    import tomli as tomllib\n+\n+from packaging.requirements import Requirement\n+from packaging.version import Version\n+\n+if TYPE_CHECKING:\n+    from collections.abc import Generator, Iterable\n+\n+\n+def min_dep(req: Requirement) -> Requirement:\n+    \"\"\"\n+    Given a requirement, return the minimum version specifier.\n+\n+    Example\n+    -------\n+\n+    >>> min_dep(Requirement(\"numpy>=1.0\"))\n+    \"numpy==1.0\"\n+    \"\"\"\n+    req_name = req.name\n+    if req.extras:\n+        req_name = f\"{req_name}[{','.join(req.extras)}]\"\n+\n+    if not req.specifier:\n+        return Requirement(req_name)\n+\n+    min_version = Version(\"0.0.0.a1\")\n+    for spec in req.specifier:\n+        if spec.operator in [\">\", \">=\", \"~=\"]:\n+            min_version = max(min_version, Version(spec.version))\n+        elif spec.operator == \"==\":\n+            min_version = Version(spec.version)\n+\n+    return Requirement(f\"{req_name}=={min_version}.*\")\n+\n+\n+def extract_min_deps(\n+    dependencies: Iterable[Requirement], *, pyproject\n+) -> Generator[Requirement, None, None]:\n+    dependencies = deque(dependencies)  # We'll be mutating this\n+    project_name = pyproject[\"project\"][\"name\"]\n+\n+    while len(dependencies) > 0:\n+        req = dependencies.pop()\n+\n+        # If we are referring to other optional dependency lists, resolve them\n+        if req.name == project_name:\n+            assert req.extras, f\"Project included itself as dependency, without specifying extras: {req}\"\n+            for extra in req.extras:\n+                extra_deps = pyproject[\"project\"][\"optional-dependencies\"][extra]\n+                dependencies += map(Requirement, extra_deps)\n+        else:\n+            yield min_dep(req)\n+\n+\n+def main():\n+    parser = argparse.ArgumentParser(\n+        prog=\"min-deps\",\n+        description=\"\"\"Parse a pyproject.toml file and output a list of minimum dependencies.\n+\n+        Output is directly passable to `pip install`.\"\"\",\n+        usage=\"pip install `python min-deps.py pyproject.toml`\",\n+    )\n+    parser.add_argument(\n+        \"path\", type=Path, help=\"pyproject.toml to parse minimum dependencies from\"\n+    )\n+    parser.add_argument(\n+        \"--extras\", type=str, nargs=\"*\", default=(), help=\"extras to install\"\n+    )\n+\n+    args = parser.parse_args()\n+\n+    pyproject = tomllib.loads(args.path.read_text())\n+\n+    project_name = pyproject[\"project\"][\"name\"]\n+    deps = [\n+        *map(Requirement, pyproject[\"project\"][\"dependencies\"]),\n+        *(Requirement(f\"{project_name}[{extra}]\") for extra in args.extras),\n+    ]\n+\n+    min_deps = extract_min_deps(deps, pyproject=pyproject)\n+\n+    print(\" \".join(map(str, min_deps)))\n+\n+\n+if __name__ == \"__main__\":\n+    main()"
        },
        {
            "sha": "578bc71d96ec876cfe60fa9ef78d96050416e988",
            "filename": "conftest.py",
            "status": "modified",
            "additions": 72,
            "deletions": 8,
            "changes": 80,
            "blob_url": "https://github.com/scverse/anndata/blob/f88f7bd4250b963752d615e491b7e676ce5eb7f0/conftest.py",
            "raw_url": "https://github.com/scverse/anndata/raw/f88f7bd4250b963752d615e491b7e676ce5eb7f0/conftest.py",
            "contents_url": "https://api.github.com/repos/scverse/anndata/contents/conftest.py?ref=f88f7bd4250b963752d615e491b7e676ce5eb7f0",
            "patch": "@@ -4,30 +4,50 @@\n # TODO: Fix that, e.g. with the `pytest -p anndata.testing._pytest` pattern.\n from __future__ import annotations\n \n-from typing import TYPE_CHECKING\n+import re\n+import warnings\n+from typing import TYPE_CHECKING, cast\n \n import pytest\n \n from anndata.compat import chdir\n+from anndata.utils import import_name\n \n if TYPE_CHECKING:\n+    from collections.abc import Generator, Iterable\n     from pathlib import Path\n \n-doctest_marker = pytest.mark.usefixtures(\"doctest_env\")\n \n+@pytest.fixture(autouse=True)\n+def _suppress_env_for_doctests(request: pytest.FixtureRequest) -> None:\n+    if isinstance(request.node, pytest.DoctestItem):\n+        request.getfixturevalue(\"_doctest_env\")\n \n-@pytest.fixture\n-def doctest_env(cache: pytest.Cache, tmp_path: Path) -> None:\n+\n+@pytest.fixture()\n+def _doctest_env(\n+    request: pytest.FixtureRequest, cache: pytest.Cache, tmp_path: Path\n+) -> Generator[None, None, None]:\n     from scanpy import settings\n \n+    assert isinstance(request.node.parent, pytest.Module)\n+    # request.node.parent is either a DoctestModule or a DoctestTextFile.\n+    # Only DoctestModule has a .obj attribute (the imported module).\n+    if request.node.parent.obj:\n+        func = import_name(request.node.name)\n+        warning_detail: tuple[type[Warning], str, bool] | None\n+        if warning_detail := getattr(func, \"__deprecated\", None):\n+            cat, msg, _ = warning_detail\n+            warnings.filterwarnings(\"ignore\", category=cat, message=re.escape(msg))\n+\n     old_dd, settings.datasetdir = settings.datasetdir, cache.mkdir(\"scanpy-data\")\n     with chdir(tmp_path):\n         yield\n     settings.datasetdir = old_dd\n \n \n-def pytest_itemcollected(item):\n-    \"\"\"Define behavior of pytest.mark.gpu and doctests.\"\"\"\n+def pytest_itemcollected(item: pytest.Item) -> None:\n+    \"\"\"Define behavior of pytest.mark.gpu.\"\"\"\n     from importlib.util import find_spec\n \n     is_gpu = len([mark for mark in item.iter_markers(name=\"gpu\")]) > 0\n@@ -36,5 +56,49 @@ def pytest_itemcollected(item):\n             pytest.mark.skipif(not find_spec(\"cupy\"), reason=\"Cupy not installed.\")\n         )\n \n-    if isinstance(item, pytest.DoctestItem):\n-        item.add_marker(doctest_marker)\n+\n+def pytest_addoption(parser: pytest.Parser) -> None:\n+    \"\"\"Hook to register custom CLI options and config values\"\"\"\n+    parser.addoption(\n+        \"--strict-warnings\",\n+        action=\"store_true\",\n+        default=False,\n+        help=\"Turn warnings into errors that are not overridden by `filterwarnings` or `filterwarnings_when_strict`.\",\n+    )\n+\n+    parser.addini(\n+        \"filterwarnings_when_strict\",\n+        \"Filters to apply after `-Werror` when --strict-warnings is active\",\n+        type=\"linelist\",\n+        default=[],\n+    )\n+\n+\n+def pytest_collection_modifyitems(\n+    session: pytest.Session, config: pytest.Config, items: Iterable[pytest.Item]\n+):\n+    if not config.getoption(\"--strict-warnings\"):\n+        return\n+\n+    warning_filters = [\n+        \"error\",\n+        *_config_get_strlist(config, \"filterwarnings\"),\n+        *_config_get_strlist(config, \"filterwarnings_when_strict\"),\n+    ]\n+    warning_marks = [pytest.mark.filterwarnings(f) for f in warning_filters]\n+\n+    # Add warning filters defined in the config to all tests items.\n+    # Test items might already have @pytest.mark.filterwarnings applied,\n+    # so we prepend ours to ensure that an item’s explicit filters override these.\n+    # Reversing then individually prepending ensures that the order is preserved.\n+    for item in items:\n+        for mark in reversed(warning_marks):\n+            item.add_marker(mark, append=False)\n+\n+\n+def _config_get_strlist(config: pytest.Config, name: str) -> list[str]:\n+    if strs := config.getini(name):\n+        assert isinstance(strs, list)\n+        assert all(isinstance(item, str) for item in strs)\n+        return cast(list[str], strs)\n+    return []"
        },
        {
            "sha": "fb8f40f9318026d52386f018a3a8d72641e0fe01",
            "filename": "docs/api.md",
            "status": "modified",
            "additions": 10,
            "deletions": 0,
            "changes": 10,
            "blob_url": "https://github.com/scverse/anndata/blob/f88f7bd4250b963752d615e491b7e676ce5eb7f0/docs%2Fapi.md",
            "raw_url": "https://github.com/scverse/anndata/raw/f88f7bd4250b963752d615e491b7e676ce5eb7f0/docs%2Fapi.md",
            "contents_url": "https://api.github.com/repos/scverse/anndata/contents/docs%2Fapi.md?ref=f88f7bd4250b963752d615e491b7e676ce5eb7f0",
            "patch": "@@ -142,3 +142,13 @@ Utilities for customizing the IO process:\n \n    ImplicitModificationWarning\n ```\n+\n+## Settings\n+\n+```{eval-rst}\n+.. autosummary::\n+   :toctree: generated/\n+\n+   settings\n+   settings.override\n+```"
        },
        {
            "sha": "886bfa0f607c16edf756c023ce8e48ba70d9cd6d",
            "filename": "docs/benchmark-read-write.ipynb",
            "status": "modified",
            "additions": 9,
            "deletions": 6,
            "changes": 15,
            "blob_url": "https://github.com/scverse/anndata/blob/f88f7bd4250b963752d615e491b7e676ce5eb7f0/docs%2Fbenchmark-read-write.ipynb",
            "raw_url": "https://github.com/scverse/anndata/raw/f88f7bd4250b963752d615e491b7e676ce5eb7f0/docs%2Fbenchmark-read-write.ipynb",
            "contents_url": "https://api.github.com/repos/scverse/anndata/contents/docs%2Fbenchmark-read-write.ipynb?ref=f88f7bd4250b963752d615e491b7e676ce5eb7f0",
            "patch": "@@ -20,8 +20,11 @@\n    \"metadata\": {},\n    \"outputs\": [],\n    \"source\": [\n-    \"import anndata as ad\\n\",\n-    \"import scanpy as sc\"\n+    \"from __future__ import annotations\\n\",\n+    \"\\n\",\n+    \"import scanpy as sc\\n\",\n+    \"\\n\",\n+    \"import anndata as ad\"\n    ]\n   },\n   {\n@@ -84,7 +87,7 @@\n    ],\n    \"source\": [\n     \"%%time\\n\",\n-    \"adata.write('test.h5ad')\"\n+    \"adata.write(\\\"test.h5ad\\\")\"\n    ]\n   },\n   {\n@@ -103,7 +106,7 @@\n    ],\n    \"source\": [\n     \"%%time\\n\",\n-    \"adata = ad.read('test.h5ad')\"\n+    \"adata = ad.read_h5ad(\\\"test.h5ad\\\")\"\n    ]\n   },\n   {\n@@ -129,7 +132,7 @@\n    ],\n    \"source\": [\n     \"%%time\\n\",\n-    \"adata.write_loom('test.loom')\"\n+    \"adata.write_loom(\\\"test.loom\\\")\"\n    ]\n   },\n   {\n@@ -156,7 +159,7 @@\n    ],\n    \"source\": [\n     \"%%time\\n\",\n-    \"adata = ad.read_loom('test.loom')\"\n+    \"adata = ad.read_loom(\\\"test.loom\\\")\"\n    ]\n   }\n  ],"
        },
        {
            "sha": "be644dceb31b39f647ca99d26c704b0bd1c9573b",
            "filename": "docs/concatenation.rst",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/scverse/anndata/blob/f88f7bd4250b963752d615e491b7e676ce5eb7f0/docs%2Fconcatenation.rst",
            "raw_url": "https://github.com/scverse/anndata/raw/f88f7bd4250b963752d615e491b7e676ce5eb7f0/docs%2Fconcatenation.rst",
            "contents_url": "https://api.github.com/repos/scverse/anndata/contents/docs%2Fconcatenation.rst?ref=f88f7bd4250b963752d615e491b7e676ce5eb7f0",
            "patch": "@@ -33,7 +33,7 @@ Let's start off with an example:\n \n If we split this object up by clusters of observations, then stack those subsets we'll obtain the same values – just ordered differently.\n \n-    >>> groups = pbmc.obs.groupby(\"louvain\").indices\n+    >>> groups = pbmc.obs.groupby(\"louvain\", observed=True).indices\n     >>> pbmc_concat = ad.concat([pbmc[inds] for inds in groups.values()], merge=\"same\")\n     >>> assert np.array_equal(pbmc.X, pbmc_concat[pbmc.obs_names].X)\n     >>> pbmc_concat\n@@ -69,7 +69,7 @@ For example, given two anndata objects with differing variables:\n            [0., 1., 0.],\n            [1., 0., 0.]])\n \n-The join argument is used for any element which has both (1) an axis being concatenated and (2) has an axis not being concatenated.\n+The join argument is used for any element which has both (1) an axis being concatenated and (2) an axis not being concatenated.\n When concatenating along the `obs` dimension, this means elements of `.X`, `obs`, `.layers`, and `.obsm` will be affected by the choice of `join`.\n \n To demonstrate this, let's say we're trying to combine a droplet based experiment with a spatial one.\n@@ -153,7 +153,7 @@ We provide a few strategies for merging elements aligned to the alternative axes\n * `None`: No elements aligned to alternative axes are present in the result object.\n * `\"same\"`: Elements that are the same in each of the objects.\n * `\"unique\"`: Elements for which there is only one possible value.\n-* `\"first\"`: The first element seen at each from each position.\n+* `\"first\"`: The first element seen in each from each position.\n * `\"only\"`: Elements that show up in only one of the objects.\n \n We'll show how this works with elements aligned to the alternative axis, and then how merging works with `.uns`.\n@@ -187,7 +187,7 @@ Now we will split this object by the categorical `\"blobs\"` and recombine it to i\n \n `adatas` is now a list of datasets with disjoint sets of observations and a common set of variables.\n Each object has had QC metrics computed, with observation-wise metrics stored under `\"qc\"` in `.obsm`, and variable-wise metrics stored with a unique key for each subset.\n-Taking a look at how this effects concatenation:\n+Taking a look at how this affects concatenation:\n \n     >>> ad.concat(adatas)\n     AnnData object with n_obs × n_vars = 640 × 30"
        },
        {
            "sha": "29491e94758f1afcc260b957cd28f01aa00eadb2",
            "filename": "docs/conf.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/scverse/anndata/blob/f88f7bd4250b963752d615e491b7e676ce5eb7f0/docs%2Fconf.py",
            "raw_url": "https://github.com/scverse/anndata/raw/f88f7bd4250b963752d615e491b7e676ce5eb7f0/docs%2Fconf.py",
            "contents_url": "https://api.github.com/repos/scverse/anndata/contents/docs%2Fconf.py?ref=f88f7bd4250b963752d615e491b7e676ce5eb7f0",
            "patch": "@@ -15,9 +15,6 @@\n \n # -- General configuration ------------------------------------------------\n \n-\n-needs_sphinx = \"1.7\"  # autosummary bugfix\n-\n # General information\n project = \"anndata\"\n author = f\"{project} developers\"\n@@ -52,6 +49,7 @@\n     \"sphinx_autodoc_typehints\",  # needs to be after napoleon\n     \"sphinx_issues\",\n     \"sphinx_design\",\n+    \"sphinx_search.extension\",\n     \"sphinxext.opengraph\",\n     \"scanpydoc\",  # needs to be before linkcode\n     \"sphinx.ext.linkcode\",\n@@ -126,11 +124,13 @@ def setup(app: Sphinx):\n # -- Options for HTML output ----------------------------------------------\n \n \n-html_theme = \"sphinx_book_theme\"\n+# The theme is sphinx-book-theme, with patches for readthedocs-sphinx-search\n+html_theme = \"scanpydoc\"\n html_theme_options = dict(\n     use_repository_button=True,\n     repository_url=\"https://github.com/scverse/anndata\",\n     repository_branch=\"main\",\n+    navigation_with_keys=False,  # https://github.com/pydata/pydata-sphinx-theme/issues/1492\n )\n html_logo = \"_static/img/anndata_schema.svg\"\n issues_github_path = \"scverse/anndata\""
        },
        {
            "sha": "3fdc68788cddf1e034a274b1f8337a455ef45494",
            "filename": "docs/fileformat-prose.md",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/scverse/anndata/blob/f88f7bd4250b963752d615e491b7e676ce5eb7f0/docs%2Ffileformat-prose.md",
            "raw_url": "https://github.com/scverse/anndata/raw/f88f7bd4250b963752d615e491b7e676ce5eb7f0/docs%2Ffileformat-prose.md",
            "contents_url": "https://api.github.com/repos/scverse/anndata/contents/docs%2Ffileformat-prose.md?ref=f88f7bd4250b963752d615e491b7e676ce5eb7f0",
            "patch": "@@ -57,10 +57,10 @@ var                      Group\n varm                     Group\n ``` -->\n \n-In general, `AnnData` objects are comprised of a various types of elements.\n+In general, `AnnData` objects are comprised of various types of elements.\n Each element is encoded as either an Array (or Dataset in hdf5 terminology) or a collection of elements (e.g. Group) in the store.\n-We record the type of an element using the `encoding-type` and `encoding-version` keys in it's attributes.\n-For example, we can this file represents an `AnnData` object from this metadata:\n+We record the type of an element using the `encoding-type` and `encoding-version` keys in its attributes.\n+For example, we can see that this file represents an `AnnData` object from its metadata:\n \n ```python\n >>> dict(store.attrs)\n@@ -319,7 +319,7 @@ pca/variance_ratio <zarr.core.Array '/uns/pca/variance_ratio' (50,) float64 read\n ### Mapping specifications (v0.1.0)\n \n * Each mapping MUST be its own group\n-* The groups metadata MUST contain the encoding metadata `\"encoding-type\": \"dict\"`, `\"encoding-version\": \"0.1.0\"`\n+* The group's metadata MUST contain the encoding metadata `\"encoding-type\": \"dict\"`, `\"encoding-version\": \"0.1.0\"`\n \n ## Scalars\n \n@@ -426,7 +426,7 @@ codes <zarr.core.Array '/obs/development_stage/codes' (164114,) int8 read-only>\n ## String arrays\n \n Arrays of strings are handled differently than numeric arrays since numpy doesn't really have a good way of representing arrays of unicode strings.\n-`anndata` assumes strings are text-like data, so uses a variable length encoding.\n+`anndata` assumes strings are text-like data, so it uses a variable length encoding.\n \n `````{tab-set}\n "
        },
        {
            "sha": "2b775c69664b83d75cbd2dc1f2ebdca70ec2ecb2",
            "filename": "docs/release-notes/0.10.1.md",
            "status": "modified",
            "additions": 2,
            "deletions": 6,
            "changes": 8,
            "blob_url": "https://github.com/scverse/anndata/blob/f88f7bd4250b963752d615e491b7e676ce5eb7f0/docs%2Frelease-notes%2F0.10.1.md",
            "raw_url": "https://github.com/scverse/anndata/raw/f88f7bd4250b963752d615e491b7e676ce5eb7f0/docs%2Frelease-notes%2F0.10.1.md",
            "contents_url": "https://api.github.com/repos/scverse/anndata/contents/docs%2Frelease-notes%2F0.10.1.md?ref=f88f7bd4250b963752d615e491b7e676ce5eb7f0",
            "patch": "@@ -1,10 +1,6 @@\n-### 0.10.1 {small}`the future`\n+### 0.10.1 {small}`2023-10-08`\n \n ```{rubric} Bugfix\n ```\n \n-```{rubric} Documentation\n-```\n-\n-```{rubric} Performance\n-```\n+* Fix `ad.concat` erroring when concatenating a categorical and object column {pr}`1171` {user}`ivirshup`"
        },
        {
            "sha": "411d6a07295a3a2d83c2125fd1b3b7342ad3b79c",
            "filename": "docs/release-notes/0.10.2.md",
            "status": "added",
            "additions": 17,
            "deletions": 0,
            "changes": 17,
            "blob_url": "https://github.com/scverse/anndata/blob/f88f7bd4250b963752d615e491b7e676ce5eb7f0/docs%2Frelease-notes%2F0.10.2.md",
            "raw_url": "https://github.com/scverse/anndata/raw/f88f7bd4250b963752d615e491b7e676ce5eb7f0/docs%2Frelease-notes%2F0.10.2.md",
            "contents_url": "https://api.github.com/repos/scverse/anndata/contents/docs%2Frelease-notes%2F0.10.2.md?ref=f88f7bd4250b963752d615e491b7e676ce5eb7f0",
            "patch": "@@ -0,0 +1,17 @@\n+### 0.10.2 {small}`2023-10-11`\n+\n+```{rubric} Bugfix\n+```\n+\n+* Added compatibility layer for packages relying on `anndata._core.sparse_dataset.SparseDataset`.\n+  Note that this API is *deprecated* and new code should use {class}`~anndata.experimental.CSRDataset`, {class}`~anndata.experimental.CSCDataset`, and {func}`~anndata.experimental.sparse_dataset` instead.\n+  {pr}`1185` {user}`ivirshup`\n+* Handle deprecation warning from `pd.Categorical.map` thrown during `anndata.concat` {pr}`1189` {user}`flying-sheep` {user}`ivirshup`\n+* Fixed extra steps being included in IO tracebacks {pr}`1193` {user}`flying-sheep`\n+* `as_dense` argument of `write_h5ad` no longer writes an array without encoding metadata {pr}`1193` {user}`flying-sheep`\n+\n+\n+```{rubric} Performance\n+```\n+\n+* Improved performance of `concat_on_disk` with dense arrays in some cases {pr}`1169` {user}`selmanozleyen`"
        },
        {
            "sha": "9b39925e8477715333132b8adcaaed9e6541e598",
            "filename": "docs/release-notes/0.10.3.md",
            "status": "added",
            "additions": 14,
            "deletions": 0,
            "changes": 14,
            "blob_url": "https://github.com/scverse/anndata/blob/f88f7bd4250b963752d615e491b7e676ce5eb7f0/docs%2Frelease-notes%2F0.10.3.md",
            "raw_url": "https://github.com/scverse/anndata/raw/f88f7bd4250b963752d615e491b7e676ce5eb7f0/docs%2Frelease-notes%2F0.10.3.md",
            "contents_url": "https://api.github.com/repos/scverse/anndata/contents/docs%2Frelease-notes%2F0.10.3.md?ref=f88f7bd4250b963752d615e491b7e676ce5eb7f0",
            "patch": "@@ -0,0 +1,14 @@\n+### 0.10.3 {small}`2023-10-31`\n+\n+```{rubric} Bugfix\n+```\n+* Prevent pandas from causing infinite recursion when setting a slice of a categorical column {pr}`1211` {user}`flying-sheep`\n+\n+```{rubric} Documentation\n+```\n+* Stop showing “Support for Awkward Arrays is currently experimental” warnings when\n+  reading, concatenating, slicing, or transposing AnnData objects {pr}`1182` {user}`flying-sheep`\n+\n+```{rubric} Other updates\n+```\n+* Fail canary CI job when tests raise unexpected warnings. {pr}`1182` {user}`flying-sheep`"
        },
        {
            "sha": "5588f534a04c4b2563a3c07caa8f2f278d31a6fb",
            "filename": "docs/release-notes/0.10.4.md",
            "status": "added",
            "additions": 14,
            "deletions": 0,
            "changes": 14,
            "blob_url": "https://github.com/scverse/anndata/blob/f88f7bd4250b963752d615e491b7e676ce5eb7f0/docs%2Frelease-notes%2F0.10.4.md",
            "raw_url": "https://github.com/scverse/anndata/raw/f88f7bd4250b963752d615e491b7e676ce5eb7f0/docs%2Frelease-notes%2F0.10.4.md",
            "contents_url": "https://api.github.com/repos/scverse/anndata/contents/docs%2Frelease-notes%2F0.10.4.md?ref=f88f7bd4250b963752d615e491b7e676ce5eb7f0",
            "patch": "@@ -0,0 +1,14 @@\n+### 0.10.4 {small}`2024-01-04`\n+\n+```{rubric} Bugfix\n+```\n+* Only try to use `Categorical.map(na_action=…)` in actually supported Pandas ≥2.1 {pr}`1226` {user}`flying-sheep`\n+* `AnnData.__sizeof__()` support for backed datasets {pr}`1230` {user}`Neah-Ko`\n+* `adata[:, []]` now returns an `AnnData` object empty on the appropriate dimensions instead of erroring {pr}`1243` {user}`ilan-gold`\n+* `adata.X[mask]` works in newer `numpy` versions when `X` is `backed` {pr}`1255` {user}`ilan-gold`\n+* `adata.X[...]` fixed for `X` as a `BaseCompressedSparseDataset` with `zarr` backend {pr}`1265` {user}`ilan-gold`\n+* Improve read/write error reporting {pr}`1273` {user}`flying-sheep`\n+\n+```{rubric} Documentation\n+```\n+* Improve aligned mapping error messages {pr}`1252` {user}`flying-sheep`"
        },
        {
            "sha": "8ffb759a6e81957ab14c4c6f259e72ff21751e4d",
            "filename": "docs/release-notes/0.10.5.md",
            "status": "added",
            "additions": 19,
            "deletions": 0,
            "changes": 19,
            "blob_url": "https://github.com/scverse/anndata/blob/f88f7bd4250b963752d615e491b7e676ce5eb7f0/docs%2Frelease-notes%2F0.10.5.md",
            "raw_url": "https://github.com/scverse/anndata/raw/f88f7bd4250b963752d615e491b7e676ce5eb7f0/docs%2Frelease-notes%2F0.10.5.md",
            "contents_url": "https://api.github.com/repos/scverse/anndata/contents/docs%2Frelease-notes%2F0.10.5.md?ref=f88f7bd4250b963752d615e491b7e676ce5eb7f0",
            "patch": "@@ -0,0 +1,19 @@\n+### 0.10.5 {small}`2024-01-25`\n+\n+```{rubric} Bugfix\n+```\n+\n+* Fix outer concatenation along variables when only a subset of objects had an entry in layers {pr}`1291` {user}`ivirshup`\n+* Fix comparison of >2d arrays in `uns` during concatenation {pr}`1300` {user}`ivirshup`\n+* Fix IO with awkward array version 2.5.2 {pr}`1328` {user}`ivirshup`\n+* Fix bug (introduced in 0.10.4) where indexing an AnnData with `list[bool]` would return the wrong result {pr}`1332` {user}`ivirshup`\n+\n+```{rubric} Documentation\n+```\n+* Re-add search-as-you-type, this time via `readthedocs-sphinx-search` {pr}`1311` {user}`flying-sheep`\n+\n+```{rubric} Performance\n+```\n+\n+* `BaseCompressedSparseDataset`'s `indptr` is cached {pr}`1266` {user}`ilan-gold`\n+* Improved performance when indexing backed sparse matrices with boolean masks along their major axis {pr}`1233` {user}`ilan-gold`"
        },
        {
            "sha": "e8618118a9854748d4616119d56703a50df2d876",
            "filename": "docs/release-notes/0.10.6.md",
            "status": "added",
            "additions": 21,
            "deletions": 0,
            "changes": 21,
            "blob_url": "https://github.com/scverse/anndata/blob/f88f7bd4250b963752d615e491b7e676ce5eb7f0/docs%2Frelease-notes%2F0.10.6.md",
            "raw_url": "https://github.com/scverse/anndata/raw/f88f7bd4250b963752d615e491b7e676ce5eb7f0/docs%2Frelease-notes%2F0.10.6.md",
            "contents_url": "https://api.github.com/repos/scverse/anndata/contents/docs%2Frelease-notes%2F0.10.6.md?ref=f88f7bd4250b963752d615e491b7e676ce5eb7f0",
            "patch": "@@ -0,0 +1,21 @@\n+### 0.10.6 {small}`the future`\n+\n+```{rubric} Bugfix\n+```\n+\n+* Defer import of zarr in test helpers, as scanpy CI job relies on them {pr}`1343` {user}`ilan-gold`\n+* Writing a dataframe with non-unique column names now throws an error, instead of silently overwriting {pr}`1335` {user}`ivirshup`\n+* Bring optimization from {pr}`1233` to indexing on the whole `AnnData` object, not just the sparse dataset itself {pr}`1365` {user}`ilan-gold`\n+* Fix mean slice length checking to use improved performance when indexing backed sparse matrices with boolean masks along their major axis {pr}`1366` {user}`ilan-gold`\n+\n+```{rubric} Documentation\n+```\n+\n+```{rubric} Performance\n+```\n+\n+```{rubric} Development\n+```\n+\n+* `anndata`'s CI now tests against minimum versions of it's dependencies. As a result, several dependencies had their minimum required version bumped. See diff for details {pr}`1314` {user}`ivirshup`\n+* `anndata` now tests against Python 3.12 {pr}`1373` {user}`ivirshup`"
        },
        {
            "sha": "220e176c43e51794685ad43ff7eb215b67be94f8",
            "filename": "docs/release-notes/0.11.0.md",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/scverse/anndata/blob/f88f7bd4250b963752d615e491b7e676ce5eb7f0/docs%2Frelease-notes%2F0.11.0.md",
            "raw_url": "https://github.com/scverse/anndata/raw/f88f7bd4250b963752d615e491b7e676ce5eb7f0/docs%2Frelease-notes%2F0.11.0.md",
            "contents_url": "https://api.github.com/repos/scverse/anndata/contents/docs%2Frelease-notes%2F0.11.0.md?ref=f88f7bd4250b963752d615e491b7e676ce5eb7f0",
            "patch": "@@ -2,6 +2,8 @@\n \n ```{rubric} Features\n ```\n+* Add `settings` object with methods for altering internally-used options, like checking for uniqueness on `obs`' index {pr}`1270` {user}`ilan-gold`\n+* Add `remove_unused_categories` option to `anndata.settings` to override current behavior.  Default is `True` (i.e., previous behavior).  Please refer to the [documentation](https://anndata.readthedocs.io/en/latest/generated/anndata.settings.html) for usage.  {pr}`1340` {user}`ilan-gold`\n \n ```{rubric} Bugfix\n ```\n@@ -11,3 +13,8 @@\n \n ```{rubric} Performance\n ```\n+\n+```{rubric} Breaking\n+```\n+\n+* Removed deprecated modules `anndata.core` and `anndata.readwrite` {pr}`1197` {user}`ivirshup`"
        },
        {
            "sha": "ab4316f64ef26a24475570c5a325872320f72aa3",
            "filename": "docs/release-notes/0.6.0.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/scverse/anndata/blob/f88f7bd4250b963752d615e491b7e676ce5eb7f0/docs%2Frelease-notes%2F0.6.0.md",
            "raw_url": "https://github.com/scverse/anndata/raw/f88f7bd4250b963752d615e491b7e676ce5eb7f0/docs%2Frelease-notes%2F0.6.0.md",
            "contents_url": "https://api.github.com/repos/scverse/anndata/contents/docs%2Frelease-notes%2F0.6.0.md?ref=f88f7bd4250b963752d615e491b7e676ce5eb7f0",
            "patch": "@@ -26,7 +26,7 @@\n ### 0.6.0 {small}`1 May, 2018`\n \n - compatibility with Seurat converter\n-- tremendous speedup for {func}`~anndata.AnnData.concatenate`\n+- tremendous speedup for {meth}`~anndata.AnnData.concatenate`\n - bug fix for deep copy of unstructured annotation after slicing\n - bug fix for reading HDF5 stored single-category annotations\n - `'outer join'` concatenation: adds zeros for concatenation of sparse data and nans for dense data"
        },
        {
            "sha": "c36f7a8a548dfa80ae0bcc032986ae28c3592128",
            "filename": "docs/release-notes/release-latest.md",
            "status": "modified",
            "additions": 15,
            "deletions": 0,
            "changes": 15,
            "blob_url": "https://github.com/scverse/anndata/blob/f88f7bd4250b963752d615e491b7e676ce5eb7f0/docs%2Frelease-notes%2Frelease-latest.md",
            "raw_url": "https://github.com/scverse/anndata/raw/f88f7bd4250b963752d615e491b7e676ce5eb7f0/docs%2Frelease-notes%2Frelease-latest.md",
            "contents_url": "https://api.github.com/repos/scverse/anndata/contents/docs%2Frelease-notes%2Frelease-latest.md?ref=f88f7bd4250b963752d615e491b7e676ce5eb7f0",
            "patch": "@@ -5,6 +5,21 @@\n \n ## Version 0.10\n \n+```{include} /release-notes/0.10.6.md\n+```\n+\n+```{include} /release-notes/0.10.5.md\n+```\n+\n+```{include} /release-notes/0.10.4.md\n+```\n+\n+```{include} /release-notes/0.10.3.md\n+```\n+\n+```{include} /release-notes/0.10.2.md\n+```\n+\n ```{include} /release-notes/0.10.1.md\n ```\n "
        },
        {
            "sha": "85bd6ac91104b8c40b75126d84fd50cf1c6fb3ce",
            "filename": "pyproject.toml",
            "status": "modified",
            "additions": 52,
            "deletions": 24,
            "changes": 76,
            "blob_url": "https://github.com/scverse/anndata/blob/f88f7bd4250b963752d615e491b7e676ce5eb7f0/pyproject.toml",
            "raw_url": "https://github.com/scverse/anndata/raw/f88f7bd4250b963752d615e491b7e676ce5eb7f0/pyproject.toml",
            "contents_url": "https://api.github.com/repos/scverse/anndata/contents/pyproject.toml?ref=f88f7bd4250b963752d615e491b7e676ce5eb7f0",
            "patch": "@@ -32,19 +32,20 @@ classifiers = [\n     \"Programming Language :: Python :: 3.9\",\n     \"Programming Language :: Python :: 3.10\",\n     \"Programming Language :: Python :: 3.11\",\n+    \"Programming Language :: Python :: 3.12\",\n     \"Topic :: Scientific/Engineering :: Bio-Informatics\",\n     \"Topic :: Scientific/Engineering :: Visualization\",\n ]\n dependencies = [\n-    # pandas <1.1.1 has pandas/issues/35446\n+    # pandas <1.4 has pandas/issues/35446\n     # pandas 2.1.0rc0 has pandas/issues/54622\n-    \"pandas >=1.1.1, !=2.1.0rc0\",\n-    \"numpy>=1.16.5\",                         # required by pandas 1.x\n-    \"scipy>1.4\",\n-    \"h5py>=3\",\n+    \"pandas >=1.4, !=2.1.0rc0, !=2.1.2\",\n+    \"numpy>=1.23\",\n+    \"scipy>1.8\",\n+    \"h5py>=3.1\",\n     \"exceptiongroup; python_version<'3.11'\",\n     \"natsort\",\n-    \"packaging>=20\",\n+    \"packaging>=20.0\",\n     \"array_api_compat\",\n ]\n dynamic = [\"version\"]\n@@ -64,22 +65,23 @@ dev = [\n ]\n doc = [\n     \"sphinx>=4.4\",\n-    \"sphinx-book-theme>=1.0.1\",\n+    \"sphinx-book-theme>=1.1.0\",\n     \"sphinx-autodoc-typehints>=1.11.0\",\n     \"sphinx-issues\",\n     \"sphinx-copybutton\",\n     \"sphinxext.opengraph\",\n     \"nbsphinx\",\n-    \"scanpydoc>=0.9\",\n+    \"scanpydoc[theme,typehints] >=0.13.4\",\n     \"zarr\",\n     \"awkward>=2.0.7\",\n     \"IPython\",                          # For syntax highlighting in notebooks\n     \"myst_parser\",\n     \"sphinx_design>=0.5.0\",\n+    \"readthedocs-sphinx-search\",\n ]\n test = [\n     \"loompy>=3.0.5\",\n-    \"pytest>=6.0\",\n+    \"pytest>=7.3\",\n     \"pytest-cov>=2.10\",\n     \"zarr\",\n     \"matplotlib\",\n@@ -89,9 +91,11 @@ test = [\n     \"boltons\",\n     \"scanpy\",\n     \"httpx\", # For data downloading\n-    \"dask[array,distributed]\",\n+    \"dask[array,distributed]>=2022.09.2\",\n     \"awkward>=2.3\",\n+    \"pyarrow\",\n     \"pytest_memray\",\n+    \"pytest-mock\"\n ]\n gpu = [\"cupy\"]\n \n@@ -104,26 +108,42 @@ version-file = \"anndata/_version.py\"\n \n [tool.coverage.run]\n source = [\"anndata\"]\n-omit = [\"setup.py\", \"versioneer.py\", \"anndata/_version.py\", \"**/test_*.py\"]\n+omit = [\"anndata/_version.py\", \"**/test_*.py\"]\n+\n+[tool.coverage.report]\n+exclude_also = [\n+    \"if TYPE_CHECKING:\",\n+]\n \n [tool.pytest.ini_options]\n-addopts = \"--doctest-modules\"\n+addopts = [\n+    \"--strict-markers\",\n+    \"--doctest-modules\",\n+]\n+filterwarnings = [\n+    'ignore:Support for Awkward Arrays is currently experimental',\n+    'ignore:Outer joins on awkward\\.Arrays',\n+    # TODO: replace both lines above with this one once we figured out how prevent ImportPathMismatchError\n+    # 'ignore::anndata._warnings.ExperimentalFeatureWarning',\n+]\n+# When `--strict-warnings` is used, all warnings are treated as errors, except those:\n+filterwarnings_when_strict = [\n+    \"default::anndata._warnings.ImplicitModificationWarning\",\n+    \"default:Transforming to str index:UserWarning\",\n+    \"default:(Observation|Variable) names are not unique. To make them unique:UserWarning\",\n+    \"default::scipy.sparse.SparseEfficiencyWarning\",\n+    \"default::dask.array.core.PerformanceWarning\",\n+]\n python_files = \"test_*.py\"\n testpaths = [\"anndata\", \"docs/concatenation.rst\"]\n-filterwarnings = ['ignore:X\\.dtype being converted to np.float32:FutureWarning']\n # For some reason this effects how logging is shown when tests are run\n xfail_strict = true\n markers = [\"gpu: mark test to run on GPU\"]\n \n-[tool.ruff]\n-ignore = [\n-    # line too long -> we accept long comment lines; black gets rid of long code lines\n-    \"E501\",\n-    # Do not assign a lambda expression, use a def -> AnnData allows lambda expression assignments,\n-    \"E731\",\n-    # allow I, O, l as variable names -> I is the identity matrix, i, j, k, l is reasonable indexing notation\n-    \"E741\",\n-]\n+[tool.ruff.format]\n+docstring-code-format = true\n+\n+[tool.ruff.lint]\n select = [\n     \"E\",   # Error detected by Pycodestyle\n     \"F\",   # Errors detected by Pyflakes\n@@ -134,10 +154,18 @@ select = [\n     \"ICN\", # Follow import conventions\n     \"PTH\", # Pathlib instead of os.path\n ]\n-[tool.ruff.per-file-ignores]\n+ignore = [\n+    # line too long -> we accept long comment lines; formatter gets rid of long code lines\n+    \"E501\",\n+    # Do not assign a lambda expression, use a def -> AnnData allows lambda expression assignments,\n+    \"E731\",\n+    # allow I, O, l as variable names -> I is the identity matrix, i, j, k, l is reasonable indexing notation\n+    \"E741\",\n+]\n+[tool.ruff.lint.per-file-ignores]\n # E721 comparing types, but we specifically are checking that we aren't getting subtypes (views)\n \"anndata/tests/test_readwrite.py\" = [\"E721\"]\n-[tool.ruff.isort]\n+[tool.ruff.lint.isort]\n known-first-party = [\"anndata\"]\n required-imports = [\"from __future__ import annotations\"]\n "
        }
    ]
}